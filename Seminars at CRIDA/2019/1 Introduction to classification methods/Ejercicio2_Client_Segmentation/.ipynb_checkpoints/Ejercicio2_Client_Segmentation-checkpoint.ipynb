{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vamos a realizar un clasificador multiclase\n",
    "Podemos imaginar que obtenemos cierta información de nuestros clientes: datos históricos, a través de que canal han llegado a trabajar con nosotros, línea de negocio, balances...\n",
    "Un problema muy interesante sería poder priorizar la atención y recursos gastados en aquellos clientes que nos van a resultar en un mayor beneficio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos librerias y clientes\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#clients = pd.read_csv('clients.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(clients.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(clients.groupby('client_type').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#sns.countplot(clients['client_type'],label=\"Count\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Revisamos las features\n",
    "# Dibujamos los histogramas normalizados, transparencias...\n",
    "#plt.hist(clients[clients[\"client_type\"] == \"conservative\"].feat4, bins=10, label = \"conservative\", density=True, alpha = 0.5)\n",
    "#plt.hist(clients[clients[\"client_type\"] == \"VIP\"].feat4, bins=10, label = \"VIP\", density=True, alpha = 0.5)\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.xlabel(\"feature\")\n",
    "#plt.ylabel(\"Probability\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividimos entre train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZACION DE DATOS\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "#pd.DataFrame(data = {\"predict\":clf.predict(X_test), \"real\": y_test})\n",
    "\n",
    "#print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "#     .format(clf.score(X_train, y_train)))\n",
    "#print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "#     .format(clf.score(X_test, y_test)))\n",
    "#clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "#pd.DataFrame(data = {\"predict\":clf.predict(X_test), \"real\": y_test})\n",
    "\n",
    "#print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "#     .format(clf.score(X_train, y_train)))\n",
    "#print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "#     .format(clf.score(X_test, y_test)))\n",
    "\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import GradientBoostingClassifier\n",
    "#clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "#pd.DataFrame(data = {\"predict\":clf.predict(X_test), \"real\": y_test})\n",
    "\n",
    "#print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "#     .format(clf.score(X_train, y_train)))\n",
    "#print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "#     .format(clf.score(X_test, y_test)))\n",
    "\n",
    "#print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "# Probar diferentes kernels\n",
    "#from sklearn.svm import SVC\n",
    "#svm = SVC()\n",
    "#svm.fit(X_train, y_train)\n",
    "#print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "#     .format(svm.score(X_train, y_train)))\n",
    "#print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "#     .format(svm.score(X_test, y_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
