{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "CRIDA_2022_Ejercicio_3_3.ipynb",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ivangtorre/Curso_CRIDA_2022/blob/main/CRIDA_2022_Ejercicio_3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## CIFAR-100 Classification using ALexNet\n"
   ],
   "metadata": {
    "id": "pnIb9xUM7BDb",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "metadata": {
    "id": "PtKvmZx-WmUu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8fe0ec5d-d895-429b-caeb-8809b1d40351",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# First install Pytorch\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "id": "bGU6NwlsXFSt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CIFAR-100 Dataset\n",
    "he CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "zzJlPBFs7d1i",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "metadata": {
    "id": "lCsBCXMwbpH5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "100c3209-f6f7-4f94-92e2-019f03652b5a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "tranform_train = transforms.Compose([transforms.Resize((227,227)), transforms.RandomHorizontalFlip(p=0.7), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "tranform_test = transforms.Compose([transforms.Resize((227,227)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "train_data = datasets.CIFAR100(root = './data', train = True, transform = tranform_train, download = True)\n",
    "test_data = datasets.CIFAR100(root = './data', train = False, transform = tranform_test)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "id": "_bNfVLRUYqZA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Hyperparameters\n",
    "batch_size = 128 # the size of input data took for one iteration\n",
    "num_classes = 100 # number of output classes discrete range [0,99] ** DO NOT MODIFY\n",
    "num_epochs = 10 # number of times which the entire dataset is passed throughout the model\n",
    "lr = 5e-4 # size of step "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "rfDPBdnYgfGp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "#Load the data\n",
    "train_gen = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_gen = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize the dataset\n",
    "When we run the code, we will get the greyscale visualization of the MNIST images"
   ],
   "metadata": {
    "id": "io5V029BFBzi",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "classes = train_data.classes\n",
    "image, label = train_data[0] # Visualize the data\n",
    "\n",
    "print(classes[label])\n",
    "plt.imshow(image.permute(1, 2, 0))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "GeEskU7EDVME",
    "outputId": "d6d255c1-4df7-44a3-f6ea-be8bb7539d02",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cattle\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb31940eb90>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ALEXNET\n",
    "AlexNet is the name of a convolutional neural network (CNN) architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton, who was Krizhevsky's Ph.D. advisor.\n",
    "\n",
    "AlexNet competed in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012.[3] The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of graphics processing units (GPUs) during training\n",
    "\n"
   ],
   "metadata": {
    "id": "HBwrQNk68fwZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "metadata": {
    "id": "fL-YXTvghaz_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the following we can visualize the structure of the Deep Neural Network that we have just built."
   ],
   "metadata": {
    "id": "peIM_nC4_G0C",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Build the Model\n",
    "net = AlexNet()\n",
    "if torch.cuda.is_available():\n",
    "  net.cuda()\n",
    "print(net)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ri124kAZ8Ne8",
    "outputId": "3bafdcc6-c708-4f3c-f9ef-f5f22be923f0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "id": "ePLIwvAFj2zH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "#Define loss-function & optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam( net.parameters(), lr=lr)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the model\n",
    "It is now time to train the model. During training all images will be input into the DNN during one epoch. The objetive is to minimize a loss function (Cross Entropy) between the output and the ground truth. The errors are backpropagated and the optimizer adjust the parameters of the DNN in order to search an optimal solution. Usually the more epochs we train, the better model we will get (although overfitting could happen)"
   ],
   "metadata": {
    "id": "8PFybdLq_tYt",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "metadata": {
    "id": "u75Xa5VckuTH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "08cabbee-8104-4735-c645-17f1d30beaa7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "#Training the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr= lr) #Adam seems to be the most popular for deep learning\n",
    "\n",
    "for epoch in range(num_epochs): #I decided to train the model for 50 epochs\n",
    "    loss_ep = 0\n",
    "    for batch_idx, (data, targets) in enumerate(train_gen):\n",
    "        data = data.cuda()\n",
    "        targets = targets.cuda()\n",
    "        ## Forward Pass\n",
    "        optimizer.zero_grad()\n",
    "        scores = net(data)\n",
    "        loss = criterion(scores,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ep += loss.item()\n",
    "        print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, batch_idx+1, len(train_data)//batch_size, loss.data))\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/10], Step [1/390], Loss: 6.9064\n",
      "Epoch [1/10], Step [2/390], Loss: 6.6774\n",
      "Epoch [1/10], Step [3/390], Loss: 12.9917\n",
      "Epoch [1/10], Step [4/390], Loss: 5.5959\n",
      "Epoch [1/10], Step [5/390], Loss: 6.2047\n",
      "Epoch [1/10], Step [6/390], Loss: 6.3363\n",
      "Epoch [1/10], Step [7/390], Loss: 6.2325\n",
      "Epoch [1/10], Step [8/390], Loss: 5.8542\n",
      "Epoch [1/10], Step [9/390], Loss: 5.2662\n",
      "Epoch [1/10], Step [10/390], Loss: 4.9030\n",
      "Epoch [1/10], Step [11/390], Loss: 5.1657\n",
      "Epoch [1/10], Step [12/390], Loss: 5.3836\n",
      "Epoch [1/10], Step [13/390], Loss: 5.0546\n",
      "Epoch [1/10], Step [14/390], Loss: 4.8539\n",
      "Epoch [1/10], Step [15/390], Loss: 4.8414\n",
      "Epoch [1/10], Step [16/390], Loss: 4.6948\n",
      "Epoch [1/10], Step [17/390], Loss: 4.7954\n",
      "Epoch [1/10], Step [18/390], Loss: 4.7152\n",
      "Epoch [1/10], Step [19/390], Loss: 4.7937\n",
      "Epoch [1/10], Step [20/390], Loss: 4.7563\n",
      "Epoch [1/10], Step [21/390], Loss: 4.7560\n",
      "Epoch [1/10], Step [22/390], Loss: 4.7901\n",
      "Epoch [1/10], Step [23/390], Loss: 4.7712\n",
      "Epoch [1/10], Step [24/390], Loss: 4.6933\n",
      "Epoch [1/10], Step [25/390], Loss: 4.8137\n",
      "Epoch [1/10], Step [26/390], Loss: 4.8009\n",
      "Epoch [1/10], Step [27/390], Loss: 4.8490\n",
      "Epoch [1/10], Step [28/390], Loss: 4.6755\n",
      "Epoch [1/10], Step [29/390], Loss: 4.7130\n",
      "Epoch [1/10], Step [30/390], Loss: 4.7532\n",
      "Epoch [1/10], Step [31/390], Loss: 4.7192\n",
      "Epoch [1/10], Step [32/390], Loss: 4.7828\n",
      "Epoch [1/10], Step [33/390], Loss: 4.7870\n",
      "Epoch [1/10], Step [34/390], Loss: 4.7735\n",
      "Epoch [1/10], Step [35/390], Loss: 4.7977\n",
      "Epoch [1/10], Step [36/390], Loss: 4.7741\n",
      "Epoch [1/10], Step [37/390], Loss: 4.6538\n",
      "Epoch [1/10], Step [38/390], Loss: 4.7536\n",
      "Epoch [1/10], Step [39/390], Loss: 4.7915\n",
      "Epoch [1/10], Step [40/390], Loss: 4.6468\n",
      "Epoch [1/10], Step [41/390], Loss: 4.7213\n",
      "Epoch [1/10], Step [42/390], Loss: 4.7770\n",
      "Epoch [1/10], Step [43/390], Loss: 4.6694\n",
      "Epoch [1/10], Step [44/390], Loss: 4.6806\n",
      "Epoch [1/10], Step [45/390], Loss: 4.6792\n",
      "Epoch [1/10], Step [46/390], Loss: 4.6516\n",
      "Epoch [1/10], Step [47/390], Loss: 4.6411\n",
      "Epoch [1/10], Step [48/390], Loss: 4.7286\n",
      "Epoch [1/10], Step [49/390], Loss: 4.6749\n",
      "Epoch [1/10], Step [50/390], Loss: 4.6603\n",
      "Epoch [1/10], Step [51/390], Loss: 4.6296\n",
      "Epoch [1/10], Step [52/390], Loss: 4.6397\n",
      "Epoch [1/10], Step [53/390], Loss: 4.6727\n",
      "Epoch [1/10], Step [54/390], Loss: 4.6758\n",
      "Epoch [1/10], Step [55/390], Loss: 4.6998\n",
      "Epoch [1/10], Step [56/390], Loss: 4.7329\n",
      "Epoch [1/10], Step [57/390], Loss: 4.7904\n",
      "Epoch [1/10], Step [58/390], Loss: 4.7636\n",
      "Epoch [1/10], Step [59/390], Loss: 4.6580\n",
      "Epoch [1/10], Step [60/390], Loss: 4.7019\n",
      "Epoch [1/10], Step [61/390], Loss: 4.6636\n",
      "Epoch [1/10], Step [62/390], Loss: 4.6480\n",
      "Epoch [1/10], Step [63/390], Loss: 4.7038\n",
      "Epoch [1/10], Step [64/390], Loss: 4.6892\n",
      "Epoch [1/10], Step [65/390], Loss: 4.6446\n",
      "Epoch [1/10], Step [66/390], Loss: 4.6834\n",
      "Epoch [1/10], Step [67/390], Loss: 4.6834\n",
      "Epoch [1/10], Step [68/390], Loss: 4.6824\n",
      "Epoch [1/10], Step [69/390], Loss: 4.6585\n",
      "Epoch [1/10], Step [70/390], Loss: 4.7063\n",
      "Epoch [1/10], Step [71/390], Loss: 4.6887\n",
      "Epoch [1/10], Step [72/390], Loss: 4.6704\n",
      "Epoch [1/10], Step [73/390], Loss: 4.6218\n",
      "Epoch [1/10], Step [74/390], Loss: 4.6826\n",
      "Epoch [1/10], Step [75/390], Loss: 4.6423\n",
      "Epoch [1/10], Step [76/390], Loss: 4.6639\n",
      "Epoch [1/10], Step [77/390], Loss: 4.6793\n",
      "Epoch [1/10], Step [78/390], Loss: 4.6436\n",
      "Epoch [1/10], Step [79/390], Loss: 4.6948\n",
      "Epoch [1/10], Step [80/390], Loss: 4.6618\n",
      "Epoch [1/10], Step [81/390], Loss: 4.6423\n",
      "Epoch [1/10], Step [82/390], Loss: 4.7056\n",
      "Epoch [1/10], Step [83/390], Loss: 4.6707\n",
      "Epoch [1/10], Step [84/390], Loss: 4.6536\n",
      "Epoch [1/10], Step [85/390], Loss: 4.6411\n",
      "Epoch [1/10], Step [86/390], Loss: 4.6990\n",
      "Epoch [1/10], Step [87/390], Loss: 4.6460\n",
      "Epoch [1/10], Step [88/390], Loss: 4.6069\n",
      "Epoch [1/10], Step [89/390], Loss: 4.7158\n",
      "Epoch [1/10], Step [90/390], Loss: 4.6737\n",
      "Epoch [1/10], Step [91/390], Loss: 4.6075\n",
      "Epoch [1/10], Step [92/390], Loss: 4.6924\n",
      "Epoch [1/10], Step [93/390], Loss: 4.6247\n",
      "Epoch [1/10], Step [94/390], Loss: 4.6533\n",
      "Epoch [1/10], Step [95/390], Loss: 4.6294\n",
      "Epoch [1/10], Step [96/390], Loss: 4.6514\n",
      "Epoch [1/10], Step [97/390], Loss: 4.6537\n",
      "Epoch [1/10], Step [98/390], Loss: 4.6599\n",
      "Epoch [1/10], Step [99/390], Loss: 4.6172\n",
      "Epoch [1/10], Step [100/390], Loss: 4.6627\n",
      "Epoch [1/10], Step [101/390], Loss: 4.6937\n",
      "Epoch [1/10], Step [102/390], Loss: 4.6703\n",
      "Epoch [1/10], Step [103/390], Loss: 4.6894\n",
      "Epoch [1/10], Step [104/390], Loss: 4.6657\n",
      "Epoch [1/10], Step [105/390], Loss: 4.6978\n",
      "Epoch [1/10], Step [106/390], Loss: 4.6894\n",
      "Epoch [1/10], Step [107/390], Loss: 4.6408\n",
      "Epoch [1/10], Step [108/390], Loss: 4.6528\n",
      "Epoch [1/10], Step [109/390], Loss: 4.6780\n",
      "Epoch [1/10], Step [110/390], Loss: 4.6598\n",
      "Epoch [1/10], Step [111/390], Loss: 4.6315\n",
      "Epoch [1/10], Step [112/390], Loss: 4.6451\n",
      "Epoch [1/10], Step [113/390], Loss: 4.6517\n",
      "Epoch [1/10], Step [114/390], Loss: 4.6718\n",
      "Epoch [1/10], Step [115/390], Loss: 4.6296\n",
      "Epoch [1/10], Step [116/390], Loss: 4.6233\n",
      "Epoch [1/10], Step [117/390], Loss: 4.6270\n",
      "Epoch [1/10], Step [118/390], Loss: 4.6240\n",
      "Epoch [1/10], Step [119/390], Loss: 4.6521\n",
      "Epoch [1/10], Step [120/390], Loss: 4.6118\n",
      "Epoch [1/10], Step [121/390], Loss: 4.6342\n",
      "Epoch [1/10], Step [122/390], Loss: 4.6430\n",
      "Epoch [1/10], Step [123/390], Loss: 4.6595\n",
      "Epoch [1/10], Step [124/390], Loss: 4.6447\n",
      "Epoch [1/10], Step [125/390], Loss: 4.6698\n",
      "Epoch [1/10], Step [126/390], Loss: 4.6465\n",
      "Epoch [1/10], Step [127/390], Loss: 4.6255\n",
      "Epoch [1/10], Step [128/390], Loss: 4.5932\n",
      "Epoch [1/10], Step [129/390], Loss: 4.6287\n",
      "Epoch [1/10], Step [130/390], Loss: 4.7034\n",
      "Epoch [1/10], Step [131/390], Loss: 4.6510\n",
      "Epoch [1/10], Step [132/390], Loss: 4.6680\n",
      "Epoch [1/10], Step [133/390], Loss: 4.6889\n",
      "Epoch [1/10], Step [134/390], Loss: 4.6252\n",
      "Epoch [1/10], Step [135/390], Loss: 4.5983\n",
      "Epoch [1/10], Step [136/390], Loss: 4.6215\n",
      "Epoch [1/10], Step [137/390], Loss: 4.6415\n",
      "Epoch [1/10], Step [138/390], Loss: 4.6247\n",
      "Epoch [1/10], Step [139/390], Loss: 4.6150\n",
      "Epoch [1/10], Step [140/390], Loss: 4.5864\n",
      "Epoch [1/10], Step [141/390], Loss: 4.5920\n",
      "Epoch [1/10], Step [142/390], Loss: 4.5853\n",
      "Epoch [1/10], Step [143/390], Loss: 4.6214\n",
      "Epoch [1/10], Step [144/390], Loss: 4.5812\n",
      "Epoch [1/10], Step [145/390], Loss: 4.5485\n",
      "Epoch [1/10], Step [146/390], Loss: 4.5717\n",
      "Epoch [1/10], Step [147/390], Loss: 4.5782\n",
      "Epoch [1/10], Step [148/390], Loss: 4.4990\n",
      "Epoch [1/10], Step [149/390], Loss: 4.5671\n",
      "Epoch [1/10], Step [150/390], Loss: 4.5129\n",
      "Epoch [1/10], Step [151/390], Loss: 4.4618\n",
      "Epoch [1/10], Step [152/390], Loss: 4.6147\n",
      "Epoch [1/10], Step [153/390], Loss: 4.4682\n",
      "Epoch [1/10], Step [154/390], Loss: 4.4514\n",
      "Epoch [1/10], Step [155/390], Loss: 4.4779\n",
      "Epoch [1/10], Step [156/390], Loss: 4.5472\n",
      "Epoch [1/10], Step [157/390], Loss: 4.4783\n",
      "Epoch [1/10], Step [158/390], Loss: 4.5086\n",
      "Epoch [1/10], Step [159/390], Loss: 4.4963\n",
      "Epoch [1/10], Step [160/390], Loss: 4.4293\n",
      "Epoch [1/10], Step [161/390], Loss: 4.4994\n",
      "Epoch [1/10], Step [162/390], Loss: 4.5932\n",
      "Epoch [1/10], Step [163/390], Loss: 4.4723\n",
      "Epoch [1/10], Step [164/390], Loss: 4.4147\n",
      "Epoch [1/10], Step [165/390], Loss: 4.4457\n",
      "Epoch [1/10], Step [166/390], Loss: 4.4400\n",
      "Epoch [1/10], Step [167/390], Loss: 4.4968\n",
      "Epoch [1/10], Step [168/390], Loss: 4.4894\n",
      "Epoch [1/10], Step [169/390], Loss: 4.4748\n",
      "Epoch [1/10], Step [170/390], Loss: 4.5283\n",
      "Epoch [1/10], Step [171/390], Loss: 4.4145\n",
      "Epoch [1/10], Step [172/390], Loss: 4.4366\n",
      "Epoch [1/10], Step [173/390], Loss: 4.4481\n",
      "Epoch [1/10], Step [174/390], Loss: 4.3067\n",
      "Epoch [1/10], Step [175/390], Loss: 4.5213\n",
      "Epoch [1/10], Step [176/390], Loss: 4.3504\n",
      "Epoch [1/10], Step [177/390], Loss: 4.4165\n",
      "Epoch [1/10], Step [178/390], Loss: 4.3763\n",
      "Epoch [1/10], Step [179/390], Loss: 4.4119\n",
      "Epoch [1/10], Step [180/390], Loss: 4.5414\n",
      "Epoch [1/10], Step [181/390], Loss: 4.3839\n",
      "Epoch [1/10], Step [182/390], Loss: 4.4484\n",
      "Epoch [1/10], Step [183/390], Loss: 4.4613\n",
      "Epoch [1/10], Step [184/390], Loss: 4.4114\n",
      "Epoch [1/10], Step [185/390], Loss: 4.3595\n",
      "Epoch [1/10], Step [186/390], Loss: 4.5083\n",
      "Epoch [1/10], Step [187/390], Loss: 4.4199\n",
      "Epoch [1/10], Step [188/390], Loss: 4.3159\n",
      "Epoch [1/10], Step [189/390], Loss: 4.4320\n",
      "Epoch [1/10], Step [190/390], Loss: 4.4231\n",
      "Epoch [1/10], Step [191/390], Loss: 4.4601\n",
      "Epoch [1/10], Step [192/390], Loss: 4.3751\n",
      "Epoch [1/10], Step [193/390], Loss: 4.3884\n",
      "Epoch [1/10], Step [194/390], Loss: 4.3247\n",
      "Epoch [1/10], Step [195/390], Loss: 4.3319\n",
      "Epoch [1/10], Step [196/390], Loss: 4.3662\n",
      "Epoch [1/10], Step [197/390], Loss: 4.6168\n",
      "Epoch [1/10], Step [198/390], Loss: 4.2554\n",
      "Epoch [1/10], Step [199/390], Loss: 4.3528\n",
      "Epoch [1/10], Step [200/390], Loss: 4.4069\n",
      "Epoch [1/10], Step [201/390], Loss: 4.3473\n",
      "Epoch [1/10], Step [202/390], Loss: 4.3049\n",
      "Epoch [1/10], Step [203/390], Loss: 4.3074\n",
      "Epoch [1/10], Step [204/390], Loss: 4.3428\n",
      "Epoch [1/10], Step [205/390], Loss: 4.4386\n",
      "Epoch [1/10], Step [206/390], Loss: 4.3296\n",
      "Epoch [1/10], Step [207/390], Loss: 4.2910\n",
      "Epoch [1/10], Step [208/390], Loss: 4.2336\n",
      "Epoch [1/10], Step [209/390], Loss: 4.4364\n",
      "Epoch [1/10], Step [210/390], Loss: 4.3010\n",
      "Epoch [1/10], Step [211/390], Loss: 4.3540\n",
      "Epoch [1/10], Step [212/390], Loss: 4.3870\n",
      "Epoch [1/10], Step [213/390], Loss: 4.3488\n",
      "Epoch [1/10], Step [214/390], Loss: 4.3521\n",
      "Epoch [1/10], Step [215/390], Loss: 4.1821\n",
      "Epoch [1/10], Step [216/390], Loss: 4.2909\n",
      "Epoch [1/10], Step [217/390], Loss: 4.3797\n",
      "Epoch [1/10], Step [218/390], Loss: 4.2818\n",
      "Epoch [1/10], Step [219/390], Loss: 4.2733\n",
      "Epoch [1/10], Step [220/390], Loss: 4.1932\n",
      "Epoch [1/10], Step [221/390], Loss: 4.4361\n",
      "Epoch [1/10], Step [222/390], Loss: 4.2467\n",
      "Epoch [1/10], Step [223/390], Loss: 4.4195\n",
      "Epoch [1/10], Step [224/390], Loss: 4.2562\n",
      "Epoch [1/10], Step [225/390], Loss: 4.2674\n",
      "Epoch [1/10], Step [226/390], Loss: 4.2867\n",
      "Epoch [1/10], Step [227/390], Loss: 4.2830\n",
      "Epoch [1/10], Step [228/390], Loss: 4.2760\n",
      "Epoch [1/10], Step [229/390], Loss: 4.1842\n",
      "Epoch [1/10], Step [230/390], Loss: 4.2778\n",
      "Epoch [1/10], Step [231/390], Loss: 4.2260\n",
      "Epoch [1/10], Step [232/390], Loss: 4.1790\n",
      "Epoch [1/10], Step [233/390], Loss: 4.3177\n",
      "Epoch [1/10], Step [234/390], Loss: 4.3932\n",
      "Epoch [1/10], Step [235/390], Loss: 4.1831\n",
      "Epoch [1/10], Step [236/390], Loss: 4.2346\n",
      "Epoch [1/10], Step [237/390], Loss: 4.2727\n",
      "Epoch [1/10], Step [238/390], Loss: 4.2578\n",
      "Epoch [1/10], Step [239/390], Loss: 4.3193\n",
      "Epoch [1/10], Step [240/390], Loss: 4.1733\n",
      "Epoch [1/10], Step [241/390], Loss: 4.2537\n",
      "Epoch [1/10], Step [242/390], Loss: 4.3487\n",
      "Epoch [1/10], Step [243/390], Loss: 4.2289\n",
      "Epoch [1/10], Step [244/390], Loss: 4.1500\n",
      "Epoch [1/10], Step [245/390], Loss: 4.2548\n",
      "Epoch [1/10], Step [246/390], Loss: 4.2884\n",
      "Epoch [1/10], Step [247/390], Loss: 4.2814\n",
      "Epoch [1/10], Step [248/390], Loss: 4.3097\n",
      "Epoch [1/10], Step [249/390], Loss: 4.3447\n",
      "Epoch [1/10], Step [250/390], Loss: 4.3196\n",
      "Epoch [1/10], Step [251/390], Loss: 4.2168\n",
      "Epoch [1/10], Step [252/390], Loss: 4.2068\n",
      "Epoch [1/10], Step [253/390], Loss: 4.3330\n",
      "Epoch [1/10], Step [254/390], Loss: 4.1677\n",
      "Epoch [1/10], Step [255/390], Loss: 4.2194\n",
      "Epoch [1/10], Step [256/390], Loss: 4.2198\n",
      "Epoch [1/10], Step [257/390], Loss: 4.1526\n",
      "Epoch [1/10], Step [258/390], Loss: 4.2301\n",
      "Epoch [1/10], Step [259/390], Loss: 4.2822\n",
      "Epoch [1/10], Step [260/390], Loss: 4.2668\n",
      "Epoch [1/10], Step [261/390], Loss: 4.1385\n",
      "Epoch [1/10], Step [262/390], Loss: 4.3296\n",
      "Epoch [1/10], Step [263/390], Loss: 4.0800\n",
      "Epoch [1/10], Step [264/390], Loss: 4.2620\n",
      "Epoch [1/10], Step [265/390], Loss: 4.2844\n",
      "Epoch [1/10], Step [266/390], Loss: 4.3933\n",
      "Epoch [1/10], Step [267/390], Loss: 4.3402\n",
      "Epoch [1/10], Step [268/390], Loss: 4.1717\n",
      "Epoch [1/10], Step [269/390], Loss: 4.1947\n",
      "Epoch [1/10], Step [270/390], Loss: 4.2956\n",
      "Epoch [1/10], Step [271/390], Loss: 4.2236\n",
      "Epoch [1/10], Step [272/390], Loss: 4.3210\n",
      "Epoch [1/10], Step [273/390], Loss: 4.2641\n",
      "Epoch [1/10], Step [274/390], Loss: 4.1322\n",
      "Epoch [1/10], Step [275/390], Loss: 4.2552\n",
      "Epoch [1/10], Step [276/390], Loss: 4.2871\n",
      "Epoch [1/10], Step [277/390], Loss: 4.0703\n",
      "Epoch [1/10], Step [278/390], Loss: 4.2220\n",
      "Epoch [1/10], Step [279/390], Loss: 4.1888\n",
      "Epoch [1/10], Step [280/390], Loss: 4.2461\n",
      "Epoch [1/10], Step [281/390], Loss: 4.0998\n",
      "Epoch [1/10], Step [282/390], Loss: 4.3001\n",
      "Epoch [1/10], Step [283/390], Loss: 4.1389\n",
      "Epoch [1/10], Step [284/390], Loss: 4.2545\n",
      "Epoch [1/10], Step [285/390], Loss: 4.1404\n",
      "Epoch [1/10], Step [286/390], Loss: 4.1291\n",
      "Epoch [1/10], Step [287/390], Loss: 4.2257\n",
      "Epoch [1/10], Step [288/390], Loss: 4.2345\n",
      "Epoch [1/10], Step [289/390], Loss: 4.2368\n",
      "Epoch [1/10], Step [290/390], Loss: 4.1614\n",
      "Epoch [1/10], Step [291/390], Loss: 4.2502\n",
      "Epoch [1/10], Step [292/390], Loss: 4.0955\n",
      "Epoch [1/10], Step [293/390], Loss: 4.1564\n",
      "Epoch [1/10], Step [294/390], Loss: 3.9864\n",
      "Epoch [1/10], Step [295/390], Loss: 4.1573\n",
      "Epoch [1/10], Step [296/390], Loss: 4.1224\n",
      "Epoch [1/10], Step [297/390], Loss: 4.0925\n",
      "Epoch [1/10], Step [298/390], Loss: 4.1606\n",
      "Epoch [1/10], Step [299/390], Loss: 4.0520\n",
      "Epoch [1/10], Step [300/390], Loss: 4.0985\n",
      "Epoch [1/10], Step [301/390], Loss: 4.2870\n",
      "Epoch [1/10], Step [302/390], Loss: 4.1845\n",
      "Epoch [1/10], Step [303/390], Loss: 4.1897\n",
      "Epoch [1/10], Step [304/390], Loss: 4.2722\n",
      "Epoch [1/10], Step [305/390], Loss: 4.0548\n",
      "Epoch [1/10], Step [306/390], Loss: 4.1472\n",
      "Epoch [1/10], Step [307/390], Loss: 4.0318\n",
      "Epoch [1/10], Step [308/390], Loss: 4.1421\n",
      "Epoch [1/10], Step [309/390], Loss: 4.0816\n",
      "Epoch [1/10], Step [310/390], Loss: 4.2369\n",
      "Epoch [1/10], Step [311/390], Loss: 4.0978\n",
      "Epoch [1/10], Step [312/390], Loss: 4.1781\n",
      "Epoch [1/10], Step [313/390], Loss: 4.0698\n",
      "Epoch [1/10], Step [314/390], Loss: 4.2143\n",
      "Epoch [1/10], Step [315/390], Loss: 4.1557\n",
      "Epoch [1/10], Step [316/390], Loss: 4.0827\n",
      "Epoch [1/10], Step [317/390], Loss: 4.1906\n",
      "Epoch [1/10], Step [318/390], Loss: 4.2302\n",
      "Epoch [1/10], Step [319/390], Loss: 4.0457\n",
      "Epoch [1/10], Step [320/390], Loss: 4.1710\n",
      "Epoch [1/10], Step [321/390], Loss: 4.0819\n",
      "Epoch [1/10], Step [322/390], Loss: 4.1593\n",
      "Epoch [1/10], Step [323/390], Loss: 4.1591\n",
      "Epoch [1/10], Step [324/390], Loss: 4.0900\n",
      "Epoch [1/10], Step [325/390], Loss: 4.1841\n",
      "Epoch [1/10], Step [326/390], Loss: 4.0223\n",
      "Epoch [1/10], Step [327/390], Loss: 4.0702\n",
      "Epoch [1/10], Step [328/390], Loss: 4.0539\n",
      "Epoch [1/10], Step [329/390], Loss: 3.8921\n",
      "Epoch [1/10], Step [330/390], Loss: 4.1627\n",
      "Epoch [1/10], Step [331/390], Loss: 3.9873\n",
      "Epoch [1/10], Step [332/390], Loss: 4.0196\n",
      "Epoch [1/10], Step [333/390], Loss: 4.2369\n",
      "Epoch [1/10], Step [334/390], Loss: 3.9636\n",
      "Epoch [1/10], Step [335/390], Loss: 4.0684\n",
      "Epoch [1/10], Step [336/390], Loss: 4.2238\n",
      "Epoch [1/10], Step [337/390], Loss: 4.1759\n",
      "Epoch [1/10], Step [338/390], Loss: 4.2652\n",
      "Epoch [1/10], Step [339/390], Loss: 4.0685\n",
      "Epoch [1/10], Step [340/390], Loss: 4.2681\n",
      "Epoch [1/10], Step [341/390], Loss: 3.9598\n",
      "Epoch [1/10], Step [342/390], Loss: 4.0944\n",
      "Epoch [1/10], Step [343/390], Loss: 4.2254\n",
      "Epoch [1/10], Step [344/390], Loss: 3.9795\n",
      "Epoch [1/10], Step [345/390], Loss: 4.1856\n",
      "Epoch [1/10], Step [346/390], Loss: 4.0339\n",
      "Epoch [1/10], Step [347/390], Loss: 3.9633\n",
      "Epoch [1/10], Step [348/390], Loss: 4.1334\n",
      "Epoch [1/10], Step [349/390], Loss: 4.1481\n",
      "Epoch [1/10], Step [350/390], Loss: 4.2088\n",
      "Epoch [1/10], Step [351/390], Loss: 3.8651\n",
      "Epoch [1/10], Step [352/390], Loss: 4.1972\n",
      "Epoch [1/10], Step [353/390], Loss: 3.9697\n",
      "Epoch [1/10], Step [354/390], Loss: 3.9095\n",
      "Epoch [1/10], Step [355/390], Loss: 3.9139\n",
      "Epoch [1/10], Step [356/390], Loss: 3.9950\n",
      "Epoch [1/10], Step [357/390], Loss: 4.0315\n",
      "Epoch [1/10], Step [358/390], Loss: 4.0256\n",
      "Epoch [1/10], Step [359/390], Loss: 4.2044\n",
      "Epoch [1/10], Step [360/390], Loss: 4.0754\n",
      "Epoch [1/10], Step [361/390], Loss: 4.0638\n",
      "Epoch [1/10], Step [362/390], Loss: 4.1064\n",
      "Epoch [1/10], Step [363/390], Loss: 4.0878\n",
      "Epoch [1/10], Step [364/390], Loss: 4.0604\n",
      "Epoch [1/10], Step [365/390], Loss: 4.0693\n",
      "Epoch [1/10], Step [366/390], Loss: 4.0263\n",
      "Epoch [1/10], Step [367/390], Loss: 3.9700\n",
      "Epoch [1/10], Step [368/390], Loss: 4.1643\n",
      "Epoch [1/10], Step [369/390], Loss: 4.1523\n",
      "Epoch [1/10], Step [370/390], Loss: 4.1572\n",
      "Epoch [1/10], Step [371/390], Loss: 4.0026\n",
      "Epoch [1/10], Step [372/390], Loss: 4.1117\n",
      "Epoch [1/10], Step [373/390], Loss: 3.8314\n",
      "Epoch [1/10], Step [374/390], Loss: 3.8879\n",
      "Epoch [1/10], Step [375/390], Loss: 4.0454\n",
      "Epoch [1/10], Step [376/390], Loss: 3.8194\n",
      "Epoch [1/10], Step [377/390], Loss: 3.9826\n",
      "Epoch [1/10], Step [378/390], Loss: 4.0179\n",
      "Epoch [1/10], Step [379/390], Loss: 4.1207\n",
      "Epoch [1/10], Step [380/390], Loss: 4.1023\n",
      "Epoch [1/10], Step [381/390], Loss: 4.0289\n",
      "Epoch [1/10], Step [382/390], Loss: 4.0699\n",
      "Epoch [1/10], Step [383/390], Loss: 4.1879\n",
      "Epoch [1/10], Step [384/390], Loss: 4.0481\n",
      "Epoch [1/10], Step [385/390], Loss: 4.0014\n",
      "Epoch [1/10], Step [386/390], Loss: 4.1400\n",
      "Epoch [1/10], Step [387/390], Loss: 4.1435\n",
      "Epoch [1/10], Step [388/390], Loss: 4.1492\n",
      "Epoch [1/10], Step [389/390], Loss: 4.0253\n",
      "Epoch [1/10], Step [390/390], Loss: 3.9988\n",
      "Epoch [1/10], Step [391/390], Loss: 3.9741\n",
      "Epoch [2/10], Step [1/390], Loss: 3.8678\n",
      "Epoch [2/10], Step [2/390], Loss: 4.0745\n",
      "Epoch [2/10], Step [3/390], Loss: 4.0940\n",
      "Epoch [2/10], Step [4/390], Loss: 4.0793\n",
      "Epoch [2/10], Step [5/390], Loss: 3.8361\n",
      "Epoch [2/10], Step [6/390], Loss: 3.8376\n",
      "Epoch [2/10], Step [7/390], Loss: 3.9516\n",
      "Epoch [2/10], Step [8/390], Loss: 4.1055\n",
      "Epoch [2/10], Step [9/390], Loss: 3.9787\n",
      "Epoch [2/10], Step [10/390], Loss: 3.9551\n",
      "Epoch [2/10], Step [11/390], Loss: 4.1287\n",
      "Epoch [2/10], Step [12/390], Loss: 4.0329\n",
      "Epoch [2/10], Step [13/390], Loss: 3.9576\n",
      "Epoch [2/10], Step [14/390], Loss: 3.8590\n",
      "Epoch [2/10], Step [15/390], Loss: 4.0074\n",
      "Epoch [2/10], Step [16/390], Loss: 3.9821\n",
      "Epoch [2/10], Step [17/390], Loss: 4.0151\n",
      "Epoch [2/10], Step [18/390], Loss: 3.9499\n",
      "Epoch [2/10], Step [19/390], Loss: 4.0662\n",
      "Epoch [2/10], Step [20/390], Loss: 3.9579\n",
      "Epoch [2/10], Step [21/390], Loss: 3.8846\n",
      "Epoch [2/10], Step [22/390], Loss: 4.0417\n",
      "Epoch [2/10], Step [23/390], Loss: 4.0581\n",
      "Epoch [2/10], Step [24/390], Loss: 3.9593\n",
      "Epoch [2/10], Step [25/390], Loss: 4.2035\n",
      "Epoch [2/10], Step [26/390], Loss: 3.9732\n",
      "Epoch [2/10], Step [27/390], Loss: 4.0312\n",
      "Epoch [2/10], Step [28/390], Loss: 3.9544\n",
      "Epoch [2/10], Step [29/390], Loss: 3.9125\n",
      "Epoch [2/10], Step [30/390], Loss: 4.1450\n",
      "Epoch [2/10], Step [31/390], Loss: 3.8661\n",
      "Epoch [2/10], Step [32/390], Loss: 4.1827\n",
      "Epoch [2/10], Step [33/390], Loss: 3.9877\n",
      "Epoch [2/10], Step [34/390], Loss: 4.0578\n",
      "Epoch [2/10], Step [35/390], Loss: 3.9730\n",
      "Epoch [2/10], Step [36/390], Loss: 4.0679\n",
      "Epoch [2/10], Step [37/390], Loss: 3.8481\n",
      "Epoch [2/10], Step [38/390], Loss: 3.9900\n",
      "Epoch [2/10], Step [39/390], Loss: 3.9304\n",
      "Epoch [2/10], Step [40/390], Loss: 4.0953\n",
      "Epoch [2/10], Step [41/390], Loss: 3.9403\n",
      "Epoch [2/10], Step [42/390], Loss: 4.0163\n",
      "Epoch [2/10], Step [43/390], Loss: 3.9146\n",
      "Epoch [2/10], Step [44/390], Loss: 3.9148\n",
      "Epoch [2/10], Step [45/390], Loss: 3.8594\n",
      "Epoch [2/10], Step [46/390], Loss: 3.9475\n",
      "Epoch [2/10], Step [47/390], Loss: 4.0363\n",
      "Epoch [2/10], Step [48/390], Loss: 4.1168\n",
      "Epoch [2/10], Step [49/390], Loss: 3.9299\n",
      "Epoch [2/10], Step [50/390], Loss: 3.9432\n",
      "Epoch [2/10], Step [51/390], Loss: 4.0530\n",
      "Epoch [2/10], Step [52/390], Loss: 4.0436\n",
      "Epoch [2/10], Step [53/390], Loss: 4.0863\n",
      "Epoch [2/10], Step [54/390], Loss: 3.9304\n",
      "Epoch [2/10], Step [55/390], Loss: 3.8518\n",
      "Epoch [2/10], Step [56/390], Loss: 3.9190\n",
      "Epoch [2/10], Step [57/390], Loss: 3.9246\n",
      "Epoch [2/10], Step [58/390], Loss: 4.0183\n",
      "Epoch [2/10], Step [59/390], Loss: 3.9410\n",
      "Epoch [2/10], Step [60/390], Loss: 4.0049\n",
      "Epoch [2/10], Step [61/390], Loss: 3.7806\n",
      "Epoch [2/10], Step [62/390], Loss: 3.8036\n",
      "Epoch [2/10], Step [63/390], Loss: 3.7675\n",
      "Epoch [2/10], Step [64/390], Loss: 3.8281\n",
      "Epoch [2/10], Step [65/390], Loss: 4.1458\n",
      "Epoch [2/10], Step [66/390], Loss: 3.9912\n",
      "Epoch [2/10], Step [67/390], Loss: 4.0595\n",
      "Epoch [2/10], Step [68/390], Loss: 3.7165\n",
      "Epoch [2/10], Step [69/390], Loss: 3.9165\n",
      "Epoch [2/10], Step [70/390], Loss: 3.9079\n",
      "Epoch [2/10], Step [71/390], Loss: 3.8692\n",
      "Epoch [2/10], Step [72/390], Loss: 3.9783\n",
      "Epoch [2/10], Step [73/390], Loss: 3.8226\n",
      "Epoch [2/10], Step [74/390], Loss: 4.0266\n",
      "Epoch [2/10], Step [75/390], Loss: 4.0104\n",
      "Epoch [2/10], Step [76/390], Loss: 3.9477\n",
      "Epoch [2/10], Step [77/390], Loss: 4.1405\n",
      "Epoch [2/10], Step [78/390], Loss: 3.9810\n",
      "Epoch [2/10], Step [79/390], Loss: 3.8345\n",
      "Epoch [2/10], Step [80/390], Loss: 3.8774\n",
      "Epoch [2/10], Step [81/390], Loss: 3.8873\n",
      "Epoch [2/10], Step [82/390], Loss: 3.9220\n",
      "Epoch [2/10], Step [83/390], Loss: 3.9452\n",
      "Epoch [2/10], Step [84/390], Loss: 3.9362\n",
      "Epoch [2/10], Step [85/390], Loss: 3.8908\n",
      "Epoch [2/10], Step [86/390], Loss: 3.8477\n",
      "Epoch [2/10], Step [87/390], Loss: 3.9973\n",
      "Epoch [2/10], Step [88/390], Loss: 3.9981\n",
      "Epoch [2/10], Step [89/390], Loss: 3.8516\n",
      "Epoch [2/10], Step [90/390], Loss: 3.7434\n",
      "Epoch [2/10], Step [91/390], Loss: 3.8660\n",
      "Epoch [2/10], Step [92/390], Loss: 3.9811\n",
      "Epoch [2/10], Step [93/390], Loss: 3.8315\n",
      "Epoch [2/10], Step [94/390], Loss: 3.8348\n",
      "Epoch [2/10], Step [95/390], Loss: 4.0078\n",
      "Epoch [2/10], Step [96/390], Loss: 3.7082\n",
      "Epoch [2/10], Step [97/390], Loss: 3.8484\n",
      "Epoch [2/10], Step [98/390], Loss: 3.7530\n",
      "Epoch [2/10], Step [99/390], Loss: 3.7716\n",
      "Epoch [2/10], Step [100/390], Loss: 3.7419\n",
      "Epoch [2/10], Step [101/390], Loss: 3.7279\n",
      "Epoch [2/10], Step [102/390], Loss: 3.5718\n",
      "Epoch [2/10], Step [103/390], Loss: 3.9222\n",
      "Epoch [2/10], Step [104/390], Loss: 3.8496\n",
      "Epoch [2/10], Step [105/390], Loss: 3.8740\n",
      "Epoch [2/10], Step [106/390], Loss: 3.9469\n",
      "Epoch [2/10], Step [107/390], Loss: 3.7759\n",
      "Epoch [2/10], Step [108/390], Loss: 3.8845\n",
      "Epoch [2/10], Step [109/390], Loss: 3.7438\n",
      "Epoch [2/10], Step [110/390], Loss: 3.6181\n",
      "Epoch [2/10], Step [111/390], Loss: 3.7302\n",
      "Epoch [2/10], Step [112/390], Loss: 3.7626\n",
      "Epoch [2/10], Step [113/390], Loss: 3.8636\n",
      "Epoch [2/10], Step [114/390], Loss: 3.8412\n",
      "Epoch [2/10], Step [115/390], Loss: 3.7872\n",
      "Epoch [2/10], Step [116/390], Loss: 3.7951\n",
      "Epoch [2/10], Step [117/390], Loss: 3.8844\n",
      "Epoch [2/10], Step [118/390], Loss: 3.8480\n",
      "Epoch [2/10], Step [119/390], Loss: 3.8364\n",
      "Epoch [2/10], Step [120/390], Loss: 3.8793\n",
      "Epoch [2/10], Step [121/390], Loss: 3.8562\n",
      "Epoch [2/10], Step [122/390], Loss: 3.8158\n",
      "Epoch [2/10], Step [123/390], Loss: 3.6381\n",
      "Epoch [2/10], Step [124/390], Loss: 3.8868\n",
      "Epoch [2/10], Step [125/390], Loss: 3.7542\n",
      "Epoch [2/10], Step [126/390], Loss: 3.9475\n",
      "Epoch [2/10], Step [127/390], Loss: 3.8458\n",
      "Epoch [2/10], Step [128/390], Loss: 3.9194\n",
      "Epoch [2/10], Step [129/390], Loss: 3.9299\n",
      "Epoch [2/10], Step [130/390], Loss: 3.8001\n",
      "Epoch [2/10], Step [131/390], Loss: 3.8023\n",
      "Epoch [2/10], Step [132/390], Loss: 3.8132\n",
      "Epoch [2/10], Step [133/390], Loss: 3.6758\n",
      "Epoch [2/10], Step [134/390], Loss: 3.8398\n",
      "Epoch [2/10], Step [135/390], Loss: 3.7595\n",
      "Epoch [2/10], Step [136/390], Loss: 3.7588\n",
      "Epoch [2/10], Step [137/390], Loss: 3.9129\n",
      "Epoch [2/10], Step [138/390], Loss: 3.8589\n",
      "Epoch [2/10], Step [139/390], Loss: 3.8026\n",
      "Epoch [2/10], Step [140/390], Loss: 3.9103\n",
      "Epoch [2/10], Step [141/390], Loss: 3.8768\n",
      "Epoch [2/10], Step [142/390], Loss: 3.8917\n",
      "Epoch [2/10], Step [143/390], Loss: 3.9622\n",
      "Epoch [2/10], Step [144/390], Loss: 3.8942\n",
      "Epoch [2/10], Step [145/390], Loss: 3.7277\n",
      "Epoch [2/10], Step [146/390], Loss: 3.7736\n",
      "Epoch [2/10], Step [147/390], Loss: 3.7305\n",
      "Epoch [2/10], Step [148/390], Loss: 3.8024\n",
      "Epoch [2/10], Step [149/390], Loss: 3.8199\n",
      "Epoch [2/10], Step [150/390], Loss: 3.7890\n",
      "Epoch [2/10], Step [151/390], Loss: 3.7462\n",
      "Epoch [2/10], Step [152/390], Loss: 3.9201\n",
      "Epoch [2/10], Step [153/390], Loss: 3.8255\n",
      "Epoch [2/10], Step [154/390], Loss: 3.8303\n",
      "Epoch [2/10], Step [155/390], Loss: 3.9212\n",
      "Epoch [2/10], Step [156/390], Loss: 3.7755\n",
      "Epoch [2/10], Step [157/390], Loss: 3.8065\n",
      "Epoch [2/10], Step [158/390], Loss: 3.6627\n",
      "Epoch [2/10], Step [159/390], Loss: 3.6716\n",
      "Epoch [2/10], Step [160/390], Loss: 3.7650\n",
      "Epoch [2/10], Step [161/390], Loss: 3.8118\n",
      "Epoch [2/10], Step [162/390], Loss: 3.6398\n",
      "Epoch [2/10], Step [163/390], Loss: 3.7445\n",
      "Epoch [2/10], Step [164/390], Loss: 3.7653\n",
      "Epoch [2/10], Step [165/390], Loss: 3.8454\n",
      "Epoch [2/10], Step [166/390], Loss: 3.6307\n",
      "Epoch [2/10], Step [167/390], Loss: 3.7738\n",
      "Epoch [2/10], Step [168/390], Loss: 3.6783\n",
      "Epoch [2/10], Step [169/390], Loss: 3.6352\n",
      "Epoch [2/10], Step [170/390], Loss: 3.5632\n",
      "Epoch [2/10], Step [171/390], Loss: 3.9476\n",
      "Epoch [2/10], Step [172/390], Loss: 3.5111\n",
      "Epoch [2/10], Step [173/390], Loss: 3.6649\n",
      "Epoch [2/10], Step [174/390], Loss: 3.8383\n",
      "Epoch [2/10], Step [175/390], Loss: 3.9211\n",
      "Epoch [2/10], Step [176/390], Loss: 3.6184\n",
      "Epoch [2/10], Step [177/390], Loss: 3.8916\n",
      "Epoch [2/10], Step [178/390], Loss: 3.7428\n",
      "Epoch [2/10], Step [179/390], Loss: 3.7731\n",
      "Epoch [2/10], Step [180/390], Loss: 3.7713\n",
      "Epoch [2/10], Step [181/390], Loss: 3.8202\n",
      "Epoch [2/10], Step [182/390], Loss: 3.9434\n",
      "Epoch [2/10], Step [183/390], Loss: 3.6725\n",
      "Epoch [2/10], Step [184/390], Loss: 3.5475\n",
      "Epoch [2/10], Step [185/390], Loss: 3.5829\n",
      "Epoch [2/10], Step [186/390], Loss: 3.6530\n",
      "Epoch [2/10], Step [187/390], Loss: 3.6956\n",
      "Epoch [2/10], Step [188/390], Loss: 3.3631\n",
      "Epoch [2/10], Step [189/390], Loss: 3.4940\n",
      "Epoch [2/10], Step [190/390], Loss: 3.6202\n",
      "Epoch [2/10], Step [191/390], Loss: 3.7054\n",
      "Epoch [2/10], Step [192/390], Loss: 3.6422\n",
      "Epoch [2/10], Step [193/390], Loss: 3.8234\n",
      "Epoch [2/10], Step [194/390], Loss: 3.6597\n",
      "Epoch [2/10], Step [195/390], Loss: 3.6471\n",
      "Epoch [2/10], Step [196/390], Loss: 3.7002\n",
      "Epoch [2/10], Step [197/390], Loss: 3.9062\n",
      "Epoch [2/10], Step [198/390], Loss: 3.8217\n",
      "Epoch [2/10], Step [199/390], Loss: 3.4859\n",
      "Epoch [2/10], Step [200/390], Loss: 3.4995\n",
      "Epoch [2/10], Step [201/390], Loss: 3.7485\n",
      "Epoch [2/10], Step [202/390], Loss: 3.7344\n",
      "Epoch [2/10], Step [203/390], Loss: 3.6000\n",
      "Epoch [2/10], Step [204/390], Loss: 3.5663\n",
      "Epoch [2/10], Step [205/390], Loss: 3.7107\n",
      "Epoch [2/10], Step [206/390], Loss: 3.7781\n",
      "Epoch [2/10], Step [207/390], Loss: 3.7708\n",
      "Epoch [2/10], Step [208/390], Loss: 3.7039\n",
      "Epoch [2/10], Step [209/390], Loss: 3.6367\n",
      "Epoch [2/10], Step [210/390], Loss: 3.7243\n",
      "Epoch [2/10], Step [211/390], Loss: 3.4792\n",
      "Epoch [2/10], Step [212/390], Loss: 3.8760\n",
      "Epoch [2/10], Step [213/390], Loss: 3.7648\n",
      "Epoch [2/10], Step [214/390], Loss: 3.7581\n",
      "Epoch [2/10], Step [215/390], Loss: 3.6472\n",
      "Epoch [2/10], Step [216/390], Loss: 3.4849\n",
      "Epoch [2/10], Step [217/390], Loss: 3.7174\n",
      "Epoch [2/10], Step [218/390], Loss: 3.7095\n",
      "Epoch [2/10], Step [219/390], Loss: 3.6669\n",
      "Epoch [2/10], Step [220/390], Loss: 3.5207\n",
      "Epoch [2/10], Step [221/390], Loss: 3.5886\n",
      "Epoch [2/10], Step [222/390], Loss: 3.5624\n",
      "Epoch [2/10], Step [223/390], Loss: 3.5986\n",
      "Epoch [2/10], Step [224/390], Loss: 3.6913\n",
      "Epoch [2/10], Step [225/390], Loss: 3.5843\n",
      "Epoch [2/10], Step [226/390], Loss: 3.9002\n",
      "Epoch [2/10], Step [227/390], Loss: 3.6709\n",
      "Epoch [2/10], Step [228/390], Loss: 3.6044\n",
      "Epoch [2/10], Step [229/390], Loss: 3.6090\n",
      "Epoch [2/10], Step [230/390], Loss: 3.7103\n",
      "Epoch [2/10], Step [231/390], Loss: 3.5933\n",
      "Epoch [2/10], Step [232/390], Loss: 3.7545\n",
      "Epoch [2/10], Step [233/390], Loss: 3.8215\n",
      "Epoch [2/10], Step [234/390], Loss: 3.6354\n",
      "Epoch [2/10], Step [235/390], Loss: 3.6892\n",
      "Epoch [2/10], Step [236/390], Loss: 3.6841\n",
      "Epoch [2/10], Step [237/390], Loss: 3.7274\n",
      "Epoch [2/10], Step [238/390], Loss: 3.7534\n",
      "Epoch [2/10], Step [239/390], Loss: 3.6581\n",
      "Epoch [2/10], Step [240/390], Loss: 3.6617\n",
      "Epoch [2/10], Step [241/390], Loss: 3.7006\n",
      "Epoch [2/10], Step [242/390], Loss: 3.5861\n",
      "Epoch [2/10], Step [243/390], Loss: 3.6919\n",
      "Epoch [2/10], Step [244/390], Loss: 3.5073\n",
      "Epoch [2/10], Step [245/390], Loss: 3.6325\n",
      "Epoch [2/10], Step [246/390], Loss: 3.5386\n",
      "Epoch [2/10], Step [247/390], Loss: 3.5801\n",
      "Epoch [2/10], Step [248/390], Loss: 3.7509\n",
      "Epoch [2/10], Step [249/390], Loss: 3.3864\n",
      "Epoch [2/10], Step [250/390], Loss: 3.5894\n",
      "Epoch [2/10], Step [251/390], Loss: 3.4983\n",
      "Epoch [2/10], Step [252/390], Loss: 3.6071\n",
      "Epoch [2/10], Step [253/390], Loss: 3.4988\n",
      "Epoch [2/10], Step [254/390], Loss: 3.5610\n",
      "Epoch [2/10], Step [255/390], Loss: 3.7627\n",
      "Epoch [2/10], Step [256/390], Loss: 3.6629\n",
      "Epoch [2/10], Step [257/390], Loss: 3.5853\n",
      "Epoch [2/10], Step [258/390], Loss: 3.6541\n",
      "Epoch [2/10], Step [259/390], Loss: 3.7072\n",
      "Epoch [2/10], Step [260/390], Loss: 3.3583\n",
      "Epoch [2/10], Step [261/390], Loss: 3.7672\n",
      "Epoch [2/10], Step [262/390], Loss: 3.5299\n",
      "Epoch [2/10], Step [263/390], Loss: 3.6996\n",
      "Epoch [2/10], Step [264/390], Loss: 3.4730\n",
      "Epoch [2/10], Step [265/390], Loss: 3.5996\n",
      "Epoch [2/10], Step [266/390], Loss: 3.6521\n",
      "Epoch [2/10], Step [267/390], Loss: 3.5464\n",
      "Epoch [2/10], Step [268/390], Loss: 3.8591\n",
      "Epoch [2/10], Step [269/390], Loss: 3.4227\n",
      "Epoch [2/10], Step [270/390], Loss: 3.7665\n",
      "Epoch [2/10], Step [271/390], Loss: 3.5564\n",
      "Epoch [2/10], Step [272/390], Loss: 3.3910\n",
      "Epoch [2/10], Step [273/390], Loss: 3.3943\n",
      "Epoch [2/10], Step [274/390], Loss: 3.7364\n",
      "Epoch [2/10], Step [275/390], Loss: 3.7241\n",
      "Epoch [2/10], Step [276/390], Loss: 3.5636\n",
      "Epoch [2/10], Step [277/390], Loss: 3.4336\n",
      "Epoch [2/10], Step [278/390], Loss: 3.3717\n",
      "Epoch [2/10], Step [279/390], Loss: 3.7251\n",
      "Epoch [2/10], Step [280/390], Loss: 3.5917\n",
      "Epoch [2/10], Step [281/390], Loss: 3.3022\n",
      "Epoch [2/10], Step [282/390], Loss: 3.4503\n",
      "Epoch [2/10], Step [283/390], Loss: 3.5356\n",
      "Epoch [2/10], Step [284/390], Loss: 3.4893\n",
      "Epoch [2/10], Step [285/390], Loss: 3.7190\n",
      "Epoch [2/10], Step [286/390], Loss: 3.4117\n",
      "Epoch [2/10], Step [287/390], Loss: 3.6861\n",
      "Epoch [2/10], Step [288/390], Loss: 3.5144\n",
      "Epoch [2/10], Step [289/390], Loss: 3.7082\n",
      "Epoch [2/10], Step [290/390], Loss: 3.3797\n",
      "Epoch [2/10], Step [291/390], Loss: 3.5918\n",
      "Epoch [2/10], Step [292/390], Loss: 3.7868\n",
      "Epoch [2/10], Step [293/390], Loss: 3.6076\n",
      "Epoch [2/10], Step [294/390], Loss: 3.5542\n",
      "Epoch [2/10], Step [295/390], Loss: 3.4586\n",
      "Epoch [2/10], Step [296/390], Loss: 3.6971\n",
      "Epoch [2/10], Step [297/390], Loss: 3.5142\n",
      "Epoch [2/10], Step [298/390], Loss: 3.6565\n",
      "Epoch [2/10], Step [299/390], Loss: 3.6224\n",
      "Epoch [2/10], Step [300/390], Loss: 3.5081\n",
      "Epoch [2/10], Step [301/390], Loss: 3.5352\n",
      "Epoch [2/10], Step [302/390], Loss: 3.6090\n",
      "Epoch [2/10], Step [303/390], Loss: 3.5614\n",
      "Epoch [2/10], Step [304/390], Loss: 3.6976\n",
      "Epoch [2/10], Step [305/390], Loss: 3.3879\n",
      "Epoch [2/10], Step [306/390], Loss: 3.4902\n",
      "Epoch [2/10], Step [307/390], Loss: 3.2673\n",
      "Epoch [2/10], Step [308/390], Loss: 3.6403\n",
      "Epoch [2/10], Step [309/390], Loss: 3.5756\n",
      "Epoch [2/10], Step [310/390], Loss: 3.5745\n",
      "Epoch [2/10], Step [311/390], Loss: 3.7581\n",
      "Epoch [2/10], Step [312/390], Loss: 3.2863\n",
      "Epoch [2/10], Step [313/390], Loss: 3.4870\n",
      "Epoch [2/10], Step [314/390], Loss: 3.5898\n",
      "Epoch [2/10], Step [315/390], Loss: 3.3707\n",
      "Epoch [2/10], Step [316/390], Loss: 3.5406\n",
      "Epoch [2/10], Step [317/390], Loss: 3.6423\n",
      "Epoch [2/10], Step [318/390], Loss: 3.5748\n",
      "Epoch [2/10], Step [319/390], Loss: 3.3166\n",
      "Epoch [2/10], Step [320/390], Loss: 3.7488\n",
      "Epoch [2/10], Step [321/390], Loss: 3.4139\n",
      "Epoch [2/10], Step [322/390], Loss: 3.4867\n",
      "Epoch [2/10], Step [323/390], Loss: 3.4099\n",
      "Epoch [2/10], Step [324/390], Loss: 3.6407\n",
      "Epoch [2/10], Step [325/390], Loss: 3.5260\n",
      "Epoch [2/10], Step [326/390], Loss: 3.4506\n",
      "Epoch [2/10], Step [327/390], Loss: 3.7104\n",
      "Epoch [2/10], Step [328/390], Loss: 3.5572\n",
      "Epoch [2/10], Step [329/390], Loss: 3.4397\n",
      "Epoch [2/10], Step [330/390], Loss: 3.4795\n",
      "Epoch [2/10], Step [331/390], Loss: 3.5978\n",
      "Epoch [2/10], Step [332/390], Loss: 3.4191\n",
      "Epoch [2/10], Step [333/390], Loss: 3.6387\n",
      "Epoch [2/10], Step [334/390], Loss: 3.2501\n",
      "Epoch [2/10], Step [335/390], Loss: 3.4651\n",
      "Epoch [2/10], Step [336/390], Loss: 3.4766\n",
      "Epoch [2/10], Step [337/390], Loss: 3.4587\n",
      "Epoch [2/10], Step [338/390], Loss: 3.5946\n",
      "Epoch [2/10], Step [339/390], Loss: 3.3043\n",
      "Epoch [2/10], Step [340/390], Loss: 3.7702\n",
      "Epoch [2/10], Step [341/390], Loss: 3.5594\n",
      "Epoch [2/10], Step [342/390], Loss: 3.4455\n",
      "Epoch [2/10], Step [343/390], Loss: 3.5564\n",
      "Epoch [2/10], Step [344/390], Loss: 3.5041\n",
      "Epoch [2/10], Step [345/390], Loss: 3.5885\n",
      "Epoch [2/10], Step [346/390], Loss: 3.4027\n",
      "Epoch [2/10], Step [347/390], Loss: 3.4097\n",
      "Epoch [2/10], Step [348/390], Loss: 3.4986\n",
      "Epoch [2/10], Step [349/390], Loss: 3.5013\n",
      "Epoch [2/10], Step [350/390], Loss: 3.7619\n",
      "Epoch [2/10], Step [351/390], Loss: 3.2557\n",
      "Epoch [2/10], Step [352/390], Loss: 3.6290\n",
      "Epoch [2/10], Step [353/390], Loss: 3.5393\n",
      "Epoch [2/10], Step [354/390], Loss: 3.4125\n",
      "Epoch [2/10], Step [355/390], Loss: 3.5222\n",
      "Epoch [2/10], Step [356/390], Loss: 3.4556\n",
      "Epoch [2/10], Step [357/390], Loss: 3.6241\n",
      "Epoch [2/10], Step [358/390], Loss: 3.6138\n",
      "Epoch [2/10], Step [359/390], Loss: 3.4644\n",
      "Epoch [2/10], Step [360/390], Loss: 3.4605\n",
      "Epoch [2/10], Step [361/390], Loss: 3.4490\n",
      "Epoch [2/10], Step [362/390], Loss: 3.1823\n",
      "Epoch [2/10], Step [363/390], Loss: 3.4441\n",
      "Epoch [2/10], Step [364/390], Loss: 3.5371\n",
      "Epoch [2/10], Step [365/390], Loss: 3.4684\n",
      "Epoch [2/10], Step [366/390], Loss: 3.5900\n",
      "Epoch [2/10], Step [367/390], Loss: 3.4183\n",
      "Epoch [2/10], Step [368/390], Loss: 3.4676\n",
      "Epoch [2/10], Step [369/390], Loss: 3.3870\n",
      "Epoch [2/10], Step [370/390], Loss: 3.5328\n",
      "Epoch [2/10], Step [371/390], Loss: 3.4403\n",
      "Epoch [2/10], Step [372/390], Loss: 3.5362\n",
      "Epoch [2/10], Step [373/390], Loss: 3.6651\n",
      "Epoch [2/10], Step [374/390], Loss: 3.5177\n",
      "Epoch [2/10], Step [375/390], Loss: 3.2317\n",
      "Epoch [2/10], Step [376/390], Loss: 3.4278\n",
      "Epoch [2/10], Step [377/390], Loss: 3.4602\n",
      "Epoch [2/10], Step [378/390], Loss: 3.4544\n",
      "Epoch [2/10], Step [379/390], Loss: 3.4400\n",
      "Epoch [2/10], Step [380/390], Loss: 3.3344\n",
      "Epoch [2/10], Step [381/390], Loss: 3.5282\n",
      "Epoch [2/10], Step [382/390], Loss: 3.3188\n",
      "Epoch [2/10], Step [383/390], Loss: 3.4094\n",
      "Epoch [2/10], Step [384/390], Loss: 3.5366\n",
      "Epoch [2/10], Step [385/390], Loss: 3.5932\n",
      "Epoch [2/10], Step [386/390], Loss: 3.3389\n",
      "Epoch [2/10], Step [387/390], Loss: 3.4241\n",
      "Epoch [2/10], Step [388/390], Loss: 3.5358\n",
      "Epoch [2/10], Step [389/390], Loss: 3.3933\n",
      "Epoch [2/10], Step [390/390], Loss: 3.4042\n",
      "Epoch [2/10], Step [391/390], Loss: 3.4201\n",
      "Epoch [3/10], Step [1/390], Loss: 3.5700\n",
      "Epoch [3/10], Step [2/390], Loss: 3.5539\n",
      "Epoch [3/10], Step [3/390], Loss: 3.4382\n",
      "Epoch [3/10], Step [4/390], Loss: 3.4311\n",
      "Epoch [3/10], Step [5/390], Loss: 3.3752\n",
      "Epoch [3/10], Step [6/390], Loss: 3.3230\n",
      "Epoch [3/10], Step [7/390], Loss: 3.1902\n",
      "Epoch [3/10], Step [8/390], Loss: 3.1557\n",
      "Epoch [3/10], Step [9/390], Loss: 3.4344\n",
      "Epoch [3/10], Step [10/390], Loss: 3.3093\n",
      "Epoch [3/10], Step [11/390], Loss: 3.2355\n",
      "Epoch [3/10], Step [12/390], Loss: 3.4365\n",
      "Epoch [3/10], Step [13/390], Loss: 3.1605\n",
      "Epoch [3/10], Step [14/390], Loss: 3.4356\n",
      "Epoch [3/10], Step [15/390], Loss: 3.3669\n",
      "Epoch [3/10], Step [16/390], Loss: 3.2368\n",
      "Epoch [3/10], Step [17/390], Loss: 3.3700\n",
      "Epoch [3/10], Step [18/390], Loss: 3.4644\n",
      "Epoch [3/10], Step [19/390], Loss: 3.3190\n",
      "Epoch [3/10], Step [20/390], Loss: 3.3965\n",
      "Epoch [3/10], Step [21/390], Loss: 3.5373\n",
      "Epoch [3/10], Step [22/390], Loss: 3.4243\n",
      "Epoch [3/10], Step [23/390], Loss: 3.4805\n",
      "Epoch [3/10], Step [24/390], Loss: 3.3851\n",
      "Epoch [3/10], Step [25/390], Loss: 3.4356\n",
      "Epoch [3/10], Step [26/390], Loss: 3.4094\n",
      "Epoch [3/10], Step [27/390], Loss: 3.4677\n",
      "Epoch [3/10], Step [28/390], Loss: 3.3846\n",
      "Epoch [3/10], Step [29/390], Loss: 2.9790\n",
      "Epoch [3/10], Step [30/390], Loss: 3.2908\n",
      "Epoch [3/10], Step [31/390], Loss: 3.4348\n",
      "Epoch [3/10], Step [32/390], Loss: 3.2714\n",
      "Epoch [3/10], Step [33/390], Loss: 3.1377\n",
      "Epoch [3/10], Step [34/390], Loss: 3.3321\n",
      "Epoch [3/10], Step [35/390], Loss: 3.5244\n",
      "Epoch [3/10], Step [36/390], Loss: 3.3180\n",
      "Epoch [3/10], Step [37/390], Loss: 3.4755\n",
      "Epoch [3/10], Step [38/390], Loss: 3.2932\n",
      "Epoch [3/10], Step [39/390], Loss: 3.4086\n",
      "Epoch [3/10], Step [40/390], Loss: 3.4966\n",
      "Epoch [3/10], Step [41/390], Loss: 3.3446\n",
      "Epoch [3/10], Step [42/390], Loss: 3.4637\n",
      "Epoch [3/10], Step [43/390], Loss: 3.2098\n",
      "Epoch [3/10], Step [44/390], Loss: 3.1382\n",
      "Epoch [3/10], Step [45/390], Loss: 3.3809\n",
      "Epoch [3/10], Step [46/390], Loss: 3.4409\n",
      "Epoch [3/10], Step [47/390], Loss: 3.1236\n",
      "Epoch [3/10], Step [48/390], Loss: 3.3667\n",
      "Epoch [3/10], Step [49/390], Loss: 3.4060\n",
      "Epoch [3/10], Step [50/390], Loss: 3.3152\n",
      "Epoch [3/10], Step [51/390], Loss: 3.5425\n",
      "Epoch [3/10], Step [52/390], Loss: 3.2523\n",
      "Epoch [3/10], Step [53/390], Loss: 3.1713\n",
      "Epoch [3/10], Step [54/390], Loss: 3.2204\n",
      "Epoch [3/10], Step [55/390], Loss: 3.4777\n",
      "Epoch [3/10], Step [56/390], Loss: 3.5348\n",
      "Epoch [3/10], Step [57/390], Loss: 3.3713\n",
      "Epoch [3/10], Step [58/390], Loss: 3.3290\n",
      "Epoch [3/10], Step [59/390], Loss: 3.4069\n",
      "Epoch [3/10], Step [60/390], Loss: 3.4061\n",
      "Epoch [3/10], Step [61/390], Loss: 3.3958\n",
      "Epoch [3/10], Step [62/390], Loss: 3.2940\n",
      "Epoch [3/10], Step [63/390], Loss: 3.4003\n",
      "Epoch [3/10], Step [64/390], Loss: 3.2653\n",
      "Epoch [3/10], Step [65/390], Loss: 3.3566\n",
      "Epoch [3/10], Step [66/390], Loss: 3.4816\n",
      "Epoch [3/10], Step [67/390], Loss: 3.4961\n",
      "Epoch [3/10], Step [68/390], Loss: 3.2131\n",
      "Epoch [3/10], Step [69/390], Loss: 3.4476\n",
      "Epoch [3/10], Step [70/390], Loss: 3.5071\n",
      "Epoch [3/10], Step [71/390], Loss: 3.2124\n",
      "Epoch [3/10], Step [72/390], Loss: 3.4070\n",
      "Epoch [3/10], Step [73/390], Loss: 3.1548\n",
      "Epoch [3/10], Step [74/390], Loss: 3.1374\n",
      "Epoch [3/10], Step [75/390], Loss: 3.2636\n",
      "Epoch [3/10], Step [76/390], Loss: 3.0988\n",
      "Epoch [3/10], Step [77/390], Loss: 3.3363\n",
      "Epoch [3/10], Step [78/390], Loss: 3.3349\n",
      "Epoch [3/10], Step [79/390], Loss: 3.2843\n",
      "Epoch [3/10], Step [80/390], Loss: 3.0151\n",
      "Epoch [3/10], Step [81/390], Loss: 3.1707\n",
      "Epoch [3/10], Step [82/390], Loss: 3.2702\n",
      "Epoch [3/10], Step [83/390], Loss: 3.1541\n",
      "Epoch [3/10], Step [84/390], Loss: 2.9274\n",
      "Epoch [3/10], Step [85/390], Loss: 3.1754\n",
      "Epoch [3/10], Step [86/390], Loss: 3.1190\n",
      "Epoch [3/10], Step [87/390], Loss: 3.2765\n",
      "Epoch [3/10], Step [88/390], Loss: 2.8972\n",
      "Epoch [3/10], Step [89/390], Loss: 3.4793\n",
      "Epoch [3/10], Step [90/390], Loss: 3.3055\n",
      "Epoch [3/10], Step [91/390], Loss: 3.0481\n",
      "Epoch [3/10], Step [92/390], Loss: 3.2776\n",
      "Epoch [3/10], Step [93/390], Loss: 3.0544\n",
      "Epoch [3/10], Step [94/390], Loss: 3.4839\n",
      "Epoch [3/10], Step [95/390], Loss: 3.1311\n",
      "Epoch [3/10], Step [96/390], Loss: 3.2620\n",
      "Epoch [3/10], Step [97/390], Loss: 3.0838\n",
      "Epoch [3/10], Step [98/390], Loss: 3.3678\n",
      "Epoch [3/10], Step [99/390], Loss: 3.1951\n",
      "Epoch [3/10], Step [100/390], Loss: 3.3141\n",
      "Epoch [3/10], Step [101/390], Loss: 3.3948\n",
      "Epoch [3/10], Step [102/390], Loss: 3.2185\n",
      "Epoch [3/10], Step [103/390], Loss: 3.0679\n",
      "Epoch [3/10], Step [104/390], Loss: 3.3598\n",
      "Epoch [3/10], Step [105/390], Loss: 3.4477\n",
      "Epoch [3/10], Step [106/390], Loss: 3.1423\n",
      "Epoch [3/10], Step [107/390], Loss: 3.2292\n",
      "Epoch [3/10], Step [108/390], Loss: 3.1124\n",
      "Epoch [3/10], Step [109/390], Loss: 3.4210\n",
      "Epoch [3/10], Step [110/390], Loss: 3.6831\n",
      "Epoch [3/10], Step [111/390], Loss: 3.1040\n",
      "Epoch [3/10], Step [112/390], Loss: 3.2869\n",
      "Epoch [3/10], Step [113/390], Loss: 3.3033\n",
      "Epoch [3/10], Step [114/390], Loss: 3.1969\n",
      "Epoch [3/10], Step [115/390], Loss: 3.1219\n",
      "Epoch [3/10], Step [116/390], Loss: 3.2457\n",
      "Epoch [3/10], Step [117/390], Loss: 3.0913\n",
      "Epoch [3/10], Step [118/390], Loss: 3.3553\n",
      "Epoch [3/10], Step [119/390], Loss: 3.5230\n",
      "Epoch [3/10], Step [120/390], Loss: 3.2854\n",
      "Epoch [3/10], Step [121/390], Loss: 3.1470\n",
      "Epoch [3/10], Step [122/390], Loss: 3.0529\n",
      "Epoch [3/10], Step [123/390], Loss: 3.0894\n",
      "Epoch [3/10], Step [124/390], Loss: 3.5183\n",
      "Epoch [3/10], Step [125/390], Loss: 3.2691\n",
      "Epoch [3/10], Step [126/390], Loss: 3.3455\n",
      "Epoch [3/10], Step [127/390], Loss: 3.2940\n",
      "Epoch [3/10], Step [128/390], Loss: 3.0494\n",
      "Epoch [3/10], Step [129/390], Loss: 3.0022\n",
      "Epoch [3/10], Step [130/390], Loss: 3.2697\n",
      "Epoch [3/10], Step [131/390], Loss: 3.3049\n",
      "Epoch [3/10], Step [132/390], Loss: 3.2904\n",
      "Epoch [3/10], Step [133/390], Loss: 3.4270\n",
      "Epoch [3/10], Step [134/390], Loss: 3.4021\n",
      "Epoch [3/10], Step [135/390], Loss: 3.3061\n",
      "Epoch [3/10], Step [136/390], Loss: 3.2138\n",
      "Epoch [3/10], Step [137/390], Loss: 3.0536\n",
      "Epoch [3/10], Step [138/390], Loss: 3.3172\n",
      "Epoch [3/10], Step [139/390], Loss: 3.0164\n",
      "Epoch [3/10], Step [140/390], Loss: 3.0985\n",
      "Epoch [3/10], Step [141/390], Loss: 3.1226\n",
      "Epoch [3/10], Step [142/390], Loss: 3.2824\n",
      "Epoch [3/10], Step [143/390], Loss: 3.1642\n",
      "Epoch [3/10], Step [144/390], Loss: 3.2009\n",
      "Epoch [3/10], Step [145/390], Loss: 2.9545\n",
      "Epoch [3/10], Step [146/390], Loss: 3.2530\n",
      "Epoch [3/10], Step [147/390], Loss: 2.9891\n",
      "Epoch [3/10], Step [148/390], Loss: 3.2963\n",
      "Epoch [3/10], Step [149/390], Loss: 3.1656\n",
      "Epoch [3/10], Step [150/390], Loss: 3.1317\n",
      "Epoch [3/10], Step [151/390], Loss: 3.2804\n",
      "Epoch [3/10], Step [152/390], Loss: 3.2753\n",
      "Epoch [3/10], Step [153/390], Loss: 3.0706\n",
      "Epoch [3/10], Step [154/390], Loss: 3.2927\n",
      "Epoch [3/10], Step [155/390], Loss: 3.2789\n",
      "Epoch [3/10], Step [156/390], Loss: 3.0222\n",
      "Epoch [3/10], Step [157/390], Loss: 3.3112\n",
      "Epoch [3/10], Step [158/390], Loss: 3.1431\n",
      "Epoch [3/10], Step [159/390], Loss: 3.2636\n",
      "Epoch [3/10], Step [160/390], Loss: 3.0712\n",
      "Epoch [3/10], Step [161/390], Loss: 2.9328\n",
      "Epoch [3/10], Step [162/390], Loss: 3.1313\n",
      "Epoch [3/10], Step [163/390], Loss: 2.9403\n",
      "Epoch [3/10], Step [164/390], Loss: 3.1536\n",
      "Epoch [3/10], Step [165/390], Loss: 3.1635\n",
      "Epoch [3/10], Step [166/390], Loss: 3.0468\n",
      "Epoch [3/10], Step [167/390], Loss: 3.1763\n",
      "Epoch [3/10], Step [168/390], Loss: 3.3711\n",
      "Epoch [3/10], Step [169/390], Loss: 3.1872\n",
      "Epoch [3/10], Step [170/390], Loss: 3.2061\n",
      "Epoch [3/10], Step [171/390], Loss: 2.9927\n",
      "Epoch [3/10], Step [172/390], Loss: 3.2600\n",
      "Epoch [3/10], Step [173/390], Loss: 3.1648\n",
      "Epoch [3/10], Step [174/390], Loss: 3.1906\n",
      "Epoch [3/10], Step [175/390], Loss: 3.2368\n",
      "Epoch [3/10], Step [176/390], Loss: 3.1018\n",
      "Epoch [3/10], Step [177/390], Loss: 3.3045\n",
      "Epoch [3/10], Step [178/390], Loss: 3.3389\n",
      "Epoch [3/10], Step [179/390], Loss: 3.1072\n",
      "Epoch [3/10], Step [180/390], Loss: 3.1856\n",
      "Epoch [3/10], Step [181/390], Loss: 3.2375\n",
      "Epoch [3/10], Step [182/390], Loss: 3.4352\n",
      "Epoch [3/10], Step [183/390], Loss: 3.0994\n",
      "Epoch [3/10], Step [184/390], Loss: 3.2281\n",
      "Epoch [3/10], Step [185/390], Loss: 3.1302\n",
      "Epoch [3/10], Step [186/390], Loss: 3.0190\n",
      "Epoch [3/10], Step [187/390], Loss: 3.2832\n",
      "Epoch [3/10], Step [188/390], Loss: 3.0050\n",
      "Epoch [3/10], Step [189/390], Loss: 3.1202\n",
      "Epoch [3/10], Step [190/390], Loss: 3.2015\n",
      "Epoch [3/10], Step [191/390], Loss: 3.0930\n",
      "Epoch [3/10], Step [192/390], Loss: 3.3275\n",
      "Epoch [3/10], Step [193/390], Loss: 3.2386\n",
      "Epoch [3/10], Step [194/390], Loss: 3.3574\n",
      "Epoch [3/10], Step [195/390], Loss: 3.0511\n",
      "Epoch [3/10], Step [196/390], Loss: 3.2638\n",
      "Epoch [3/10], Step [197/390], Loss: 3.3061\n",
      "Epoch [3/10], Step [198/390], Loss: 3.1666\n",
      "Epoch [3/10], Step [199/390], Loss: 3.2341\n",
      "Epoch [3/10], Step [200/390], Loss: 3.1661\n",
      "Epoch [3/10], Step [201/390], Loss: 3.1694\n",
      "Epoch [3/10], Step [202/390], Loss: 2.9649\n",
      "Epoch [3/10], Step [203/390], Loss: 3.0518\n",
      "Epoch [3/10], Step [204/390], Loss: 3.2682\n",
      "Epoch [3/10], Step [205/390], Loss: 3.3030\n",
      "Epoch [3/10], Step [206/390], Loss: 3.0156\n",
      "Epoch [3/10], Step [207/390], Loss: 3.4780\n",
      "Epoch [3/10], Step [208/390], Loss: 2.8009\n",
      "Epoch [3/10], Step [209/390], Loss: 3.1361\n",
      "Epoch [3/10], Step [210/390], Loss: 3.1948\n",
      "Epoch [3/10], Step [211/390], Loss: 3.0467\n",
      "Epoch [3/10], Step [212/390], Loss: 3.2173\n",
      "Epoch [3/10], Step [213/390], Loss: 2.9874\n",
      "Epoch [3/10], Step [214/390], Loss: 3.0904\n",
      "Epoch [3/10], Step [215/390], Loss: 3.0787\n",
      "Epoch [3/10], Step [216/390], Loss: 3.0993\n",
      "Epoch [3/10], Step [217/390], Loss: 3.0359\n",
      "Epoch [3/10], Step [218/390], Loss: 3.0918\n",
      "Epoch [3/10], Step [219/390], Loss: 3.2937\n",
      "Epoch [3/10], Step [220/390], Loss: 3.1120\n",
      "Epoch [3/10], Step [221/390], Loss: 3.2785\n",
      "Epoch [3/10], Step [222/390], Loss: 3.2228\n",
      "Epoch [3/10], Step [223/390], Loss: 3.2261\n",
      "Epoch [3/10], Step [224/390], Loss: 3.0367\n",
      "Epoch [3/10], Step [225/390], Loss: 2.9051\n",
      "Epoch [3/10], Step [226/390], Loss: 3.0717\n",
      "Epoch [3/10], Step [227/390], Loss: 3.2182\n",
      "Epoch [3/10], Step [228/390], Loss: 3.0051\n",
      "Epoch [3/10], Step [229/390], Loss: 2.9588\n",
      "Epoch [3/10], Step [230/390], Loss: 3.3048\n",
      "Epoch [3/10], Step [231/390], Loss: 3.1990\n",
      "Epoch [3/10], Step [232/390], Loss: 2.9915\n",
      "Epoch [3/10], Step [233/390], Loss: 3.1193\n",
      "Epoch [3/10], Step [234/390], Loss: 3.0679\n",
      "Epoch [3/10], Step [235/390], Loss: 3.4862\n",
      "Epoch [3/10], Step [236/390], Loss: 2.8241\n",
      "Epoch [3/10], Step [237/390], Loss: 2.9376\n",
      "Epoch [3/10], Step [238/390], Loss: 3.0723\n",
      "Epoch [3/10], Step [239/390], Loss: 3.4809\n",
      "Epoch [3/10], Step [240/390], Loss: 3.2724\n",
      "Epoch [3/10], Step [241/390], Loss: 3.1133\n",
      "Epoch [3/10], Step [242/390], Loss: 3.0241\n",
      "Epoch [3/10], Step [243/390], Loss: 3.1584\n",
      "Epoch [3/10], Step [244/390], Loss: 3.1291\n",
      "Epoch [3/10], Step [245/390], Loss: 3.1005\n",
      "Epoch [3/10], Step [246/390], Loss: 3.4216\n",
      "Epoch [3/10], Step [247/390], Loss: 3.2177\n",
      "Epoch [3/10], Step [248/390], Loss: 3.3240\n",
      "Epoch [3/10], Step [249/390], Loss: 3.1437\n",
      "Epoch [3/10], Step [250/390], Loss: 3.3326\n",
      "Epoch [3/10], Step [251/390], Loss: 2.9780\n",
      "Epoch [3/10], Step [252/390], Loss: 3.2253\n",
      "Epoch [3/10], Step [253/390], Loss: 3.1936\n",
      "Epoch [3/10], Step [254/390], Loss: 3.2402\n",
      "Epoch [3/10], Step [255/390], Loss: 3.1029\n",
      "Epoch [3/10], Step [256/390], Loss: 3.1915\n",
      "Epoch [3/10], Step [257/390], Loss: 2.9992\n",
      "Epoch [3/10], Step [258/390], Loss: 3.0185\n",
      "Epoch [3/10], Step [259/390], Loss: 3.1063\n",
      "Epoch [3/10], Step [260/390], Loss: 2.8856\n",
      "Epoch [3/10], Step [261/390], Loss: 3.1163\n",
      "Epoch [3/10], Step [262/390], Loss: 3.1809\n",
      "Epoch [3/10], Step [263/390], Loss: 3.2294\n",
      "Epoch [3/10], Step [264/390], Loss: 3.2059\n",
      "Epoch [3/10], Step [265/390], Loss: 3.1652\n",
      "Epoch [3/10], Step [266/390], Loss: 3.1899\n",
      "Epoch [3/10], Step [267/390], Loss: 3.1020\n",
      "Epoch [3/10], Step [268/390], Loss: 3.1009\n",
      "Epoch [3/10], Step [269/390], Loss: 3.0599\n",
      "Epoch [3/10], Step [270/390], Loss: 2.9817\n",
      "Epoch [3/10], Step [271/390], Loss: 3.1630\n",
      "Epoch [3/10], Step [272/390], Loss: 3.0463\n",
      "Epoch [3/10], Step [273/390], Loss: 2.8740\n",
      "Epoch [3/10], Step [274/390], Loss: 3.4249\n",
      "Epoch [3/10], Step [275/390], Loss: 3.1762\n",
      "Epoch [3/10], Step [276/390], Loss: 3.3692\n",
      "Epoch [3/10], Step [277/390], Loss: 3.0330\n",
      "Epoch [3/10], Step [278/390], Loss: 3.2813\n",
      "Epoch [3/10], Step [279/390], Loss: 3.0617\n",
      "Epoch [3/10], Step [280/390], Loss: 3.0510\n",
      "Epoch [3/10], Step [281/390], Loss: 3.1780\n",
      "Epoch [3/10], Step [282/390], Loss: 3.1964\n",
      "Epoch [3/10], Step [283/390], Loss: 3.0141\n",
      "Epoch [3/10], Step [284/390], Loss: 3.0881\n",
      "Epoch [3/10], Step [285/390], Loss: 2.9639\n",
      "Epoch [3/10], Step [286/390], Loss: 3.1966\n",
      "Epoch [3/10], Step [287/390], Loss: 3.1179\n",
      "Epoch [3/10], Step [288/390], Loss: 3.1514\n",
      "Epoch [3/10], Step [289/390], Loss: 3.0189\n",
      "Epoch [3/10], Step [290/390], Loss: 3.0499\n",
      "Epoch [3/10], Step [291/390], Loss: 3.2738\n",
      "Epoch [3/10], Step [292/390], Loss: 3.2610\n",
      "Epoch [3/10], Step [293/390], Loss: 3.1357\n",
      "Epoch [3/10], Step [294/390], Loss: 3.3355\n",
      "Epoch [3/10], Step [295/390], Loss: 3.1590\n",
      "Epoch [3/10], Step [296/390], Loss: 2.9671\n",
      "Epoch [3/10], Step [297/390], Loss: 3.2799\n",
      "Epoch [3/10], Step [298/390], Loss: 3.0197\n",
      "Epoch [3/10], Step [299/390], Loss: 2.8531\n",
      "Epoch [3/10], Step [300/390], Loss: 3.0166\n",
      "Epoch [3/10], Step [301/390], Loss: 2.9745\n",
      "Epoch [3/10], Step [302/390], Loss: 3.1302\n",
      "Epoch [3/10], Step [303/390], Loss: 3.2155\n",
      "Epoch [3/10], Step [304/390], Loss: 3.0173\n",
      "Epoch [3/10], Step [305/390], Loss: 3.1550\n",
      "Epoch [3/10], Step [306/390], Loss: 3.2014\n",
      "Epoch [3/10], Step [307/390], Loss: 3.1009\n",
      "Epoch [3/10], Step [308/390], Loss: 3.0114\n",
      "Epoch [3/10], Step [309/390], Loss: 3.0103\n",
      "Epoch [3/10], Step [310/390], Loss: 3.1690\n",
      "Epoch [3/10], Step [311/390], Loss: 3.1673\n",
      "Epoch [3/10], Step [312/390], Loss: 2.7912\n",
      "Epoch [3/10], Step [313/390], Loss: 3.0905\n",
      "Epoch [3/10], Step [314/390], Loss: 3.2978\n",
      "Epoch [3/10], Step [315/390], Loss: 3.3256\n",
      "Epoch [3/10], Step [316/390], Loss: 3.2517\n",
      "Epoch [3/10], Step [317/390], Loss: 3.1298\n",
      "Epoch [3/10], Step [318/390], Loss: 2.9905\n",
      "Epoch [3/10], Step [319/390], Loss: 3.1569\n",
      "Epoch [3/10], Step [320/390], Loss: 2.9694\n",
      "Epoch [3/10], Step [321/390], Loss: 2.9563\n",
      "Epoch [3/10], Step [322/390], Loss: 2.8500\n",
      "Epoch [3/10], Step [323/390], Loss: 3.0530\n",
      "Epoch [3/10], Step [324/390], Loss: 2.9633\n",
      "Epoch [3/10], Step [325/390], Loss: 3.2107\n",
      "Epoch [3/10], Step [326/390], Loss: 3.3366\n",
      "Epoch [3/10], Step [327/390], Loss: 2.9959\n",
      "Epoch [3/10], Step [328/390], Loss: 3.0320\n",
      "Epoch [3/10], Step [329/390], Loss: 3.1575\n",
      "Epoch [3/10], Step [330/390], Loss: 3.3750\n",
      "Epoch [3/10], Step [331/390], Loss: 3.2202\n",
      "Epoch [3/10], Step [332/390], Loss: 3.1130\n",
      "Epoch [3/10], Step [333/390], Loss: 3.1538\n",
      "Epoch [3/10], Step [334/390], Loss: 3.0908\n",
      "Epoch [3/10], Step [335/390], Loss: 3.3508\n",
      "Epoch [3/10], Step [336/390], Loss: 3.1031\n",
      "Epoch [3/10], Step [337/390], Loss: 3.1459\n",
      "Epoch [3/10], Step [338/390], Loss: 3.0501\n",
      "Epoch [3/10], Step [339/390], Loss: 3.0323\n",
      "Epoch [3/10], Step [340/390], Loss: 2.8060\n",
      "Epoch [3/10], Step [341/390], Loss: 2.9949\n",
      "Epoch [3/10], Step [342/390], Loss: 3.1168\n",
      "Epoch [3/10], Step [343/390], Loss: 2.8119\n",
      "Epoch [3/10], Step [344/390], Loss: 3.2099\n",
      "Epoch [3/10], Step [345/390], Loss: 3.0244\n",
      "Epoch [3/10], Step [346/390], Loss: 2.9699\n",
      "Epoch [3/10], Step [347/390], Loss: 3.0115\n",
      "Epoch [3/10], Step [348/390], Loss: 3.1943\n",
      "Epoch [3/10], Step [349/390], Loss: 2.9884\n",
      "Epoch [3/10], Step [350/390], Loss: 2.9441\n",
      "Epoch [3/10], Step [351/390], Loss: 3.1384\n",
      "Epoch [3/10], Step [352/390], Loss: 3.1206\n",
      "Epoch [3/10], Step [353/390], Loss: 2.9234\n",
      "Epoch [3/10], Step [354/390], Loss: 2.9911\n",
      "Epoch [3/10], Step [355/390], Loss: 3.0053\n",
      "Epoch [3/10], Step [356/390], Loss: 3.3567\n",
      "Epoch [3/10], Step [357/390], Loss: 2.8948\n",
      "Epoch [3/10], Step [358/390], Loss: 3.1789\n",
      "Epoch [3/10], Step [359/390], Loss: 3.0561\n",
      "Epoch [3/10], Step [360/390], Loss: 2.8707\n",
      "Epoch [3/10], Step [361/390], Loss: 2.9157\n",
      "Epoch [3/10], Step [362/390], Loss: 3.2626\n",
      "Epoch [3/10], Step [363/390], Loss: 2.9247\n",
      "Epoch [3/10], Step [364/390], Loss: 3.0132\n",
      "Epoch [3/10], Step [365/390], Loss: 3.4357\n",
      "Epoch [3/10], Step [366/390], Loss: 2.9926\n",
      "Epoch [3/10], Step [367/390], Loss: 2.6946\n",
      "Epoch [3/10], Step [368/390], Loss: 2.9899\n",
      "Epoch [3/10], Step [369/390], Loss: 3.1583\n",
      "Epoch [3/10], Step [370/390], Loss: 2.9733\n",
      "Epoch [3/10], Step [371/390], Loss: 3.1512\n",
      "Epoch [3/10], Step [372/390], Loss: 2.8882\n",
      "Epoch [3/10], Step [373/390], Loss: 3.0402\n",
      "Epoch [3/10], Step [374/390], Loss: 3.0799\n",
      "Epoch [3/10], Step [375/390], Loss: 2.9909\n",
      "Epoch [3/10], Step [376/390], Loss: 2.9845\n",
      "Epoch [3/10], Step [377/390], Loss: 3.0592\n",
      "Epoch [3/10], Step [378/390], Loss: 3.0362\n",
      "Epoch [3/10], Step [379/390], Loss: 2.8095\n",
      "Epoch [3/10], Step [380/390], Loss: 3.1576\n",
      "Epoch [3/10], Step [381/390], Loss: 3.0551\n",
      "Epoch [3/10], Step [382/390], Loss: 3.1431\n",
      "Epoch [3/10], Step [383/390], Loss: 3.0982\n",
      "Epoch [3/10], Step [384/390], Loss: 3.1389\n",
      "Epoch [3/10], Step [385/390], Loss: 3.0573\n",
      "Epoch [3/10], Step [386/390], Loss: 3.2322\n",
      "Epoch [3/10], Step [387/390], Loss: 3.1610\n",
      "Epoch [3/10], Step [388/390], Loss: 2.9886\n",
      "Epoch [3/10], Step [389/390], Loss: 2.8652\n",
      "Epoch [3/10], Step [390/390], Loss: 2.6979\n",
      "Epoch [3/10], Step [391/390], Loss: 3.0546\n",
      "Epoch [4/10], Step [1/390], Loss: 3.0748\n",
      "Epoch [4/10], Step [2/390], Loss: 2.8538\n",
      "Epoch [4/10], Step [3/390], Loss: 2.8630\n",
      "Epoch [4/10], Step [4/390], Loss: 2.9563\n",
      "Epoch [4/10], Step [5/390], Loss: 2.8918\n",
      "Epoch [4/10], Step [6/390], Loss: 2.8740\n",
      "Epoch [4/10], Step [7/390], Loss: 2.6831\n",
      "Epoch [4/10], Step [8/390], Loss: 2.8169\n",
      "Epoch [4/10], Step [9/390], Loss: 2.7882\n",
      "Epoch [4/10], Step [10/390], Loss: 3.0309\n",
      "Epoch [4/10], Step [11/390], Loss: 2.8872\n",
      "Epoch [4/10], Step [12/390], Loss: 2.8019\n",
      "Epoch [4/10], Step [13/390], Loss: 3.0803\n",
      "Epoch [4/10], Step [14/390], Loss: 3.0332\n",
      "Epoch [4/10], Step [15/390], Loss: 3.0457\n",
      "Epoch [4/10], Step [16/390], Loss: 3.0413\n",
      "Epoch [4/10], Step [17/390], Loss: 3.1890\n",
      "Epoch [4/10], Step [18/390], Loss: 2.8763\n",
      "Epoch [4/10], Step [19/390], Loss: 2.6575\n",
      "Epoch [4/10], Step [20/390], Loss: 3.0263\n",
      "Epoch [4/10], Step [21/390], Loss: 3.2215\n",
      "Epoch [4/10], Step [22/390], Loss: 2.9749\n",
      "Epoch [4/10], Step [23/390], Loss: 2.7538\n",
      "Epoch [4/10], Step [24/390], Loss: 2.9230\n",
      "Epoch [4/10], Step [25/390], Loss: 2.8809\n",
      "Epoch [4/10], Step [26/390], Loss: 2.9722\n",
      "Epoch [4/10], Step [27/390], Loss: 2.7948\n",
      "Epoch [4/10], Step [28/390], Loss: 3.0833\n",
      "Epoch [4/10], Step [29/390], Loss: 2.9678\n",
      "Epoch [4/10], Step [30/390], Loss: 2.8086\n",
      "Epoch [4/10], Step [31/390], Loss: 3.1670\n",
      "Epoch [4/10], Step [32/390], Loss: 3.2102\n",
      "Epoch [4/10], Step [33/390], Loss: 2.7872\n",
      "Epoch [4/10], Step [34/390], Loss: 3.0851\n",
      "Epoch [4/10], Step [35/390], Loss: 3.1525\n",
      "Epoch [4/10], Step [36/390], Loss: 2.7438\n",
      "Epoch [4/10], Step [37/390], Loss: 2.9300\n",
      "Epoch [4/10], Step [38/390], Loss: 3.0974\n",
      "Epoch [4/10], Step [39/390], Loss: 3.1536\n",
      "Epoch [4/10], Step [40/390], Loss: 2.9638\n",
      "Epoch [4/10], Step [41/390], Loss: 3.0291\n",
      "Epoch [4/10], Step [42/390], Loss: 2.9756\n",
      "Epoch [4/10], Step [43/390], Loss: 2.5385\n",
      "Epoch [4/10], Step [44/390], Loss: 2.7910\n",
      "Epoch [4/10], Step [45/390], Loss: 2.7273\n",
      "Epoch [4/10], Step [46/390], Loss: 2.7625\n",
      "Epoch [4/10], Step [47/390], Loss: 2.8291\n",
      "Epoch [4/10], Step [48/390], Loss: 2.6905\n",
      "Epoch [4/10], Step [49/390], Loss: 3.0634\n",
      "Epoch [4/10], Step [50/390], Loss: 2.8721\n",
      "Epoch [4/10], Step [51/390], Loss: 2.7367\n",
      "Epoch [4/10], Step [52/390], Loss: 2.9734\n",
      "Epoch [4/10], Step [53/390], Loss: 3.0643\n",
      "Epoch [4/10], Step [54/390], Loss: 2.6878\n",
      "Epoch [4/10], Step [55/390], Loss: 2.6604\n",
      "Epoch [4/10], Step [56/390], Loss: 2.7177\n",
      "Epoch [4/10], Step [57/390], Loss: 2.9328\n",
      "Epoch [4/10], Step [58/390], Loss: 2.8805\n",
      "Epoch [4/10], Step [59/390], Loss: 2.9598\n",
      "Epoch [4/10], Step [60/390], Loss: 2.9071\n",
      "Epoch [4/10], Step [61/390], Loss: 2.8688\n",
      "Epoch [4/10], Step [62/390], Loss: 2.8281\n",
      "Epoch [4/10], Step [63/390], Loss: 2.8210\n",
      "Epoch [4/10], Step [64/390], Loss: 2.9631\n",
      "Epoch [4/10], Step [65/390], Loss: 3.0159\n",
      "Epoch [4/10], Step [66/390], Loss: 2.8987\n",
      "Epoch [4/10], Step [67/390], Loss: 3.0482\n",
      "Epoch [4/10], Step [68/390], Loss: 3.1862\n",
      "Epoch [4/10], Step [69/390], Loss: 2.8073\n",
      "Epoch [4/10], Step [70/390], Loss: 3.0818\n",
      "Epoch [4/10], Step [71/390], Loss: 3.0571\n",
      "Epoch [4/10], Step [72/390], Loss: 3.4312\n",
      "Epoch [4/10], Step [73/390], Loss: 2.8601\n",
      "Epoch [4/10], Step [74/390], Loss: 2.8973\n",
      "Epoch [4/10], Step [75/390], Loss: 3.0362\n",
      "Epoch [4/10], Step [76/390], Loss: 3.1363\n",
      "Epoch [4/10], Step [77/390], Loss: 2.8385\n",
      "Epoch [4/10], Step [78/390], Loss: 2.6480\n",
      "Epoch [4/10], Step [79/390], Loss: 2.6525\n",
      "Epoch [4/10], Step [80/390], Loss: 2.8505\n",
      "Epoch [4/10], Step [81/390], Loss: 3.1648\n",
      "Epoch [4/10], Step [82/390], Loss: 2.8806\n",
      "Epoch [4/10], Step [83/390], Loss: 2.8539\n",
      "Epoch [4/10], Step [84/390], Loss: 2.8936\n",
      "Epoch [4/10], Step [85/390], Loss: 3.1313\n",
      "Epoch [4/10], Step [86/390], Loss: 2.8239\n",
      "Epoch [4/10], Step [87/390], Loss: 2.8852\n",
      "Epoch [4/10], Step [88/390], Loss: 3.1850\n",
      "Epoch [4/10], Step [89/390], Loss: 3.1395\n",
      "Epoch [4/10], Step [90/390], Loss: 2.6867\n",
      "Epoch [4/10], Step [91/390], Loss: 2.9419\n",
      "Epoch [4/10], Step [92/390], Loss: 2.7970\n",
      "Epoch [4/10], Step [93/390], Loss: 3.1813\n",
      "Epoch [4/10], Step [94/390], Loss: 2.6498\n",
      "Epoch [4/10], Step [95/390], Loss: 2.7316\n",
      "Epoch [4/10], Step [96/390], Loss: 2.9361\n",
      "Epoch [4/10], Step [97/390], Loss: 2.9583\n",
      "Epoch [4/10], Step [98/390], Loss: 2.8175\n",
      "Epoch [4/10], Step [99/390], Loss: 2.7837\n",
      "Epoch [4/10], Step [100/390], Loss: 2.9454\n",
      "Epoch [4/10], Step [101/390], Loss: 2.7837\n",
      "Epoch [4/10], Step [102/390], Loss: 2.9467\n",
      "Epoch [4/10], Step [103/390], Loss: 2.8150\n",
      "Epoch [4/10], Step [104/390], Loss: 2.9592\n",
      "Epoch [4/10], Step [105/390], Loss: 2.7803\n",
      "Epoch [4/10], Step [106/390], Loss: 2.8782\n",
      "Epoch [4/10], Step [107/390], Loss: 2.9558\n",
      "Epoch [4/10], Step [108/390], Loss: 2.8748\n",
      "Epoch [4/10], Step [109/390], Loss: 2.9660\n",
      "Epoch [4/10], Step [110/390], Loss: 2.8875\n",
      "Epoch [4/10], Step [111/390], Loss: 2.6834\n",
      "Epoch [4/10], Step [112/390], Loss: 2.8849\n",
      "Epoch [4/10], Step [113/390], Loss: 2.8445\n",
      "Epoch [4/10], Step [114/390], Loss: 2.7226\n",
      "Epoch [4/10], Step [115/390], Loss: 2.9375\n",
      "Epoch [4/10], Step [116/390], Loss: 2.8829\n",
      "Epoch [4/10], Step [117/390], Loss: 2.7185\n",
      "Epoch [4/10], Step [118/390], Loss: 2.8661\n",
      "Epoch [4/10], Step [119/390], Loss: 2.7546\n",
      "Epoch [4/10], Step [120/390], Loss: 2.8108\n",
      "Epoch [4/10], Step [121/390], Loss: 2.8796\n",
      "Epoch [4/10], Step [122/390], Loss: 2.6119\n",
      "Epoch [4/10], Step [123/390], Loss: 2.8496\n",
      "Epoch [4/10], Step [124/390], Loss: 2.8359\n",
      "Epoch [4/10], Step [125/390], Loss: 2.7786\n",
      "Epoch [4/10], Step [126/390], Loss: 2.6417\n",
      "Epoch [4/10], Step [127/390], Loss: 3.2174\n",
      "Epoch [4/10], Step [128/390], Loss: 2.8260\n",
      "Epoch [4/10], Step [129/390], Loss: 3.1301\n",
      "Epoch [4/10], Step [130/390], Loss: 3.0189\n",
      "Epoch [4/10], Step [131/390], Loss: 2.6550\n",
      "Epoch [4/10], Step [132/390], Loss: 2.6741\n",
      "Epoch [4/10], Step [133/390], Loss: 2.6871\n",
      "Epoch [4/10], Step [134/390], Loss: 2.9150\n",
      "Epoch [4/10], Step [135/390], Loss: 2.9820\n",
      "Epoch [4/10], Step [136/390], Loss: 3.0685\n",
      "Epoch [4/10], Step [137/390], Loss: 3.1438\n",
      "Epoch [4/10], Step [138/390], Loss: 2.8923\n",
      "Epoch [4/10], Step [139/390], Loss: 2.9986\n",
      "Epoch [4/10], Step [140/390], Loss: 3.0747\n",
      "Epoch [4/10], Step [141/390], Loss: 2.6831\n",
      "Epoch [4/10], Step [142/390], Loss: 2.9792\n",
      "Epoch [4/10], Step [143/390], Loss: 2.9447\n",
      "Epoch [4/10], Step [144/390], Loss: 2.9085\n",
      "Epoch [4/10], Step [145/390], Loss: 2.7195\n",
      "Epoch [4/10], Step [146/390], Loss: 2.6295\n",
      "Epoch [4/10], Step [147/390], Loss: 3.2639\n",
      "Epoch [4/10], Step [148/390], Loss: 2.7378\n",
      "Epoch [4/10], Step [149/390], Loss: 2.7164\n",
      "Epoch [4/10], Step [150/390], Loss: 2.9014\n",
      "Epoch [4/10], Step [151/390], Loss: 2.8980\n",
      "Epoch [4/10], Step [152/390], Loss: 3.1523\n",
      "Epoch [4/10], Step [153/390], Loss: 2.7445\n",
      "Epoch [4/10], Step [154/390], Loss: 2.9738\n",
      "Epoch [4/10], Step [155/390], Loss: 2.9321\n",
      "Epoch [4/10], Step [156/390], Loss: 3.1106\n",
      "Epoch [4/10], Step [157/390], Loss: 2.8249\n",
      "Epoch [4/10], Step [158/390], Loss: 3.0097\n",
      "Epoch [4/10], Step [159/390], Loss: 2.8041\n",
      "Epoch [4/10], Step [160/390], Loss: 2.7513\n",
      "Epoch [4/10], Step [161/390], Loss: 2.8912\n",
      "Epoch [4/10], Step [162/390], Loss: 3.0945\n",
      "Epoch [4/10], Step [163/390], Loss: 2.9232\n",
      "Epoch [4/10], Step [164/390], Loss: 2.8825\n",
      "Epoch [4/10], Step [165/390], Loss: 2.9380\n",
      "Epoch [4/10], Step [166/390], Loss: 2.9415\n",
      "Epoch [4/10], Step [167/390], Loss: 2.9275\n",
      "Epoch [4/10], Step [168/390], Loss: 2.7206\n",
      "Epoch [4/10], Step [169/390], Loss: 3.0910\n",
      "Epoch [4/10], Step [170/390], Loss: 2.9578\n",
      "Epoch [4/10], Step [171/390], Loss: 2.7214\n",
      "Epoch [4/10], Step [172/390], Loss: 2.7945\n",
      "Epoch [4/10], Step [173/390], Loss: 2.6727\n",
      "Epoch [4/10], Step [174/390], Loss: 2.6730\n",
      "Epoch [4/10], Step [175/390], Loss: 2.9417\n",
      "Epoch [4/10], Step [176/390], Loss: 2.8419\n",
      "Epoch [4/10], Step [177/390], Loss: 2.7587\n",
      "Epoch [4/10], Step [178/390], Loss: 2.9053\n",
      "Epoch [4/10], Step [179/390], Loss: 2.6539\n",
      "Epoch [4/10], Step [180/390], Loss: 2.9984\n",
      "Epoch [4/10], Step [181/390], Loss: 2.8246\n",
      "Epoch [4/10], Step [182/390], Loss: 2.9823\n",
      "Epoch [4/10], Step [183/390], Loss: 2.6851\n",
      "Epoch [4/10], Step [184/390], Loss: 3.0814\n",
      "Epoch [4/10], Step [185/390], Loss: 3.0772\n",
      "Epoch [4/10], Step [186/390], Loss: 2.5904\n",
      "Epoch [4/10], Step [187/390], Loss: 2.8127\n",
      "Epoch [4/10], Step [188/390], Loss: 2.8699\n",
      "Epoch [4/10], Step [189/390], Loss: 2.8100\n",
      "Epoch [4/10], Step [190/390], Loss: 2.8552\n",
      "Epoch [4/10], Step [191/390], Loss: 2.6444\n",
      "Epoch [4/10], Step [192/390], Loss: 2.8912\n",
      "Epoch [4/10], Step [193/390], Loss: 2.7886\n",
      "Epoch [4/10], Step [194/390], Loss: 2.7919\n",
      "Epoch [4/10], Step [195/390], Loss: 2.7141\n",
      "Epoch [4/10], Step [196/390], Loss: 2.6692\n",
      "Epoch [4/10], Step [197/390], Loss: 2.8388\n",
      "Epoch [4/10], Step [198/390], Loss: 2.4327\n",
      "Epoch [4/10], Step [199/390], Loss: 2.8565\n",
      "Epoch [4/10], Step [200/390], Loss: 2.6629\n",
      "Epoch [4/10], Step [201/390], Loss: 2.7406\n",
      "Epoch [4/10], Step [202/390], Loss: 2.8009\n",
      "Epoch [4/10], Step [203/390], Loss: 2.5711\n",
      "Epoch [4/10], Step [204/390], Loss: 2.9069\n",
      "Epoch [4/10], Step [205/390], Loss: 2.9419\n",
      "Epoch [4/10], Step [206/390], Loss: 2.8262\n",
      "Epoch [4/10], Step [207/390], Loss: 3.1319\n",
      "Epoch [4/10], Step [208/390], Loss: 2.9276\n",
      "Epoch [4/10], Step [209/390], Loss: 2.5484\n",
      "Epoch [4/10], Step [210/390], Loss: 2.8895\n",
      "Epoch [4/10], Step [211/390], Loss: 2.8622\n",
      "Epoch [4/10], Step [212/390], Loss: 2.7602\n",
      "Epoch [4/10], Step [213/390], Loss: 3.1506\n",
      "Epoch [4/10], Step [214/390], Loss: 2.8154\n",
      "Epoch [4/10], Step [215/390], Loss: 2.7940\n",
      "Epoch [4/10], Step [216/390], Loss: 3.1513\n",
      "Epoch [4/10], Step [217/390], Loss: 2.5808\n",
      "Epoch [4/10], Step [218/390], Loss: 2.6109\n",
      "Epoch [4/10], Step [219/390], Loss: 2.9971\n",
      "Epoch [4/10], Step [220/390], Loss: 2.8145\n",
      "Epoch [4/10], Step [221/390], Loss: 2.9086\n",
      "Epoch [4/10], Step [222/390], Loss: 3.1134\n",
      "Epoch [4/10], Step [223/390], Loss: 2.8214\n",
      "Epoch [4/10], Step [224/390], Loss: 2.6946\n",
      "Epoch [4/10], Step [225/390], Loss: 2.8833\n",
      "Epoch [4/10], Step [226/390], Loss: 2.8789\n",
      "Epoch [4/10], Step [227/390], Loss: 2.7442\n",
      "Epoch [4/10], Step [228/390], Loss: 2.7548\n",
      "Epoch [4/10], Step [229/390], Loss: 3.0553\n",
      "Epoch [4/10], Step [230/390], Loss: 2.6464\n",
      "Epoch [4/10], Step [231/390], Loss: 2.7325\n",
      "Epoch [4/10], Step [232/390], Loss: 2.8291\n",
      "Epoch [4/10], Step [233/390], Loss: 2.7646\n",
      "Epoch [4/10], Step [234/390], Loss: 2.6925\n",
      "Epoch [4/10], Step [235/390], Loss: 2.8154\n",
      "Epoch [4/10], Step [236/390], Loss: 3.0526\n",
      "Epoch [4/10], Step [237/390], Loss: 2.7707\n",
      "Epoch [4/10], Step [238/390], Loss: 2.8274\n",
      "Epoch [4/10], Step [239/390], Loss: 2.9635\n",
      "Epoch [4/10], Step [240/390], Loss: 2.7698\n",
      "Epoch [4/10], Step [241/390], Loss: 2.4749\n",
      "Epoch [4/10], Step [242/390], Loss: 2.9574\n",
      "Epoch [4/10], Step [243/390], Loss: 2.7540\n",
      "Epoch [4/10], Step [244/390], Loss: 2.7769\n",
      "Epoch [4/10], Step [245/390], Loss: 2.9460\n",
      "Epoch [4/10], Step [246/390], Loss: 2.5162\n",
      "Epoch [4/10], Step [247/390], Loss: 2.8882\n",
      "Epoch [4/10], Step [248/390], Loss: 2.9968\n",
      "Epoch [4/10], Step [249/390], Loss: 2.8290\n",
      "Epoch [4/10], Step [250/390], Loss: 2.6640\n",
      "Epoch [4/10], Step [251/390], Loss: 2.8696\n",
      "Epoch [4/10], Step [252/390], Loss: 3.1719\n",
      "Epoch [4/10], Step [253/390], Loss: 2.9694\n",
      "Epoch [4/10], Step [254/390], Loss: 2.8124\n",
      "Epoch [4/10], Step [255/390], Loss: 2.8606\n",
      "Epoch [4/10], Step [256/390], Loss: 2.7236\n",
      "Epoch [4/10], Step [257/390], Loss: 2.8432\n",
      "Epoch [4/10], Step [258/390], Loss: 2.8069\n",
      "Epoch [4/10], Step [259/390], Loss: 2.6152\n",
      "Epoch [4/10], Step [260/390], Loss: 3.1810\n",
      "Epoch [4/10], Step [261/390], Loss: 2.6717\n",
      "Epoch [4/10], Step [262/390], Loss: 3.0456\n",
      "Epoch [4/10], Step [263/390], Loss: 2.7693\n",
      "Epoch [4/10], Step [264/390], Loss: 2.6601\n",
      "Epoch [4/10], Step [265/390], Loss: 3.2019\n",
      "Epoch [4/10], Step [266/390], Loss: 2.8147\n",
      "Epoch [4/10], Step [267/390], Loss: 2.7751\n",
      "Epoch [4/10], Step [268/390], Loss: 2.9525\n",
      "Epoch [4/10], Step [269/390], Loss: 2.8756\n",
      "Epoch [4/10], Step [270/390], Loss: 3.1478\n",
      "Epoch [4/10], Step [271/390], Loss: 2.9282\n",
      "Epoch [4/10], Step [272/390], Loss: 2.5947\n",
      "Epoch [4/10], Step [273/390], Loss: 2.7984\n",
      "Epoch [4/10], Step [274/390], Loss: 2.7103\n",
      "Epoch [4/10], Step [275/390], Loss: 2.7984\n",
      "Epoch [4/10], Step [276/390], Loss: 2.8990\n",
      "Epoch [4/10], Step [277/390], Loss: 2.7382\n",
      "Epoch [4/10], Step [278/390], Loss: 2.9061\n",
      "Epoch [4/10], Step [279/390], Loss: 2.6055\n",
      "Epoch [4/10], Step [280/390], Loss: 2.7639\n",
      "Epoch [4/10], Step [281/390], Loss: 2.5467\n",
      "Epoch [4/10], Step [282/390], Loss: 2.8781\n",
      "Epoch [4/10], Step [283/390], Loss: 2.4895\n",
      "Epoch [4/10], Step [284/390], Loss: 2.9790\n",
      "Epoch [4/10], Step [285/390], Loss: 2.6355\n",
      "Epoch [4/10], Step [286/390], Loss: 2.8371\n",
      "Epoch [4/10], Step [287/390], Loss: 2.8944\n",
      "Epoch [4/10], Step [288/390], Loss: 2.7758\n",
      "Epoch [4/10], Step [289/390], Loss: 2.8672\n",
      "Epoch [4/10], Step [290/390], Loss: 2.6664\n",
      "Epoch [4/10], Step [291/390], Loss: 2.8695\n",
      "Epoch [4/10], Step [292/390], Loss: 2.6229\n",
      "Epoch [4/10], Step [293/390], Loss: 2.6806\n",
      "Epoch [4/10], Step [294/390], Loss: 2.7229\n",
      "Epoch [4/10], Step [295/390], Loss: 2.8539\n",
      "Epoch [4/10], Step [296/390], Loss: 2.6145\n",
      "Epoch [4/10], Step [297/390], Loss: 2.6418\n",
      "Epoch [4/10], Step [298/390], Loss: 2.9630\n",
      "Epoch [4/10], Step [299/390], Loss: 2.6553\n",
      "Epoch [4/10], Step [300/390], Loss: 2.8519\n",
      "Epoch [4/10], Step [301/390], Loss: 2.7797\n",
      "Epoch [4/10], Step [302/390], Loss: 2.8861\n",
      "Epoch [4/10], Step [303/390], Loss: 2.8675\n",
      "Epoch [4/10], Step [304/390], Loss: 2.7825\n",
      "Epoch [4/10], Step [305/390], Loss: 2.7407\n",
      "Epoch [4/10], Step [306/390], Loss: 2.8183\n",
      "Epoch [4/10], Step [307/390], Loss: 2.9040\n",
      "Epoch [4/10], Step [308/390], Loss: 3.1085\n",
      "Epoch [4/10], Step [309/390], Loss: 2.7002\n",
      "Epoch [4/10], Step [310/390], Loss: 2.5647\n",
      "Epoch [4/10], Step [311/390], Loss: 2.8866\n",
      "Epoch [4/10], Step [312/390], Loss: 2.8182\n",
      "Epoch [4/10], Step [313/390], Loss: 2.7105\n",
      "Epoch [4/10], Step [314/390], Loss: 2.5110\n",
      "Epoch [4/10], Step [315/390], Loss: 2.7961\n",
      "Epoch [4/10], Step [316/390], Loss: 2.8974\n",
      "Epoch [4/10], Step [317/390], Loss: 2.8237\n",
      "Epoch [4/10], Step [318/390], Loss: 2.7113\n",
      "Epoch [4/10], Step [319/390], Loss: 2.7123\n",
      "Epoch [4/10], Step [320/390], Loss: 2.8003\n",
      "Epoch [4/10], Step [321/390], Loss: 2.6675\n",
      "Epoch [4/10], Step [322/390], Loss: 2.7959\n",
      "Epoch [4/10], Step [323/390], Loss: 2.7559\n",
      "Epoch [4/10], Step [324/390], Loss: 2.7105\n",
      "Epoch [4/10], Step [325/390], Loss: 2.8361\n",
      "Epoch [4/10], Step [326/390], Loss: 2.8014\n",
      "Epoch [4/10], Step [327/390], Loss: 2.5755\n",
      "Epoch [4/10], Step [328/390], Loss: 2.4517\n",
      "Epoch [4/10], Step [329/390], Loss: 2.6978\n",
      "Epoch [4/10], Step [330/390], Loss: 3.0456\n",
      "Epoch [4/10], Step [331/390], Loss: 2.6211\n",
      "Epoch [4/10], Step [332/390], Loss: 2.5937\n",
      "Epoch [4/10], Step [333/390], Loss: 2.9617\n",
      "Epoch [4/10], Step [334/390], Loss: 2.8481\n",
      "Epoch [4/10], Step [335/390], Loss: 2.8582\n",
      "Epoch [4/10], Step [336/390], Loss: 2.5711\n",
      "Epoch [4/10], Step [337/390], Loss: 2.7779\n",
      "Epoch [4/10], Step [338/390], Loss: 2.8690\n",
      "Epoch [4/10], Step [339/390], Loss: 2.4986\n",
      "Epoch [4/10], Step [340/390], Loss: 2.6701\n",
      "Epoch [4/10], Step [341/390], Loss: 2.7592\n",
      "Epoch [4/10], Step [342/390], Loss: 2.6786\n",
      "Epoch [4/10], Step [343/390], Loss: 2.7627\n",
      "Epoch [4/10], Step [344/390], Loss: 2.6262\n",
      "Epoch [4/10], Step [345/390], Loss: 2.7692\n",
      "Epoch [4/10], Step [346/390], Loss: 2.7951\n",
      "Epoch [4/10], Step [347/390], Loss: 2.7517\n",
      "Epoch [4/10], Step [348/390], Loss: 2.7450\n",
      "Epoch [4/10], Step [349/390], Loss: 3.1614\n",
      "Epoch [4/10], Step [350/390], Loss: 2.8462\n",
      "Epoch [4/10], Step [351/390], Loss: 2.7676\n",
      "Epoch [4/10], Step [352/390], Loss: 2.6444\n",
      "Epoch [4/10], Step [353/390], Loss: 2.8138\n",
      "Epoch [4/10], Step [354/390], Loss: 2.8040\n",
      "Epoch [4/10], Step [355/390], Loss: 2.5873\n",
      "Epoch [4/10], Step [356/390], Loss: 2.8258\n",
      "Epoch [4/10], Step [357/390], Loss: 2.8057\n",
      "Epoch [4/10], Step [358/390], Loss: 2.7096\n",
      "Epoch [4/10], Step [359/390], Loss: 2.7659\n",
      "Epoch [4/10], Step [360/390], Loss: 2.6548\n",
      "Epoch [4/10], Step [361/390], Loss: 2.8054\n",
      "Epoch [4/10], Step [362/390], Loss: 2.8601\n",
      "Epoch [4/10], Step [363/390], Loss: 2.5398\n",
      "Epoch [4/10], Step [364/390], Loss: 2.7035\n",
      "Epoch [4/10], Step [365/390], Loss: 2.9544\n",
      "Epoch [4/10], Step [366/390], Loss: 2.5449\n",
      "Epoch [4/10], Step [367/390], Loss: 2.6794\n",
      "Epoch [4/10], Step [368/390], Loss: 2.6829\n",
      "Epoch [4/10], Step [369/390], Loss: 2.4631\n",
      "Epoch [4/10], Step [370/390], Loss: 2.7862\n",
      "Epoch [4/10], Step [371/390], Loss: 2.9218\n",
      "Epoch [4/10], Step [372/390], Loss: 2.7856\n",
      "Epoch [4/10], Step [373/390], Loss: 2.8412\n",
      "Epoch [4/10], Step [374/390], Loss: 2.8646\n",
      "Epoch [4/10], Step [375/390], Loss: 3.1080\n",
      "Epoch [4/10], Step [376/390], Loss: 2.7306\n",
      "Epoch [4/10], Step [377/390], Loss: 2.6337\n",
      "Epoch [4/10], Step [378/390], Loss: 2.8321\n",
      "Epoch [4/10], Step [379/390], Loss: 2.5254\n",
      "Epoch [4/10], Step [380/390], Loss: 2.5611\n",
      "Epoch [4/10], Step [381/390], Loss: 2.6529\n",
      "Epoch [4/10], Step [382/390], Loss: 2.5306\n",
      "Epoch [4/10], Step [383/390], Loss: 2.9080\n",
      "Epoch [4/10], Step [384/390], Loss: 2.4889\n",
      "Epoch [4/10], Step [385/390], Loss: 2.6587\n",
      "Epoch [4/10], Step [386/390], Loss: 2.7122\n",
      "Epoch [4/10], Step [387/390], Loss: 2.8243\n",
      "Epoch [4/10], Step [388/390], Loss: 2.7372\n",
      "Epoch [4/10], Step [389/390], Loss: 2.7517\n",
      "Epoch [4/10], Step [390/390], Loss: 2.4242\n",
      "Epoch [4/10], Step [391/390], Loss: 3.2601\n",
      "Epoch [5/10], Step [1/390], Loss: 2.6430\n",
      "Epoch [5/10], Step [2/390], Loss: 2.6510\n",
      "Epoch [5/10], Step [3/390], Loss: 2.5789\n",
      "Epoch [5/10], Step [4/390], Loss: 2.5549\n",
      "Epoch [5/10], Step [5/390], Loss: 2.3504\n",
      "Epoch [5/10], Step [6/390], Loss: 2.4712\n",
      "Epoch [5/10], Step [7/390], Loss: 2.2118\n",
      "Epoch [5/10], Step [8/390], Loss: 2.6681\n",
      "Epoch [5/10], Step [9/390], Loss: 2.4365\n",
      "Epoch [5/10], Step [10/390], Loss: 2.9303\n",
      "Epoch [5/10], Step [11/390], Loss: 2.6660\n",
      "Epoch [5/10], Step [12/390], Loss: 2.5355\n",
      "Epoch [5/10], Step [13/390], Loss: 2.6100\n",
      "Epoch [5/10], Step [14/390], Loss: 2.5410\n",
      "Epoch [5/10], Step [15/390], Loss: 2.9018\n",
      "Epoch [5/10], Step [16/390], Loss: 2.4377\n",
      "Epoch [5/10], Step [17/390], Loss: 2.7191\n",
      "Epoch [5/10], Step [18/390], Loss: 2.6871\n",
      "Epoch [5/10], Step [19/390], Loss: 2.5328\n",
      "Epoch [5/10], Step [20/390], Loss: 2.5371\n",
      "Epoch [5/10], Step [21/390], Loss: 2.6658\n",
      "Epoch [5/10], Step [22/390], Loss: 2.6924\n",
      "Epoch [5/10], Step [23/390], Loss: 2.6082\n",
      "Epoch [5/10], Step [24/390], Loss: 2.4977\n",
      "Epoch [5/10], Step [25/390], Loss: 2.5742\n",
      "Epoch [5/10], Step [26/390], Loss: 2.5844\n",
      "Epoch [5/10], Step [27/390], Loss: 2.5184\n",
      "Epoch [5/10], Step [28/390], Loss: 2.3764\n",
      "Epoch [5/10], Step [29/390], Loss: 2.9617\n",
      "Epoch [5/10], Step [30/390], Loss: 2.4808\n",
      "Epoch [5/10], Step [31/390], Loss: 2.5881\n",
      "Epoch [5/10], Step [32/390], Loss: 2.5505\n",
      "Epoch [5/10], Step [33/390], Loss: 2.7953\n",
      "Epoch [5/10], Step [34/390], Loss: 2.4541\n",
      "Epoch [5/10], Step [35/390], Loss: 2.5042\n",
      "Epoch [5/10], Step [36/390], Loss: 2.7223\n",
      "Epoch [5/10], Step [37/390], Loss: 2.5887\n",
      "Epoch [5/10], Step [38/390], Loss: 2.6169\n",
      "Epoch [5/10], Step [39/390], Loss: 2.7577\n",
      "Epoch [5/10], Step [40/390], Loss: 2.7617\n",
      "Epoch [5/10], Step [41/390], Loss: 2.6925\n",
      "Epoch [5/10], Step [42/390], Loss: 2.5293\n",
      "Epoch [5/10], Step [43/390], Loss: 2.6397\n",
      "Epoch [5/10], Step [44/390], Loss: 2.6650\n",
      "Epoch [5/10], Step [45/390], Loss: 2.4428\n",
      "Epoch [5/10], Step [46/390], Loss: 2.5474\n",
      "Epoch [5/10], Step [47/390], Loss: 2.4287\n",
      "Epoch [5/10], Step [48/390], Loss: 2.8619\n",
      "Epoch [5/10], Step [49/390], Loss: 2.5296\n",
      "Epoch [5/10], Step [50/390], Loss: 2.6734\n",
      "Epoch [5/10], Step [51/390], Loss: 2.3255\n",
      "Epoch [5/10], Step [52/390], Loss: 2.6402\n",
      "Epoch [5/10], Step [53/390], Loss: 2.5908\n",
      "Epoch [5/10], Step [54/390], Loss: 2.4069\n",
      "Epoch [5/10], Step [55/390], Loss: 2.7285\n",
      "Epoch [5/10], Step [56/390], Loss: 2.5409\n",
      "Epoch [5/10], Step [57/390], Loss: 2.5628\n",
      "Epoch [5/10], Step [58/390], Loss: 2.6645\n",
      "Epoch [5/10], Step [59/390], Loss: 2.4857\n",
      "Epoch [5/10], Step [60/390], Loss: 2.6973\n",
      "Epoch [5/10], Step [61/390], Loss: 2.6363\n",
      "Epoch [5/10], Step [62/390], Loss: 2.9257\n",
      "Epoch [5/10], Step [63/390], Loss: 2.4173\n",
      "Epoch [5/10], Step [64/390], Loss: 2.5137\n",
      "Epoch [5/10], Step [65/390], Loss: 2.5246\n",
      "Epoch [5/10], Step [66/390], Loss: 2.6519\n",
      "Epoch [5/10], Step [67/390], Loss: 2.3785\n",
      "Epoch [5/10], Step [68/390], Loss: 2.4898\n",
      "Epoch [5/10], Step [69/390], Loss: 2.6256\n",
      "Epoch [5/10], Step [70/390], Loss: 2.8885\n",
      "Epoch [5/10], Step [71/390], Loss: 2.7818\n",
      "Epoch [5/10], Step [72/390], Loss: 2.7892\n",
      "Epoch [5/10], Step [73/390], Loss: 2.5637\n",
      "Epoch [5/10], Step [74/390], Loss: 2.7787\n",
      "Epoch [5/10], Step [75/390], Loss: 2.5317\n",
      "Epoch [5/10], Step [76/390], Loss: 2.6872\n",
      "Epoch [5/10], Step [77/390], Loss: 2.6667\n",
      "Epoch [5/10], Step [78/390], Loss: 2.5197\n",
      "Epoch [5/10], Step [79/390], Loss: 2.7268\n",
      "Epoch [5/10], Step [80/390], Loss: 2.6353\n",
      "Epoch [5/10], Step [81/390], Loss: 2.5179\n",
      "Epoch [5/10], Step [82/390], Loss: 2.7091\n",
      "Epoch [5/10], Step [83/390], Loss: 2.5172\n",
      "Epoch [5/10], Step [84/390], Loss: 2.3900\n",
      "Epoch [5/10], Step [85/390], Loss: 2.4882\n",
      "Epoch [5/10], Step [86/390], Loss: 2.6701\n",
      "Epoch [5/10], Step [87/390], Loss: 2.2327\n",
      "Epoch [5/10], Step [88/390], Loss: 2.8929\n",
      "Epoch [5/10], Step [89/390], Loss: 2.7272\n",
      "Epoch [5/10], Step [90/390], Loss: 2.5113\n",
      "Epoch [5/10], Step [91/390], Loss: 2.6456\n",
      "Epoch [5/10], Step [92/390], Loss: 2.3753\n",
      "Epoch [5/10], Step [93/390], Loss: 2.5431\n",
      "Epoch [5/10], Step [94/390], Loss: 2.6605\n",
      "Epoch [5/10], Step [95/390], Loss: 2.8970\n",
      "Epoch [5/10], Step [96/390], Loss: 2.4874\n",
      "Epoch [5/10], Step [97/390], Loss: 2.6735\n",
      "Epoch [5/10], Step [98/390], Loss: 2.3795\n",
      "Epoch [5/10], Step [99/390], Loss: 2.4789\n",
      "Epoch [5/10], Step [100/390], Loss: 2.5128\n",
      "Epoch [5/10], Step [101/390], Loss: 2.6424\n",
      "Epoch [5/10], Step [102/390], Loss: 2.5325\n",
      "Epoch [5/10], Step [103/390], Loss: 2.6677\n",
      "Epoch [5/10], Step [104/390], Loss: 2.7387\n",
      "Epoch [5/10], Step [105/390], Loss: 2.5597\n",
      "Epoch [5/10], Step [106/390], Loss: 2.4581\n",
      "Epoch [5/10], Step [107/390], Loss: 2.7938\n",
      "Epoch [5/10], Step [108/390], Loss: 2.5374\n",
      "Epoch [5/10], Step [109/390], Loss: 2.4358\n",
      "Epoch [5/10], Step [110/390], Loss: 2.7046\n",
      "Epoch [5/10], Step [111/390], Loss: 2.6021\n",
      "Epoch [5/10], Step [112/390], Loss: 2.6650\n",
      "Epoch [5/10], Step [113/390], Loss: 2.7983\n",
      "Epoch [5/10], Step [114/390], Loss: 2.3264\n",
      "Epoch [5/10], Step [115/390], Loss: 2.3979\n",
      "Epoch [5/10], Step [116/390], Loss: 2.5409\n",
      "Epoch [5/10], Step [117/390], Loss: 2.6246\n",
      "Epoch [5/10], Step [118/390], Loss: 2.6505\n",
      "Epoch [5/10], Step [119/390], Loss: 2.7720\n",
      "Epoch [5/10], Step [120/390], Loss: 2.2897\n",
      "Epoch [5/10], Step [121/390], Loss: 2.7842\n",
      "Epoch [5/10], Step [122/390], Loss: 2.3894\n",
      "Epoch [5/10], Step [123/390], Loss: 2.8519\n",
      "Epoch [5/10], Step [124/390], Loss: 2.6862\n",
      "Epoch [5/10], Step [125/390], Loss: 2.3395\n",
      "Epoch [5/10], Step [126/390], Loss: 2.5073\n",
      "Epoch [5/10], Step [127/390], Loss: 2.3794\n",
      "Epoch [5/10], Step [128/390], Loss: 2.4484\n",
      "Epoch [5/10], Step [129/390], Loss: 2.7581\n",
      "Epoch [5/10], Step [130/390], Loss: 2.7547\n",
      "Epoch [5/10], Step [131/390], Loss: 2.4394\n",
      "Epoch [5/10], Step [132/390], Loss: 2.5627\n",
      "Epoch [5/10], Step [133/390], Loss: 2.3051\n",
      "Epoch [5/10], Step [134/390], Loss: 2.9014\n",
      "Epoch [5/10], Step [135/390], Loss: 2.4225\n",
      "Epoch [5/10], Step [136/390], Loss: 2.3781\n",
      "Epoch [5/10], Step [137/390], Loss: 2.4148\n",
      "Epoch [5/10], Step [138/390], Loss: 2.5067\n",
      "Epoch [5/10], Step [139/390], Loss: 2.8742\n",
      "Epoch [5/10], Step [140/390], Loss: 2.5000\n",
      "Epoch [5/10], Step [141/390], Loss: 2.6350\n",
      "Epoch [5/10], Step [142/390], Loss: 2.7767\n",
      "Epoch [5/10], Step [143/390], Loss: 2.6156\n",
      "Epoch [5/10], Step [144/390], Loss: 2.4420\n",
      "Epoch [5/10], Step [145/390], Loss: 2.6509\n",
      "Epoch [5/10], Step [146/390], Loss: 2.6793\n",
      "Epoch [5/10], Step [147/390], Loss: 2.5848\n",
      "Epoch [5/10], Step [148/390], Loss: 2.7088\n",
      "Epoch [5/10], Step [149/390], Loss: 2.5225\n",
      "Epoch [5/10], Step [150/390], Loss: 2.6477\n",
      "Epoch [5/10], Step [151/390], Loss: 2.6661\n",
      "Epoch [5/10], Step [152/390], Loss: 2.6815\n",
      "Epoch [5/10], Step [153/390], Loss: 2.6144\n",
      "Epoch [5/10], Step [154/390], Loss: 2.5442\n",
      "Epoch [5/10], Step [155/390], Loss: 2.7257\n",
      "Epoch [5/10], Step [156/390], Loss: 2.6163\n",
      "Epoch [5/10], Step [157/390], Loss: 2.6749\n",
      "Epoch [5/10], Step [158/390], Loss: 2.4556\n",
      "Epoch [5/10], Step [159/390], Loss: 2.6913\n",
      "Epoch [5/10], Step [160/390], Loss: 2.7086\n",
      "Epoch [5/10], Step [161/390], Loss: 2.6659\n",
      "Epoch [5/10], Step [162/390], Loss: 2.4982\n",
      "Epoch [5/10], Step [163/390], Loss: 2.4942\n",
      "Epoch [5/10], Step [164/390], Loss: 2.6627\n",
      "Epoch [5/10], Step [165/390], Loss: 2.4755\n",
      "Epoch [5/10], Step [166/390], Loss: 2.5249\n",
      "Epoch [5/10], Step [167/390], Loss: 2.6223\n",
      "Epoch [5/10], Step [168/390], Loss: 2.4070\n",
      "Epoch [5/10], Step [169/390], Loss: 2.4328\n",
      "Epoch [5/10], Step [170/390], Loss: 2.7645\n",
      "Epoch [5/10], Step [171/390], Loss: 2.3793\n",
      "Epoch [5/10], Step [172/390], Loss: 2.5222\n",
      "Epoch [5/10], Step [173/390], Loss: 2.5880\n",
      "Epoch [5/10], Step [174/390], Loss: 2.6515\n",
      "Epoch [5/10], Step [175/390], Loss: 2.5731\n",
      "Epoch [5/10], Step [176/390], Loss: 2.5708\n",
      "Epoch [5/10], Step [177/390], Loss: 2.8507\n",
      "Epoch [5/10], Step [178/390], Loss: 2.6478\n",
      "Epoch [5/10], Step [179/390], Loss: 2.4770\n",
      "Epoch [5/10], Step [180/390], Loss: 2.4506\n",
      "Epoch [5/10], Step [181/390], Loss: 2.6718\n",
      "Epoch [5/10], Step [182/390], Loss: 2.6198\n",
      "Epoch [5/10], Step [183/390], Loss: 2.8994\n",
      "Epoch [5/10], Step [184/390], Loss: 2.4894\n",
      "Epoch [5/10], Step [185/390], Loss: 2.4962\n",
      "Epoch [5/10], Step [186/390], Loss: 2.5425\n",
      "Epoch [5/10], Step [187/390], Loss: 2.4913\n",
      "Epoch [5/10], Step [188/390], Loss: 2.5531\n",
      "Epoch [5/10], Step [189/390], Loss: 2.7366\n",
      "Epoch [5/10], Step [190/390], Loss: 2.5679\n",
      "Epoch [5/10], Step [191/390], Loss: 2.4651\n",
      "Epoch [5/10], Step [192/390], Loss: 2.6653\n",
      "Epoch [5/10], Step [193/390], Loss: 2.6645\n",
      "Epoch [5/10], Step [194/390], Loss: 2.4492\n",
      "Epoch [5/10], Step [195/390], Loss: 2.6526\n",
      "Epoch [5/10], Step [196/390], Loss: 2.6785\n",
      "Epoch [5/10], Step [197/390], Loss: 2.7758\n",
      "Epoch [5/10], Step [198/390], Loss: 2.4898\n",
      "Epoch [5/10], Step [199/390], Loss: 2.7528\n",
      "Epoch [5/10], Step [200/390], Loss: 3.0838\n",
      "Epoch [5/10], Step [201/390], Loss: 2.6694\n",
      "Epoch [5/10], Step [202/390], Loss: 2.5676\n",
      "Epoch [5/10], Step [203/390], Loss: 2.6212\n",
      "Epoch [5/10], Step [204/390], Loss: 2.7043\n",
      "Epoch [5/10], Step [205/390], Loss: 2.4865\n",
      "Epoch [5/10], Step [206/390], Loss: 2.6153\n",
      "Epoch [5/10], Step [207/390], Loss: 2.6857\n",
      "Epoch [5/10], Step [208/390], Loss: 2.2415\n",
      "Epoch [5/10], Step [209/390], Loss: 2.6161\n",
      "Epoch [5/10], Step [210/390], Loss: 2.8987\n",
      "Epoch [5/10], Step [211/390], Loss: 2.6018\n",
      "Epoch [5/10], Step [212/390], Loss: 2.6403\n",
      "Epoch [5/10], Step [213/390], Loss: 2.7596\n",
      "Epoch [5/10], Step [214/390], Loss: 2.4458\n",
      "Epoch [5/10], Step [215/390], Loss: 2.5725\n",
      "Epoch [5/10], Step [216/390], Loss: 2.6033\n",
      "Epoch [5/10], Step [217/390], Loss: 2.7452\n",
      "Epoch [5/10], Step [218/390], Loss: 2.3570\n",
      "Epoch [5/10], Step [219/390], Loss: 2.5763\n",
      "Epoch [5/10], Step [220/390], Loss: 2.6841\n",
      "Epoch [5/10], Step [221/390], Loss: 2.5951\n",
      "Epoch [5/10], Step [222/390], Loss: 2.4492\n",
      "Epoch [5/10], Step [223/390], Loss: 2.6902\n",
      "Epoch [5/10], Step [224/390], Loss: 2.5101\n",
      "Epoch [5/10], Step [225/390], Loss: 2.5608\n",
      "Epoch [5/10], Step [226/390], Loss: 2.6186\n",
      "Epoch [5/10], Step [227/390], Loss: 2.5423\n",
      "Epoch [5/10], Step [228/390], Loss: 2.5096\n",
      "Epoch [5/10], Step [229/390], Loss: 2.8340\n",
      "Epoch [5/10], Step [230/390], Loss: 2.5146\n",
      "Epoch [5/10], Step [231/390], Loss: 2.3410\n",
      "Epoch [5/10], Step [232/390], Loss: 2.5680\n",
      "Epoch [5/10], Step [233/390], Loss: 2.3078\n",
      "Epoch [5/10], Step [234/390], Loss: 2.4685\n",
      "Epoch [5/10], Step [235/390], Loss: 2.3631\n",
      "Epoch [5/10], Step [236/390], Loss: 2.7126\n",
      "Epoch [5/10], Step [237/390], Loss: 2.5421\n",
      "Epoch [5/10], Step [238/390], Loss: 2.4844\n",
      "Epoch [5/10], Step [239/390], Loss: 2.6154\n",
      "Epoch [5/10], Step [240/390], Loss: 2.2715\n",
      "Epoch [5/10], Step [241/390], Loss: 2.5480\n",
      "Epoch [5/10], Step [242/390], Loss: 2.5506\n",
      "Epoch [5/10], Step [243/390], Loss: 2.4745\n",
      "Epoch [5/10], Step [244/390], Loss: 2.5840\n",
      "Epoch [5/10], Step [245/390], Loss: 2.3497\n",
      "Epoch [5/10], Step [246/390], Loss: 2.6512\n",
      "Epoch [5/10], Step [247/390], Loss: 2.6634\n",
      "Epoch [5/10], Step [248/390], Loss: 2.4259\n",
      "Epoch [5/10], Step [249/390], Loss: 2.7976\n",
      "Epoch [5/10], Step [250/390], Loss: 2.8725\n",
      "Epoch [5/10], Step [251/390], Loss: 2.3638\n",
      "Epoch [5/10], Step [252/390], Loss: 2.2994\n",
      "Epoch [5/10], Step [253/390], Loss: 2.4393\n",
      "Epoch [5/10], Step [254/390], Loss: 2.6828\n",
      "Epoch [5/10], Step [255/390], Loss: 2.4126\n",
      "Epoch [5/10], Step [256/390], Loss: 2.5854\n",
      "Epoch [5/10], Step [257/390], Loss: 2.4239\n",
      "Epoch [5/10], Step [258/390], Loss: 2.6140\n",
      "Epoch [5/10], Step [259/390], Loss: 2.6397\n",
      "Epoch [5/10], Step [260/390], Loss: 2.6924\n",
      "Epoch [5/10], Step [261/390], Loss: 2.5660\n",
      "Epoch [5/10], Step [262/390], Loss: 2.4410\n",
      "Epoch [5/10], Step [263/390], Loss: 2.4408\n",
      "Epoch [5/10], Step [264/390], Loss: 2.9291\n",
      "Epoch [5/10], Step [265/390], Loss: 2.4275\n",
      "Epoch [5/10], Step [266/390], Loss: 2.5633\n",
      "Epoch [5/10], Step [267/390], Loss: 2.3665\n",
      "Epoch [5/10], Step [268/390], Loss: 2.4491\n",
      "Epoch [5/10], Step [269/390], Loss: 2.5749\n",
      "Epoch [5/10], Step [270/390], Loss: 2.7107\n",
      "Epoch [5/10], Step [271/390], Loss: 2.7299\n",
      "Epoch [5/10], Step [272/390], Loss: 2.5758\n",
      "Epoch [5/10], Step [273/390], Loss: 2.6269\n",
      "Epoch [5/10], Step [274/390], Loss: 2.1802\n",
      "Epoch [5/10], Step [275/390], Loss: 2.8366\n",
      "Epoch [5/10], Step [276/390], Loss: 2.4692\n",
      "Epoch [5/10], Step [277/390], Loss: 2.1633\n",
      "Epoch [5/10], Step [278/390], Loss: 2.6299\n",
      "Epoch [5/10], Step [279/390], Loss: 2.5769\n",
      "Epoch [5/10], Step [280/390], Loss: 3.0000\n",
      "Epoch [5/10], Step [281/390], Loss: 2.8649\n",
      "Epoch [5/10], Step [282/390], Loss: 2.5279\n",
      "Epoch [5/10], Step [283/390], Loss: 2.7408\n",
      "Epoch [5/10], Step [284/390], Loss: 2.6597\n",
      "Epoch [5/10], Step [285/390], Loss: 2.4150\n",
      "Epoch [5/10], Step [286/390], Loss: 2.6616\n",
      "Epoch [5/10], Step [287/390], Loss: 2.4365\n",
      "Epoch [5/10], Step [288/390], Loss: 2.7017\n",
      "Epoch [5/10], Step [289/390], Loss: 2.6248\n",
      "Epoch [5/10], Step [290/390], Loss: 2.5862\n",
      "Epoch [5/10], Step [291/390], Loss: 2.3438\n",
      "Epoch [5/10], Step [292/390], Loss: 2.4052\n",
      "Epoch [5/10], Step [293/390], Loss: 2.4661\n",
      "Epoch [5/10], Step [294/390], Loss: 2.4563\n",
      "Epoch [5/10], Step [295/390], Loss: 2.5832\n",
      "Epoch [5/10], Step [296/390], Loss: 2.2661\n",
      "Epoch [5/10], Step [297/390], Loss: 2.7401\n",
      "Epoch [5/10], Step [298/390], Loss: 2.5630\n",
      "Epoch [5/10], Step [299/390], Loss: 2.8862\n",
      "Epoch [5/10], Step [300/390], Loss: 2.7201\n",
      "Epoch [5/10], Step [301/390], Loss: 2.4642\n",
      "Epoch [5/10], Step [302/390], Loss: 2.6281\n",
      "Epoch [5/10], Step [303/390], Loss: 2.4325\n",
      "Epoch [5/10], Step [304/390], Loss: 2.5598\n",
      "Epoch [5/10], Step [305/390], Loss: 2.4474\n",
      "Epoch [5/10], Step [306/390], Loss: 2.4700\n",
      "Epoch [5/10], Step [307/390], Loss: 2.4823\n",
      "Epoch [5/10], Step [308/390], Loss: 2.7006\n",
      "Epoch [5/10], Step [309/390], Loss: 2.5072\n",
      "Epoch [5/10], Step [310/390], Loss: 2.5772\n",
      "Epoch [5/10], Step [311/390], Loss: 2.4747\n",
      "Epoch [5/10], Step [312/390], Loss: 2.7337\n",
      "Epoch [5/10], Step [313/390], Loss: 2.5748\n",
      "Epoch [5/10], Step [314/390], Loss: 2.5681\n",
      "Epoch [5/10], Step [315/390], Loss: 2.5138\n",
      "Epoch [5/10], Step [316/390], Loss: 2.4493\n",
      "Epoch [5/10], Step [317/390], Loss: 2.4190\n",
      "Epoch [5/10], Step [318/390], Loss: 2.3865\n",
      "Epoch [5/10], Step [319/390], Loss: 2.2962\n",
      "Epoch [5/10], Step [320/390], Loss: 2.5464\n",
      "Epoch [5/10], Step [321/390], Loss: 2.4876\n",
      "Epoch [5/10], Step [322/390], Loss: 2.6733\n",
      "Epoch [5/10], Step [323/390], Loss: 2.2935\n",
      "Epoch [5/10], Step [324/390], Loss: 2.3894\n",
      "Epoch [5/10], Step [325/390], Loss: 2.3184\n",
      "Epoch [5/10], Step [326/390], Loss: 2.4923\n",
      "Epoch [5/10], Step [327/390], Loss: 2.4613\n",
      "Epoch [5/10], Step [328/390], Loss: 2.5818\n",
      "Epoch [5/10], Step [329/390], Loss: 2.4562\n",
      "Epoch [5/10], Step [330/390], Loss: 2.6245\n",
      "Epoch [5/10], Step [331/390], Loss: 2.2368\n",
      "Epoch [5/10], Step [332/390], Loss: 2.7465\n",
      "Epoch [5/10], Step [333/390], Loss: 2.3801\n",
      "Epoch [5/10], Step [334/390], Loss: 2.1369\n",
      "Epoch [5/10], Step [335/390], Loss: 2.6441\n",
      "Epoch [5/10], Step [336/390], Loss: 2.7179\n",
      "Epoch [5/10], Step [337/390], Loss: 2.4596\n",
      "Epoch [5/10], Step [338/390], Loss: 2.5949\n",
      "Epoch [5/10], Step [339/390], Loss: 2.5568\n",
      "Epoch [5/10], Step [340/390], Loss: 2.6796\n",
      "Epoch [5/10], Step [341/390], Loss: 2.6561\n",
      "Epoch [5/10], Step [342/390], Loss: 2.3996\n",
      "Epoch [5/10], Step [343/390], Loss: 2.5075\n",
      "Epoch [5/10], Step [344/390], Loss: 2.5488\n",
      "Epoch [5/10], Step [345/390], Loss: 2.2014\n",
      "Epoch [5/10], Step [346/390], Loss: 2.5548\n",
      "Epoch [5/10], Step [347/390], Loss: 2.4321\n",
      "Epoch [5/10], Step [348/390], Loss: 2.6312\n",
      "Epoch [5/10], Step [349/390], Loss: 2.4470\n",
      "Epoch [5/10], Step [350/390], Loss: 2.4202\n",
      "Epoch [5/10], Step [351/390], Loss: 2.4980\n",
      "Epoch [5/10], Step [352/390], Loss: 2.6547\n",
      "Epoch [5/10], Step [353/390], Loss: 2.4521\n",
      "Epoch [5/10], Step [354/390], Loss: 2.5457\n",
      "Epoch [5/10], Step [355/390], Loss: 2.3955\n",
      "Epoch [5/10], Step [356/390], Loss: 2.4085\n",
      "Epoch [5/10], Step [357/390], Loss: 2.4682\n",
      "Epoch [5/10], Step [358/390], Loss: 2.4605\n",
      "Epoch [5/10], Step [359/390], Loss: 2.3170\n",
      "Epoch [5/10], Step [360/390], Loss: 2.6374\n",
      "Epoch [5/10], Step [361/390], Loss: 2.3868\n",
      "Epoch [5/10], Step [362/390], Loss: 2.4298\n",
      "Epoch [5/10], Step [363/390], Loss: 2.4929\n",
      "Epoch [5/10], Step [364/390], Loss: 2.4698\n",
      "Epoch [5/10], Step [365/390], Loss: 2.3125\n",
      "Epoch [5/10], Step [366/390], Loss: 2.5951\n",
      "Epoch [5/10], Step [367/390], Loss: 2.5153\n",
      "Epoch [5/10], Step [368/390], Loss: 2.3109\n",
      "Epoch [5/10], Step [369/390], Loss: 2.4844\n",
      "Epoch [5/10], Step [370/390], Loss: 2.4563\n",
      "Epoch [5/10], Step [371/390], Loss: 2.4317\n",
      "Epoch [5/10], Step [372/390], Loss: 2.5962\n",
      "Epoch [5/10], Step [373/390], Loss: 2.5626\n",
      "Epoch [5/10], Step [374/390], Loss: 2.6657\n",
      "Epoch [5/10], Step [375/390], Loss: 2.4909\n",
      "Epoch [5/10], Step [376/390], Loss: 2.4215\n",
      "Epoch [5/10], Step [377/390], Loss: 2.3742\n",
      "Epoch [5/10], Step [378/390], Loss: 2.6117\n",
      "Epoch [5/10], Step [379/390], Loss: 2.5117\n",
      "Epoch [5/10], Step [380/390], Loss: 2.6505\n",
      "Epoch [5/10], Step [381/390], Loss: 2.5980\n",
      "Epoch [5/10], Step [382/390], Loss: 2.5629\n",
      "Epoch [5/10], Step [383/390], Loss: 2.7180\n",
      "Epoch [5/10], Step [384/390], Loss: 2.4931\n",
      "Epoch [5/10], Step [385/390], Loss: 2.5077\n",
      "Epoch [5/10], Step [386/390], Loss: 2.8599\n",
      "Epoch [5/10], Step [387/390], Loss: 2.4496\n",
      "Epoch [5/10], Step [388/390], Loss: 2.6485\n",
      "Epoch [5/10], Step [389/390], Loss: 2.2073\n",
      "Epoch [5/10], Step [390/390], Loss: 2.5515\n",
      "Epoch [5/10], Step [391/390], Loss: 2.3140\n",
      "Epoch [6/10], Step [1/390], Loss: 2.3826\n",
      "Epoch [6/10], Step [2/390], Loss: 2.3005\n",
      "Epoch [6/10], Step [3/390], Loss: 2.6075\n",
      "Epoch [6/10], Step [4/390], Loss: 2.3240\n",
      "Epoch [6/10], Step [5/390], Loss: 2.5786\n",
      "Epoch [6/10], Step [6/390], Loss: 2.5888\n",
      "Epoch [6/10], Step [7/390], Loss: 2.1787\n",
      "Epoch [6/10], Step [8/390], Loss: 2.3730\n",
      "Epoch [6/10], Step [9/390], Loss: 2.6293\n",
      "Epoch [6/10], Step [10/390], Loss: 2.4324\n",
      "Epoch [6/10], Step [11/390], Loss: 2.3068\n",
      "Epoch [6/10], Step [12/390], Loss: 2.4728\n",
      "Epoch [6/10], Step [13/390], Loss: 2.4177\n",
      "Epoch [6/10], Step [14/390], Loss: 2.2894\n",
      "Epoch [6/10], Step [15/390], Loss: 2.3379\n",
      "Epoch [6/10], Step [16/390], Loss: 2.1828\n",
      "Epoch [6/10], Step [17/390], Loss: 2.4401\n",
      "Epoch [6/10], Step [18/390], Loss: 2.1782\n",
      "Epoch [6/10], Step [19/390], Loss: 2.1784\n",
      "Epoch [6/10], Step [20/390], Loss: 2.4026\n",
      "Epoch [6/10], Step [21/390], Loss: 2.3521\n",
      "Epoch [6/10], Step [22/390], Loss: 2.5319\n",
      "Epoch [6/10], Step [23/390], Loss: 2.2111\n",
      "Epoch [6/10], Step [24/390], Loss: 2.3455\n",
      "Epoch [6/10], Step [25/390], Loss: 2.2412\n",
      "Epoch [6/10], Step [26/390], Loss: 2.3912\n",
      "Epoch [6/10], Step [27/390], Loss: 2.2410\n",
      "Epoch [6/10], Step [28/390], Loss: 2.3291\n",
      "Epoch [6/10], Step [29/390], Loss: 2.4814\n",
      "Epoch [6/10], Step [30/390], Loss: 2.3543\n",
      "Epoch [6/10], Step [31/390], Loss: 2.0732\n",
      "Epoch [6/10], Step [32/390], Loss: 2.3100\n",
      "Epoch [6/10], Step [33/390], Loss: 2.2111\n",
      "Epoch [6/10], Step [34/390], Loss: 2.4842\n",
      "Epoch [6/10], Step [35/390], Loss: 2.2435\n",
      "Epoch [6/10], Step [36/390], Loss: 2.2526\n",
      "Epoch [6/10], Step [37/390], Loss: 2.6046\n",
      "Epoch [6/10], Step [38/390], Loss: 2.5743\n",
      "Epoch [6/10], Step [39/390], Loss: 2.4698\n",
      "Epoch [6/10], Step [40/390], Loss: 2.3829\n",
      "Epoch [6/10], Step [41/390], Loss: 2.5033\n",
      "Epoch [6/10], Step [42/390], Loss: 2.3113\n",
      "Epoch [6/10], Step [43/390], Loss: 2.3427\n",
      "Epoch [6/10], Step [44/390], Loss: 2.1614\n",
      "Epoch [6/10], Step [45/390], Loss: 2.3150\n",
      "Epoch [6/10], Step [46/390], Loss: 2.5788\n",
      "Epoch [6/10], Step [47/390], Loss: 2.2420\n",
      "Epoch [6/10], Step [48/390], Loss: 2.4891\n",
      "Epoch [6/10], Step [49/390], Loss: 2.2400\n",
      "Epoch [6/10], Step [50/390], Loss: 2.2246\n",
      "Epoch [6/10], Step [51/390], Loss: 2.1100\n",
      "Epoch [6/10], Step [52/390], Loss: 2.4466\n",
      "Epoch [6/10], Step [53/390], Loss: 2.2329\n",
      "Epoch [6/10], Step [54/390], Loss: 2.2265\n",
      "Epoch [6/10], Step [55/390], Loss: 2.3754\n",
      "Epoch [6/10], Step [56/390], Loss: 2.6012\n",
      "Epoch [6/10], Step [57/390], Loss: 2.2298\n",
      "Epoch [6/10], Step [58/390], Loss: 2.4555\n",
      "Epoch [6/10], Step [59/390], Loss: 2.5475\n",
      "Epoch [6/10], Step [60/390], Loss: 2.2541\n",
      "Epoch [6/10], Step [61/390], Loss: 2.1737\n",
      "Epoch [6/10], Step [62/390], Loss: 2.1008\n",
      "Epoch [6/10], Step [63/390], Loss: 2.2738\n",
      "Epoch [6/10], Step [64/390], Loss: 2.3736\n",
      "Epoch [6/10], Step [65/390], Loss: 2.3023\n",
      "Epoch [6/10], Step [66/390], Loss: 2.3259\n",
      "Epoch [6/10], Step [67/390], Loss: 2.2594\n",
      "Epoch [6/10], Step [68/390], Loss: 2.2814\n",
      "Epoch [6/10], Step [69/390], Loss: 2.4034\n",
      "Epoch [6/10], Step [70/390], Loss: 2.2837\n",
      "Epoch [6/10], Step [71/390], Loss: 2.0453\n",
      "Epoch [6/10], Step [72/390], Loss: 2.5792\n",
      "Epoch [6/10], Step [73/390], Loss: 2.3355\n",
      "Epoch [6/10], Step [74/390], Loss: 2.3512\n",
      "Epoch [6/10], Step [75/390], Loss: 2.2381\n",
      "Epoch [6/10], Step [76/390], Loss: 2.2163\n",
      "Epoch [6/10], Step [77/390], Loss: 2.2923\n",
      "Epoch [6/10], Step [78/390], Loss: 2.2178\n",
      "Epoch [6/10], Step [79/390], Loss: 2.5369\n",
      "Epoch [6/10], Step [80/390], Loss: 2.3919\n",
      "Epoch [6/10], Step [81/390], Loss: 2.3218\n",
      "Epoch [6/10], Step [82/390], Loss: 2.2276\n",
      "Epoch [6/10], Step [83/390], Loss: 2.3072\n",
      "Epoch [6/10], Step [84/390], Loss: 2.4877\n",
      "Epoch [6/10], Step [85/390], Loss: 2.2602\n",
      "Epoch [6/10], Step [86/390], Loss: 2.4046\n",
      "Epoch [6/10], Step [87/390], Loss: 2.3085\n",
      "Epoch [6/10], Step [88/390], Loss: 2.2405\n",
      "Epoch [6/10], Step [89/390], Loss: 2.1914\n",
      "Epoch [6/10], Step [90/390], Loss: 2.2820\n",
      "Epoch [6/10], Step [91/390], Loss: 2.3139\n",
      "Epoch [6/10], Step [92/390], Loss: 2.1968\n",
      "Epoch [6/10], Step [93/390], Loss: 2.5395\n",
      "Epoch [6/10], Step [94/390], Loss: 2.3617\n",
      "Epoch [6/10], Step [95/390], Loss: 2.4064\n",
      "Epoch [6/10], Step [96/390], Loss: 2.3920\n",
      "Epoch [6/10], Step [97/390], Loss: 2.4078\n",
      "Epoch [6/10], Step [98/390], Loss: 2.5703\n",
      "Epoch [6/10], Step [99/390], Loss: 2.4175\n",
      "Epoch [6/10], Step [100/390], Loss: 2.5834\n",
      "Epoch [6/10], Step [101/390], Loss: 2.2791\n",
      "Epoch [6/10], Step [102/390], Loss: 2.3180\n",
      "Epoch [6/10], Step [103/390], Loss: 2.4116\n",
      "Epoch [6/10], Step [104/390], Loss: 2.3529\n",
      "Epoch [6/10], Step [105/390], Loss: 2.2659\n",
      "Epoch [6/10], Step [106/390], Loss: 2.4858\n",
      "Epoch [6/10], Step [107/390], Loss: 2.3577\n",
      "Epoch [6/10], Step [108/390], Loss: 2.5984\n",
      "Epoch [6/10], Step [109/390], Loss: 2.6026\n",
      "Epoch [6/10], Step [110/390], Loss: 2.5624\n",
      "Epoch [6/10], Step [111/390], Loss: 2.5508\n",
      "Epoch [6/10], Step [112/390], Loss: 2.5386\n",
      "Epoch [6/10], Step [113/390], Loss: 2.5347\n",
      "Epoch [6/10], Step [114/390], Loss: 2.2807\n",
      "Epoch [6/10], Step [115/390], Loss: 2.3465\n",
      "Epoch [6/10], Step [116/390], Loss: 2.3604\n",
      "Epoch [6/10], Step [117/390], Loss: 2.3206\n",
      "Epoch [6/10], Step [118/390], Loss: 2.2424\n",
      "Epoch [6/10], Step [119/390], Loss: 2.2303\n",
      "Epoch [6/10], Step [120/390], Loss: 2.3182\n",
      "Epoch [6/10], Step [121/390], Loss: 2.4262\n",
      "Epoch [6/10], Step [122/390], Loss: 2.2720\n",
      "Epoch [6/10], Step [123/390], Loss: 2.0812\n",
      "Epoch [6/10], Step [124/390], Loss: 2.4843\n",
      "Epoch [6/10], Step [125/390], Loss: 2.4008\n",
      "Epoch [6/10], Step [126/390], Loss: 2.2001\n",
      "Epoch [6/10], Step [127/390], Loss: 2.1740\n",
      "Epoch [6/10], Step [128/390], Loss: 2.1469\n",
      "Epoch [6/10], Step [129/390], Loss: 2.3407\n",
      "Epoch [6/10], Step [130/390], Loss: 2.3552\n",
      "Epoch [6/10], Step [131/390], Loss: 2.3542\n",
      "Epoch [6/10], Step [132/390], Loss: 2.5273\n",
      "Epoch [6/10], Step [133/390], Loss: 2.4159\n",
      "Epoch [6/10], Step [134/390], Loss: 2.2172\n",
      "Epoch [6/10], Step [135/390], Loss: 2.3872\n",
      "Epoch [6/10], Step [136/390], Loss: 2.2881\n",
      "Epoch [6/10], Step [137/390], Loss: 2.5083\n",
      "Epoch [6/10], Step [138/390], Loss: 2.3441\n",
      "Epoch [6/10], Step [139/390], Loss: 2.4028\n",
      "Epoch [6/10], Step [140/390], Loss: 2.6733\n",
      "Epoch [6/10], Step [141/390], Loss: 2.5622\n",
      "Epoch [6/10], Step [142/390], Loss: 2.3817\n",
      "Epoch [6/10], Step [143/390], Loss: 2.3065\n",
      "Epoch [6/10], Step [144/390], Loss: 2.3177\n",
      "Epoch [6/10], Step [145/390], Loss: 2.2605\n",
      "Epoch [6/10], Step [146/390], Loss: 2.4339\n",
      "Epoch [6/10], Step [147/390], Loss: 2.3834\n",
      "Epoch [6/10], Step [148/390], Loss: 2.2328\n",
      "Epoch [6/10], Step [149/390], Loss: 2.4566\n",
      "Epoch [6/10], Step [150/390], Loss: 2.0166\n",
      "Epoch [6/10], Step [151/390], Loss: 2.3609\n",
      "Epoch [6/10], Step [152/390], Loss: 2.1180\n",
      "Epoch [6/10], Step [153/390], Loss: 2.1724\n",
      "Epoch [6/10], Step [154/390], Loss: 2.1813\n",
      "Epoch [6/10], Step [155/390], Loss: 2.3320\n",
      "Epoch [6/10], Step [156/390], Loss: 2.2679\n",
      "Epoch [6/10], Step [157/390], Loss: 2.4016\n",
      "Epoch [6/10], Step [158/390], Loss: 2.4981\n",
      "Epoch [6/10], Step [159/390], Loss: 2.2375\n",
      "Epoch [6/10], Step [160/390], Loss: 2.3495\n",
      "Epoch [6/10], Step [161/390], Loss: 2.0900\n",
      "Epoch [6/10], Step [162/390], Loss: 2.1253\n",
      "Epoch [6/10], Step [163/390], Loss: 2.5088\n",
      "Epoch [6/10], Step [164/390], Loss: 2.1111\n",
      "Epoch [6/10], Step [165/390], Loss: 2.2030\n",
      "Epoch [6/10], Step [166/390], Loss: 2.1212\n",
      "Epoch [6/10], Step [167/390], Loss: 2.4589\n",
      "Epoch [6/10], Step [168/390], Loss: 2.3836\n",
      "Epoch [6/10], Step [169/390], Loss: 2.1226\n",
      "Epoch [6/10], Step [170/390], Loss: 2.2791\n",
      "Epoch [6/10], Step [171/390], Loss: 2.4229\n",
      "Epoch [6/10], Step [172/390], Loss: 2.2486\n",
      "Epoch [6/10], Step [173/390], Loss: 2.3034\n",
      "Epoch [6/10], Step [174/390], Loss: 2.3589\n",
      "Epoch [6/10], Step [175/390], Loss: 2.1398\n",
      "Epoch [6/10], Step [176/390], Loss: 2.4301\n",
      "Epoch [6/10], Step [177/390], Loss: 2.2097\n",
      "Epoch [6/10], Step [178/390], Loss: 2.4994\n",
      "Epoch [6/10], Step [179/390], Loss: 2.0815\n",
      "Epoch [6/10], Step [180/390], Loss: 2.2735\n",
      "Epoch [6/10], Step [181/390], Loss: 2.4400\n",
      "Epoch [6/10], Step [182/390], Loss: 2.4827\n",
      "Epoch [6/10], Step [183/390], Loss: 2.4434\n",
      "Epoch [6/10], Step [184/390], Loss: 2.5758\n",
      "Epoch [6/10], Step [185/390], Loss: 2.4174\n",
      "Epoch [6/10], Step [186/390], Loss: 2.1296\n",
      "Epoch [6/10], Step [187/390], Loss: 2.3865\n",
      "Epoch [6/10], Step [188/390], Loss: 2.5271\n",
      "Epoch [6/10], Step [189/390], Loss: 2.3185\n",
      "Epoch [6/10], Step [190/390], Loss: 2.4122\n",
      "Epoch [6/10], Step [191/390], Loss: 2.4873\n",
      "Epoch [6/10], Step [192/390], Loss: 2.3399\n",
      "Epoch [6/10], Step [193/390], Loss: 2.4855\n",
      "Epoch [6/10], Step [194/390], Loss: 2.2567\n",
      "Epoch [6/10], Step [195/390], Loss: 2.1839\n",
      "Epoch [6/10], Step [196/390], Loss: 2.3931\n",
      "Epoch [6/10], Step [197/390], Loss: 2.4180\n",
      "Epoch [6/10], Step [198/390], Loss: 2.2978\n",
      "Epoch [6/10], Step [199/390], Loss: 2.5110\n",
      "Epoch [6/10], Step [200/390], Loss: 2.5741\n",
      "Epoch [6/10], Step [201/390], Loss: 2.2651\n",
      "Epoch [6/10], Step [202/390], Loss: 2.4770\n",
      "Epoch [6/10], Step [203/390], Loss: 2.3184\n",
      "Epoch [6/10], Step [204/390], Loss: 2.5111\n",
      "Epoch [6/10], Step [205/390], Loss: 2.3093\n",
      "Epoch [6/10], Step [206/390], Loss: 2.5486\n",
      "Epoch [6/10], Step [207/390], Loss: 2.2950\n",
      "Epoch [6/10], Step [208/390], Loss: 2.2857\n",
      "Epoch [6/10], Step [209/390], Loss: 2.2269\n",
      "Epoch [6/10], Step [210/390], Loss: 2.3659\n",
      "Epoch [6/10], Step [211/390], Loss: 2.4725\n",
      "Epoch [6/10], Step [212/390], Loss: 2.2977\n",
      "Epoch [6/10], Step [213/390], Loss: 2.3781\n",
      "Epoch [6/10], Step [214/390], Loss: 2.3467\n",
      "Epoch [6/10], Step [215/390], Loss: 2.5087\n",
      "Epoch [6/10], Step [216/390], Loss: 2.4344\n",
      "Epoch [6/10], Step [217/390], Loss: 2.4524\n",
      "Epoch [6/10], Step [218/390], Loss: 2.1752\n",
      "Epoch [6/10], Step [219/390], Loss: 2.2181\n",
      "Epoch [6/10], Step [220/390], Loss: 2.3075\n",
      "Epoch [6/10], Step [221/390], Loss: 2.2294\n",
      "Epoch [6/10], Step [222/390], Loss: 2.2098\n",
      "Epoch [6/10], Step [223/390], Loss: 2.3012\n",
      "Epoch [6/10], Step [224/390], Loss: 2.0660\n",
      "Epoch [6/10], Step [225/390], Loss: 2.4231\n",
      "Epoch [6/10], Step [226/390], Loss: 2.2774\n",
      "Epoch [6/10], Step [227/390], Loss: 2.1792\n",
      "Epoch [6/10], Step [228/390], Loss: 2.4456\n",
      "Epoch [6/10], Step [229/390], Loss: 2.3664\n",
      "Epoch [6/10], Step [230/390], Loss: 1.7900\n",
      "Epoch [6/10], Step [231/390], Loss: 2.4307\n",
      "Epoch [6/10], Step [232/390], Loss: 2.3926\n",
      "Epoch [6/10], Step [233/390], Loss: 2.4743\n",
      "Epoch [6/10], Step [234/390], Loss: 1.9799\n",
      "Epoch [6/10], Step [235/390], Loss: 2.3180\n",
      "Epoch [6/10], Step [236/390], Loss: 2.3941\n",
      "Epoch [6/10], Step [237/390], Loss: 2.3775\n",
      "Epoch [6/10], Step [238/390], Loss: 2.4397\n",
      "Epoch [6/10], Step [239/390], Loss: 2.5092\n",
      "Epoch [6/10], Step [240/390], Loss: 2.1673\n",
      "Epoch [6/10], Step [241/390], Loss: 2.1380\n",
      "Epoch [6/10], Step [242/390], Loss: 2.3711\n",
      "Epoch [6/10], Step [243/390], Loss: 2.4199\n",
      "Epoch [6/10], Step [244/390], Loss: 2.5234\n",
      "Epoch [6/10], Step [245/390], Loss: 2.5584\n",
      "Epoch [6/10], Step [246/390], Loss: 2.1315\n",
      "Epoch [6/10], Step [247/390], Loss: 2.3092\n",
      "Epoch [6/10], Step [248/390], Loss: 2.2623\n",
      "Epoch [6/10], Step [249/390], Loss: 2.3773\n",
      "Epoch [6/10], Step [250/390], Loss: 2.0826\n",
      "Epoch [6/10], Step [251/390], Loss: 2.4213\n",
      "Epoch [6/10], Step [252/390], Loss: 2.5396\n",
      "Epoch [6/10], Step [253/390], Loss: 2.3303\n",
      "Epoch [6/10], Step [254/390], Loss: 2.6293\n",
      "Epoch [6/10], Step [255/390], Loss: 2.3046\n",
      "Epoch [6/10], Step [256/390], Loss: 2.3561\n",
      "Epoch [6/10], Step [257/390], Loss: 2.6691\n",
      "Epoch [6/10], Step [258/390], Loss: 2.4075\n",
      "Epoch [6/10], Step [259/390], Loss: 2.4120\n",
      "Epoch [6/10], Step [260/390], Loss: 2.1641\n",
      "Epoch [6/10], Step [261/390], Loss: 2.2624\n",
      "Epoch [6/10], Step [262/390], Loss: 2.4385\n",
      "Epoch [6/10], Step [263/390], Loss: 2.4320\n",
      "Epoch [6/10], Step [264/390], Loss: 2.3811\n",
      "Epoch [6/10], Step [265/390], Loss: 2.3381\n",
      "Epoch [6/10], Step [266/390], Loss: 2.1855\n",
      "Epoch [6/10], Step [267/390], Loss: 2.3224\n",
      "Epoch [6/10], Step [268/390], Loss: 2.4515\n",
      "Epoch [6/10], Step [269/390], Loss: 2.3417\n",
      "Epoch [6/10], Step [270/390], Loss: 2.4422\n",
      "Epoch [6/10], Step [271/390], Loss: 2.3308\n",
      "Epoch [6/10], Step [272/390], Loss: 2.3547\n",
      "Epoch [6/10], Step [273/390], Loss: 2.3291\n",
      "Epoch [6/10], Step [274/390], Loss: 2.3922\n",
      "Epoch [6/10], Step [275/390], Loss: 2.3224\n",
      "Epoch [6/10], Step [276/390], Loss: 2.3573\n",
      "Epoch [6/10], Step [277/390], Loss: 2.3137\n",
      "Epoch [6/10], Step [278/390], Loss: 2.6451\n",
      "Epoch [6/10], Step [279/390], Loss: 2.5820\n",
      "Epoch [6/10], Step [280/390], Loss: 2.4586\n",
      "Epoch [6/10], Step [281/390], Loss: 2.1705\n",
      "Epoch [6/10], Step [282/390], Loss: 2.2215\n",
      "Epoch [6/10], Step [283/390], Loss: 2.2178\n",
      "Epoch [6/10], Step [284/390], Loss: 2.2276\n",
      "Epoch [6/10], Step [285/390], Loss: 2.1700\n",
      "Epoch [6/10], Step [286/390], Loss: 2.3614\n",
      "Epoch [6/10], Step [287/390], Loss: 2.4506\n",
      "Epoch [6/10], Step [288/390], Loss: 2.3256\n",
      "Epoch [6/10], Step [289/390], Loss: 2.4246\n",
      "Epoch [6/10], Step [290/390], Loss: 2.3566\n",
      "Epoch [6/10], Step [291/390], Loss: 2.5491\n",
      "Epoch [6/10], Step [292/390], Loss: 2.5994\n",
      "Epoch [6/10], Step [293/390], Loss: 2.1280\n",
      "Epoch [6/10], Step [294/390], Loss: 2.4168\n",
      "Epoch [6/10], Step [295/390], Loss: 2.4401\n",
      "Epoch [6/10], Step [296/390], Loss: 2.4671\n",
      "Epoch [6/10], Step [297/390], Loss: 2.4207\n",
      "Epoch [6/10], Step [298/390], Loss: 2.0881\n",
      "Epoch [6/10], Step [299/390], Loss: 2.0425\n",
      "Epoch [6/10], Step [300/390], Loss: 2.1262\n",
      "Epoch [6/10], Step [301/390], Loss: 2.3016\n",
      "Epoch [6/10], Step [302/390], Loss: 2.2926\n",
      "Epoch [6/10], Step [303/390], Loss: 2.2400\n",
      "Epoch [6/10], Step [304/390], Loss: 2.4835\n",
      "Epoch [6/10], Step [305/390], Loss: 2.4610\n",
      "Epoch [6/10], Step [306/390], Loss: 2.2420\n",
      "Epoch [6/10], Step [307/390], Loss: 2.4705\n",
      "Epoch [6/10], Step [308/390], Loss: 2.1334\n",
      "Epoch [6/10], Step [309/390], Loss: 2.2220\n",
      "Epoch [6/10], Step [310/390], Loss: 2.2732\n",
      "Epoch [6/10], Step [311/390], Loss: 2.1949\n",
      "Epoch [6/10], Step [312/390], Loss: 2.3932\n",
      "Epoch [6/10], Step [313/390], Loss: 1.9983\n",
      "Epoch [6/10], Step [314/390], Loss: 2.3737\n",
      "Epoch [6/10], Step [315/390], Loss: 2.3896\n",
      "Epoch [6/10], Step [316/390], Loss: 2.1619\n",
      "Epoch [6/10], Step [317/390], Loss: 2.1717\n",
      "Epoch [6/10], Step [318/390], Loss: 2.3269\n",
      "Epoch [6/10], Step [319/390], Loss: 2.1506\n",
      "Epoch [6/10], Step [320/390], Loss: 2.2471\n",
      "Epoch [6/10], Step [321/390], Loss: 2.1836\n",
      "Epoch [6/10], Step [322/390], Loss: 1.9235\n",
      "Epoch [6/10], Step [323/390], Loss: 2.1750\n",
      "Epoch [6/10], Step [324/390], Loss: 1.9772\n",
      "Epoch [6/10], Step [325/390], Loss: 2.2219\n",
      "Epoch [6/10], Step [326/390], Loss: 2.1511\n",
      "Epoch [6/10], Step [327/390], Loss: 2.2530\n",
      "Epoch [6/10], Step [328/390], Loss: 2.4906\n",
      "Epoch [6/10], Step [329/390], Loss: 2.3985\n",
      "Epoch [6/10], Step [330/390], Loss: 2.1936\n",
      "Epoch [6/10], Step [331/390], Loss: 2.1953\n",
      "Epoch [6/10], Step [332/390], Loss: 2.0322\n",
      "Epoch [6/10], Step [333/390], Loss: 2.3606\n",
      "Epoch [6/10], Step [334/390], Loss: 2.3875\n",
      "Epoch [6/10], Step [335/390], Loss: 2.3776\n",
      "Epoch [6/10], Step [336/390], Loss: 2.4059\n",
      "Epoch [6/10], Step [337/390], Loss: 2.4804\n",
      "Epoch [6/10], Step [338/390], Loss: 2.3321\n",
      "Epoch [6/10], Step [339/390], Loss: 2.2457\n",
      "Epoch [6/10], Step [340/390], Loss: 2.4637\n",
      "Epoch [6/10], Step [341/390], Loss: 2.2286\n",
      "Epoch [6/10], Step [342/390], Loss: 2.2870\n",
      "Epoch [6/10], Step [343/390], Loss: 2.2709\n",
      "Epoch [6/10], Step [344/390], Loss: 2.3937\n",
      "Epoch [6/10], Step [345/390], Loss: 2.2508\n",
      "Epoch [6/10], Step [346/390], Loss: 2.4237\n",
      "Epoch [6/10], Step [347/390], Loss: 2.4393\n",
      "Epoch [6/10], Step [348/390], Loss: 2.1808\n",
      "Epoch [6/10], Step [349/390], Loss: 2.3633\n",
      "Epoch [6/10], Step [350/390], Loss: 2.1328\n",
      "Epoch [6/10], Step [351/390], Loss: 2.5726\n",
      "Epoch [6/10], Step [352/390], Loss: 2.1816\n",
      "Epoch [6/10], Step [353/390], Loss: 2.3897\n",
      "Epoch [6/10], Step [354/390], Loss: 2.1218\n",
      "Epoch [6/10], Step [355/390], Loss: 2.3309\n",
      "Epoch [6/10], Step [356/390], Loss: 2.2711\n",
      "Epoch [6/10], Step [357/390], Loss: 2.1648\n",
      "Epoch [6/10], Step [358/390], Loss: 1.9984\n",
      "Epoch [6/10], Step [359/390], Loss: 2.2188\n",
      "Epoch [6/10], Step [360/390], Loss: 2.4267\n",
      "Epoch [6/10], Step [361/390], Loss: 2.3045\n",
      "Epoch [6/10], Step [362/390], Loss: 2.1837\n",
      "Epoch [6/10], Step [363/390], Loss: 2.4022\n",
      "Epoch [6/10], Step [364/390], Loss: 2.2560\n",
      "Epoch [6/10], Step [365/390], Loss: 2.2697\n",
      "Epoch [6/10], Step [366/390], Loss: 2.6916\n",
      "Epoch [6/10], Step [367/390], Loss: 2.3572\n",
      "Epoch [6/10], Step [368/390], Loss: 2.3635\n",
      "Epoch [6/10], Step [369/390], Loss: 2.3812\n",
      "Epoch [6/10], Step [370/390], Loss: 2.2203\n",
      "Epoch [6/10], Step [371/390], Loss: 2.1405\n",
      "Epoch [6/10], Step [372/390], Loss: 2.4892\n",
      "Epoch [6/10], Step [373/390], Loss: 2.1967\n",
      "Epoch [6/10], Step [374/390], Loss: 2.2280\n",
      "Epoch [6/10], Step [375/390], Loss: 2.3930\n",
      "Epoch [6/10], Step [376/390], Loss: 2.3229\n",
      "Epoch [6/10], Step [377/390], Loss: 2.1893\n",
      "Epoch [6/10], Step [378/390], Loss: 2.2639\n",
      "Epoch [6/10], Step [379/390], Loss: 2.1492\n",
      "Epoch [6/10], Step [380/390], Loss: 2.2885\n",
      "Epoch [6/10], Step [381/390], Loss: 2.2555\n",
      "Epoch [6/10], Step [382/390], Loss: 2.6183\n",
      "Epoch [6/10], Step [383/390], Loss: 2.5170\n",
      "Epoch [6/10], Step [384/390], Loss: 2.1061\n",
      "Epoch [6/10], Step [385/390], Loss: 2.6698\n",
      "Epoch [6/10], Step [386/390], Loss: 2.3659\n",
      "Epoch [6/10], Step [387/390], Loss: 2.3023\n",
      "Epoch [6/10], Step [388/390], Loss: 2.2017\n",
      "Epoch [6/10], Step [389/390], Loss: 2.1951\n",
      "Epoch [6/10], Step [390/390], Loss: 2.2811\n",
      "Epoch [6/10], Step [391/390], Loss: 2.9313\n",
      "Epoch [7/10], Step [1/390], Loss: 1.9795\n",
      "Epoch [7/10], Step [2/390], Loss: 2.4456\n",
      "Epoch [7/10], Step [3/390], Loss: 2.0235\n",
      "Epoch [7/10], Step [4/390], Loss: 1.8919\n",
      "Epoch [7/10], Step [5/390], Loss: 2.2111\n",
      "Epoch [7/10], Step [6/390], Loss: 1.9802\n",
      "Epoch [7/10], Step [7/390], Loss: 2.0082\n",
      "Epoch [7/10], Step [8/390], Loss: 1.7224\n",
      "Epoch [7/10], Step [9/390], Loss: 2.2750\n",
      "Epoch [7/10], Step [10/390], Loss: 2.1059\n",
      "Epoch [7/10], Step [11/390], Loss: 2.0887\n",
      "Epoch [7/10], Step [12/390], Loss: 1.9111\n",
      "Epoch [7/10], Step [13/390], Loss: 2.1756\n",
      "Epoch [7/10], Step [14/390], Loss: 2.0395\n",
      "Epoch [7/10], Step [15/390], Loss: 1.9885\n",
      "Epoch [7/10], Step [16/390], Loss: 2.2084\n",
      "Epoch [7/10], Step [17/390], Loss: 1.8795\n",
      "Epoch [7/10], Step [18/390], Loss: 2.6580\n",
      "Epoch [7/10], Step [19/390], Loss: 1.9553\n",
      "Epoch [7/10], Step [20/390], Loss: 2.0217\n",
      "Epoch [7/10], Step [21/390], Loss: 2.1072\n",
      "Epoch [7/10], Step [22/390], Loss: 2.0256\n",
      "Epoch [7/10], Step [23/390], Loss: 2.0902\n",
      "Epoch [7/10], Step [24/390], Loss: 2.0051\n",
      "Epoch [7/10], Step [25/390], Loss: 2.1565\n",
      "Epoch [7/10], Step [26/390], Loss: 1.8593\n",
      "Epoch [7/10], Step [27/390], Loss: 2.1178\n",
      "Epoch [7/10], Step [28/390], Loss: 2.1449\n",
      "Epoch [7/10], Step [29/390], Loss: 2.3234\n",
      "Epoch [7/10], Step [30/390], Loss: 1.9934\n",
      "Epoch [7/10], Step [31/390], Loss: 2.2143\n",
      "Epoch [7/10], Step [32/390], Loss: 2.3125\n",
      "Epoch [7/10], Step [33/390], Loss: 2.1365\n",
      "Epoch [7/10], Step [34/390], Loss: 2.2690\n",
      "Epoch [7/10], Step [35/390], Loss: 2.0383\n",
      "Epoch [7/10], Step [36/390], Loss: 2.1120\n",
      "Epoch [7/10], Step [37/390], Loss: 2.2071\n",
      "Epoch [7/10], Step [38/390], Loss: 2.0580\n",
      "Epoch [7/10], Step [39/390], Loss: 2.0937\n",
      "Epoch [7/10], Step [40/390], Loss: 1.6903\n",
      "Epoch [7/10], Step [41/390], Loss: 2.3223\n",
      "Epoch [7/10], Step [42/390], Loss: 1.9867\n",
      "Epoch [7/10], Step [43/390], Loss: 2.0470\n",
      "Epoch [7/10], Step [44/390], Loss: 2.3544\n",
      "Epoch [7/10], Step [45/390], Loss: 2.2660\n",
      "Epoch [7/10], Step [46/390], Loss: 2.3511\n",
      "Epoch [7/10], Step [47/390], Loss: 2.0751\n",
      "Epoch [7/10], Step [48/390], Loss: 2.1698\n",
      "Epoch [7/10], Step [49/390], Loss: 2.4538\n",
      "Epoch [7/10], Step [50/390], Loss: 2.0438\n",
      "Epoch [7/10], Step [51/390], Loss: 1.9838\n",
      "Epoch [7/10], Step [52/390], Loss: 2.1480\n",
      "Epoch [7/10], Step [53/390], Loss: 2.2863\n",
      "Epoch [7/10], Step [54/390], Loss: 1.9626\n",
      "Epoch [7/10], Step [55/390], Loss: 2.2704\n",
      "Epoch [7/10], Step [56/390], Loss: 2.0742\n",
      "Epoch [7/10], Step [57/390], Loss: 1.9801\n",
      "Epoch [7/10], Step [58/390], Loss: 2.1740\n",
      "Epoch [7/10], Step [59/390], Loss: 2.1069\n",
      "Epoch [7/10], Step [60/390], Loss: 2.1814\n",
      "Epoch [7/10], Step [61/390], Loss: 2.0652\n",
      "Epoch [7/10], Step [62/390], Loss: 2.0825\n",
      "Epoch [7/10], Step [63/390], Loss: 2.1383\n",
      "Epoch [7/10], Step [64/390], Loss: 2.0906\n",
      "Epoch [7/10], Step [65/390], Loss: 1.8501\n",
      "Epoch [7/10], Step [66/390], Loss: 2.0032\n",
      "Epoch [7/10], Step [67/390], Loss: 2.1171\n",
      "Epoch [7/10], Step [68/390], Loss: 2.1208\n",
      "Epoch [7/10], Step [69/390], Loss: 1.8626\n",
      "Epoch [7/10], Step [70/390], Loss: 2.0164\n",
      "Epoch [7/10], Step [71/390], Loss: 2.1919\n",
      "Epoch [7/10], Step [72/390], Loss: 2.1465\n",
      "Epoch [7/10], Step [73/390], Loss: 2.3409\n",
      "Epoch [7/10], Step [74/390], Loss: 2.3179\n",
      "Epoch [7/10], Step [75/390], Loss: 1.9467\n",
      "Epoch [7/10], Step [76/390], Loss: 2.2642\n",
      "Epoch [7/10], Step [77/390], Loss: 2.0943\n",
      "Epoch [7/10], Step [78/390], Loss: 1.8405\n",
      "Epoch [7/10], Step [79/390], Loss: 1.8241\n",
      "Epoch [7/10], Step [80/390], Loss: 2.1681\n",
      "Epoch [7/10], Step [81/390], Loss: 2.3581\n",
      "Epoch [7/10], Step [82/390], Loss: 2.2160\n",
      "Epoch [7/10], Step [83/390], Loss: 2.0997\n",
      "Epoch [7/10], Step [84/390], Loss: 1.7888\n",
      "Epoch [7/10], Step [85/390], Loss: 2.2293\n",
      "Epoch [7/10], Step [86/390], Loss: 2.4172\n",
      "Epoch [7/10], Step [87/390], Loss: 2.0995\n",
      "Epoch [7/10], Step [88/390], Loss: 2.3091\n",
      "Epoch [7/10], Step [89/390], Loss: 2.3593\n",
      "Epoch [7/10], Step [90/390], Loss: 2.2317\n",
      "Epoch [7/10], Step [91/390], Loss: 2.3209\n",
      "Epoch [7/10], Step [92/390], Loss: 1.9460\n",
      "Epoch [7/10], Step [93/390], Loss: 2.3736\n",
      "Epoch [7/10], Step [94/390], Loss: 2.2881\n",
      "Epoch [7/10], Step [95/390], Loss: 1.9591\n",
      "Epoch [7/10], Step [96/390], Loss: 2.3236\n",
      "Epoch [7/10], Step [97/390], Loss: 1.9805\n",
      "Epoch [7/10], Step [98/390], Loss: 2.1179\n",
      "Epoch [7/10], Step [99/390], Loss: 2.0643\n",
      "Epoch [7/10], Step [100/390], Loss: 2.2787\n",
      "Epoch [7/10], Step [101/390], Loss: 2.0184\n",
      "Epoch [7/10], Step [102/390], Loss: 2.0961\n",
      "Epoch [7/10], Step [103/390], Loss: 1.9719\n",
      "Epoch [7/10], Step [104/390], Loss: 2.1912\n",
      "Epoch [7/10], Step [105/390], Loss: 2.1291\n",
      "Epoch [7/10], Step [106/390], Loss: 2.5940\n",
      "Epoch [7/10], Step [107/390], Loss: 2.0581\n",
      "Epoch [7/10], Step [108/390], Loss: 2.1795\n",
      "Epoch [7/10], Step [109/390], Loss: 1.8413\n",
      "Epoch [7/10], Step [110/390], Loss: 1.9748\n",
      "Epoch [7/10], Step [111/390], Loss: 2.1538\n",
      "Epoch [7/10], Step [112/390], Loss: 2.3096\n",
      "Epoch [7/10], Step [113/390], Loss: 2.3002\n",
      "Epoch [7/10], Step [114/390], Loss: 2.2202\n",
      "Epoch [7/10], Step [115/390], Loss: 2.2222\n",
      "Epoch [7/10], Step [116/390], Loss: 2.0519\n",
      "Epoch [7/10], Step [117/390], Loss: 2.1846\n",
      "Epoch [7/10], Step [118/390], Loss: 2.0326\n",
      "Epoch [7/10], Step [119/390], Loss: 2.3218\n",
      "Epoch [7/10], Step [120/390], Loss: 2.1496\n",
      "Epoch [7/10], Step [121/390], Loss: 2.1840\n",
      "Epoch [7/10], Step [122/390], Loss: 2.2947\n",
      "Epoch [7/10], Step [123/390], Loss: 2.1905\n",
      "Epoch [7/10], Step [124/390], Loss: 2.0989\n",
      "Epoch [7/10], Step [125/390], Loss: 2.0994\n",
      "Epoch [7/10], Step [126/390], Loss: 1.8000\n",
      "Epoch [7/10], Step [127/390], Loss: 2.3357\n",
      "Epoch [7/10], Step [128/390], Loss: 2.2140\n",
      "Epoch [7/10], Step [129/390], Loss: 2.3133\n",
      "Epoch [7/10], Step [130/390], Loss: 2.1725\n",
      "Epoch [7/10], Step [131/390], Loss: 2.2619\n",
      "Epoch [7/10], Step [132/390], Loss: 2.2915\n",
      "Epoch [7/10], Step [133/390], Loss: 2.2609\n",
      "Epoch [7/10], Step [134/390], Loss: 2.1162\n",
      "Epoch [7/10], Step [135/390], Loss: 2.2246\n",
      "Epoch [7/10], Step [136/390], Loss: 2.0985\n",
      "Epoch [7/10], Step [137/390], Loss: 2.1488\n",
      "Epoch [7/10], Step [138/390], Loss: 2.3338\n",
      "Epoch [7/10], Step [139/390], Loss: 2.1255\n",
      "Epoch [7/10], Step [140/390], Loss: 2.2373\n",
      "Epoch [7/10], Step [141/390], Loss: 2.3494\n",
      "Epoch [7/10], Step [142/390], Loss: 2.0262\n",
      "Epoch [7/10], Step [143/390], Loss: 2.0629\n",
      "Epoch [7/10], Step [144/390], Loss: 1.9646\n",
      "Epoch [7/10], Step [145/390], Loss: 2.2709\n",
      "Epoch [7/10], Step [146/390], Loss: 2.2989\n",
      "Epoch [7/10], Step [147/390], Loss: 2.2253\n",
      "Epoch [7/10], Step [148/390], Loss: 2.3130\n",
      "Epoch [7/10], Step [149/390], Loss: 2.4451\n",
      "Epoch [7/10], Step [150/390], Loss: 2.0317\n",
      "Epoch [7/10], Step [151/390], Loss: 2.1690\n",
      "Epoch [7/10], Step [152/390], Loss: 1.9933\n",
      "Epoch [7/10], Step [153/390], Loss: 2.0331\n",
      "Epoch [7/10], Step [154/390], Loss: 1.9930\n",
      "Epoch [7/10], Step [155/390], Loss: 1.6360\n",
      "Epoch [7/10], Step [156/390], Loss: 2.1730\n",
      "Epoch [7/10], Step [157/390], Loss: 2.3581\n",
      "Epoch [7/10], Step [158/390], Loss: 2.1327\n",
      "Epoch [7/10], Step [159/390], Loss: 2.1173\n",
      "Epoch [7/10], Step [160/390], Loss: 2.4301\n",
      "Epoch [7/10], Step [161/390], Loss: 2.0499\n",
      "Epoch [7/10], Step [162/390], Loss: 2.1396\n",
      "Epoch [7/10], Step [163/390], Loss: 2.1057\n",
      "Epoch [7/10], Step [164/390], Loss: 1.9489\n",
      "Epoch [7/10], Step [165/390], Loss: 2.0807\n",
      "Epoch [7/10], Step [166/390], Loss: 2.0605\n",
      "Epoch [7/10], Step [167/390], Loss: 2.2395\n",
      "Epoch [7/10], Step [168/390], Loss: 1.9562\n",
      "Epoch [7/10], Step [169/390], Loss: 2.1298\n",
      "Epoch [7/10], Step [170/390], Loss: 2.2655\n",
      "Epoch [7/10], Step [171/390], Loss: 2.2958\n",
      "Epoch [7/10], Step [172/390], Loss: 2.2868\n",
      "Epoch [7/10], Step [173/390], Loss: 2.2800\n",
      "Epoch [7/10], Step [174/390], Loss: 2.1787\n",
      "Epoch [7/10], Step [175/390], Loss: 2.1544\n",
      "Epoch [7/10], Step [176/390], Loss: 2.0450\n",
      "Epoch [7/10], Step [177/390], Loss: 2.4418\n",
      "Epoch [7/10], Step [178/390], Loss: 1.9434\n",
      "Epoch [7/10], Step [179/390], Loss: 1.8800\n",
      "Epoch [7/10], Step [180/390], Loss: 2.2903\n",
      "Epoch [7/10], Step [181/390], Loss: 2.1349\n",
      "Epoch [7/10], Step [182/390], Loss: 2.0349\n",
      "Epoch [7/10], Step [183/390], Loss: 2.0311\n",
      "Epoch [7/10], Step [184/390], Loss: 2.1802\n",
      "Epoch [7/10], Step [185/390], Loss: 2.4513\n",
      "Epoch [7/10], Step [186/390], Loss: 2.0396\n",
      "Epoch [7/10], Step [187/390], Loss: 2.0521\n",
      "Epoch [7/10], Step [188/390], Loss: 2.5245\n",
      "Epoch [7/10], Step [189/390], Loss: 2.2384\n",
      "Epoch [7/10], Step [190/390], Loss: 2.2822\n",
      "Epoch [7/10], Step [191/390], Loss: 2.4135\n",
      "Epoch [7/10], Step [192/390], Loss: 2.1904\n",
      "Epoch [7/10], Step [193/390], Loss: 2.0830\n",
      "Epoch [7/10], Step [194/390], Loss: 2.1625\n",
      "Epoch [7/10], Step [195/390], Loss: 2.2717\n",
      "Epoch [7/10], Step [196/390], Loss: 2.1855\n",
      "Epoch [7/10], Step [197/390], Loss: 2.2890\n",
      "Epoch [7/10], Step [198/390], Loss: 2.0783\n",
      "Epoch [7/10], Step [199/390], Loss: 1.9651\n",
      "Epoch [7/10], Step [200/390], Loss: 2.3094\n",
      "Epoch [7/10], Step [201/390], Loss: 2.2443\n",
      "Epoch [7/10], Step [202/390], Loss: 2.4001\n",
      "Epoch [7/10], Step [203/390], Loss: 2.1013\n",
      "Epoch [7/10], Step [204/390], Loss: 1.9942\n",
      "Epoch [7/10], Step [205/390], Loss: 1.8477\n",
      "Epoch [7/10], Step [206/390], Loss: 2.0639\n",
      "Epoch [7/10], Step [207/390], Loss: 2.0833\n",
      "Epoch [7/10], Step [208/390], Loss: 2.0896\n",
      "Epoch [7/10], Step [209/390], Loss: 2.1615\n",
      "Epoch [7/10], Step [210/390], Loss: 2.1326\n",
      "Epoch [7/10], Step [211/390], Loss: 2.2068\n",
      "Epoch [7/10], Step [212/390], Loss: 2.1658\n",
      "Epoch [7/10], Step [213/390], Loss: 2.4448\n",
      "Epoch [7/10], Step [214/390], Loss: 1.9330\n",
      "Epoch [7/10], Step [215/390], Loss: 2.0629\n",
      "Epoch [7/10], Step [216/390], Loss: 2.2955\n",
      "Epoch [7/10], Step [217/390], Loss: 2.0881\n",
      "Epoch [7/10], Step [218/390], Loss: 2.4436\n",
      "Epoch [7/10], Step [219/390], Loss: 2.3704\n",
      "Epoch [7/10], Step [220/390], Loss: 1.9363\n",
      "Epoch [7/10], Step [221/390], Loss: 2.2376\n",
      "Epoch [7/10], Step [222/390], Loss: 2.2834\n",
      "Epoch [7/10], Step [223/390], Loss: 2.0160\n",
      "Epoch [7/10], Step [224/390], Loss: 2.1495\n",
      "Epoch [7/10], Step [225/390], Loss: 1.9073\n",
      "Epoch [7/10], Step [226/390], Loss: 2.2575\n",
      "Epoch [7/10], Step [227/390], Loss: 2.0800\n",
      "Epoch [7/10], Step [228/390], Loss: 2.0856\n",
      "Epoch [7/10], Step [229/390], Loss: 2.1572\n",
      "Epoch [7/10], Step [230/390], Loss: 2.2762\n",
      "Epoch [7/10], Step [231/390], Loss: 2.5709\n",
      "Epoch [7/10], Step [232/390], Loss: 1.8255\n",
      "Epoch [7/10], Step [233/390], Loss: 1.9983\n",
      "Epoch [7/10], Step [234/390], Loss: 2.1423\n",
      "Epoch [7/10], Step [235/390], Loss: 2.3547\n",
      "Epoch [7/10], Step [236/390], Loss: 2.1586\n",
      "Epoch [7/10], Step [237/390], Loss: 2.3083\n",
      "Epoch [7/10], Step [238/390], Loss: 2.4923\n",
      "Epoch [7/10], Step [239/390], Loss: 2.0774\n",
      "Epoch [7/10], Step [240/390], Loss: 2.3340\n",
      "Epoch [7/10], Step [241/390], Loss: 2.1577\n",
      "Epoch [7/10], Step [242/390], Loss: 2.2532\n",
      "Epoch [7/10], Step [243/390], Loss: 2.3466\n",
      "Epoch [7/10], Step [244/390], Loss: 2.1594\n",
      "Epoch [7/10], Step [245/390], Loss: 2.0286\n",
      "Epoch [7/10], Step [246/390], Loss: 1.8751\n",
      "Epoch [7/10], Step [247/390], Loss: 2.2956\n",
      "Epoch [7/10], Step [248/390], Loss: 2.2132\n",
      "Epoch [7/10], Step [249/390], Loss: 2.2065\n",
      "Epoch [7/10], Step [250/390], Loss: 2.2328\n",
      "Epoch [7/10], Step [251/390], Loss: 2.0137\n",
      "Epoch [7/10], Step [252/390], Loss: 2.0533\n",
      "Epoch [7/10], Step [253/390], Loss: 2.2069\n",
      "Epoch [7/10], Step [254/390], Loss: 2.5967\n",
      "Epoch [7/10], Step [255/390], Loss: 2.2696\n",
      "Epoch [7/10], Step [256/390], Loss: 2.1895\n",
      "Epoch [7/10], Step [257/390], Loss: 1.9060\n",
      "Epoch [7/10], Step [258/390], Loss: 2.1519\n",
      "Epoch [7/10], Step [259/390], Loss: 2.0193\n",
      "Epoch [7/10], Step [260/390], Loss: 2.3068\n",
      "Epoch [7/10], Step [261/390], Loss: 2.2438\n",
      "Epoch [7/10], Step [262/390], Loss: 1.9911\n",
      "Epoch [7/10], Step [263/390], Loss: 2.3052\n",
      "Epoch [7/10], Step [264/390], Loss: 2.2409\n",
      "Epoch [7/10], Step [265/390], Loss: 2.1299\n",
      "Epoch [7/10], Step [266/390], Loss: 2.2383\n",
      "Epoch [7/10], Step [267/390], Loss: 2.1432\n",
      "Epoch [7/10], Step [268/390], Loss: 2.1895\n",
      "Epoch [7/10], Step [269/390], Loss: 2.1611\n",
      "Epoch [7/10], Step [270/390], Loss: 2.2366\n",
      "Epoch [7/10], Step [271/390], Loss: 2.0532\n",
      "Epoch [7/10], Step [272/390], Loss: 2.2531\n",
      "Epoch [7/10], Step [273/390], Loss: 1.9732\n",
      "Epoch [7/10], Step [274/390], Loss: 2.0234\n",
      "Epoch [7/10], Step [275/390], Loss: 2.0182\n",
      "Epoch [7/10], Step [276/390], Loss: 2.0404\n",
      "Epoch [7/10], Step [277/390], Loss: 2.0027\n",
      "Epoch [7/10], Step [278/390], Loss: 2.1101\n",
      "Epoch [7/10], Step [279/390], Loss: 2.1020\n",
      "Epoch [7/10], Step [280/390], Loss: 2.3469\n",
      "Epoch [7/10], Step [281/390], Loss: 2.1715\n",
      "Epoch [7/10], Step [282/390], Loss: 1.9637\n",
      "Epoch [7/10], Step [283/390], Loss: 2.3687\n",
      "Epoch [7/10], Step [284/390], Loss: 2.3169\n",
      "Epoch [7/10], Step [285/390], Loss: 2.0147\n",
      "Epoch [7/10], Step [286/390], Loss: 2.2594\n",
      "Epoch [7/10], Step [287/390], Loss: 1.8846\n",
      "Epoch [7/10], Step [288/390], Loss: 2.3076\n",
      "Epoch [7/10], Step [289/390], Loss: 2.2853\n",
      "Epoch [7/10], Step [290/390], Loss: 2.2686\n",
      "Epoch [7/10], Step [291/390], Loss: 2.5112\n",
      "Epoch [7/10], Step [292/390], Loss: 2.0500\n",
      "Epoch [7/10], Step [293/390], Loss: 1.9399\n",
      "Epoch [7/10], Step [294/390], Loss: 2.2493\n",
      "Epoch [7/10], Step [295/390], Loss: 2.1047\n",
      "Epoch [7/10], Step [296/390], Loss: 2.0474\n",
      "Epoch [7/10], Step [297/390], Loss: 2.1146\n",
      "Epoch [7/10], Step [298/390], Loss: 1.9612\n",
      "Epoch [7/10], Step [299/390], Loss: 2.2586\n",
      "Epoch [7/10], Step [300/390], Loss: 2.2495\n",
      "Epoch [7/10], Step [301/390], Loss: 2.3831\n",
      "Epoch [7/10], Step [302/390], Loss: 2.3600\n",
      "Epoch [7/10], Step [303/390], Loss: 2.1765\n",
      "Epoch [7/10], Step [304/390], Loss: 2.0743\n",
      "Epoch [7/10], Step [305/390], Loss: 2.1685\n",
      "Epoch [7/10], Step [306/390], Loss: 2.1271\n",
      "Epoch [7/10], Step [307/390], Loss: 2.2006\n",
      "Epoch [7/10], Step [308/390], Loss: 2.0938\n",
      "Epoch [7/10], Step [309/390], Loss: 2.2572\n",
      "Epoch [7/10], Step [310/390], Loss: 2.2075\n",
      "Epoch [7/10], Step [311/390], Loss: 1.9565\n",
      "Epoch [7/10], Step [312/390], Loss: 1.8583\n",
      "Epoch [7/10], Step [313/390], Loss: 2.4473\n",
      "Epoch [7/10], Step [314/390], Loss: 2.4375\n",
      "Epoch [7/10], Step [315/390], Loss: 2.0544\n",
      "Epoch [7/10], Step [316/390], Loss: 2.2402\n",
      "Epoch [7/10], Step [317/390], Loss: 1.9879\n",
      "Epoch [7/10], Step [318/390], Loss: 1.9732\n",
      "Epoch [7/10], Step [319/390], Loss: 2.1552\n",
      "Epoch [7/10], Step [320/390], Loss: 1.9985\n",
      "Epoch [7/10], Step [321/390], Loss: 1.9135\n",
      "Epoch [7/10], Step [322/390], Loss: 2.1715\n",
      "Epoch [7/10], Step [323/390], Loss: 2.3037\n",
      "Epoch [7/10], Step [324/390], Loss: 2.1628\n",
      "Epoch [7/10], Step [325/390], Loss: 2.2021\n",
      "Epoch [7/10], Step [326/390], Loss: 2.0984\n",
      "Epoch [7/10], Step [327/390], Loss: 2.4061\n",
      "Epoch [7/10], Step [328/390], Loss: 2.1442\n",
      "Epoch [7/10], Step [329/390], Loss: 2.1501\n",
      "Epoch [7/10], Step [330/390], Loss: 2.3788\n",
      "Epoch [7/10], Step [331/390], Loss: 2.1170\n",
      "Epoch [7/10], Step [332/390], Loss: 2.1085\n",
      "Epoch [7/10], Step [333/390], Loss: 2.1623\n",
      "Epoch [7/10], Step [334/390], Loss: 1.9961\n",
      "Epoch [7/10], Step [335/390], Loss: 2.2650\n",
      "Epoch [7/10], Step [336/390], Loss: 2.1383\n",
      "Epoch [7/10], Step [337/390], Loss: 2.0745\n",
      "Epoch [7/10], Step [338/390], Loss: 2.2474\n",
      "Epoch [7/10], Step [339/390], Loss: 1.9253\n",
      "Epoch [7/10], Step [340/390], Loss: 2.0650\n",
      "Epoch [7/10], Step [341/390], Loss: 2.3222\n",
      "Epoch [7/10], Step [342/390], Loss: 2.0070\n",
      "Epoch [7/10], Step [343/390], Loss: 1.9563\n",
      "Epoch [7/10], Step [344/390], Loss: 2.3832\n",
      "Epoch [7/10], Step [345/390], Loss: 2.1711\n",
      "Epoch [7/10], Step [346/390], Loss: 2.3195\n",
      "Epoch [7/10], Step [347/390], Loss: 2.1397\n",
      "Epoch [7/10], Step [348/390], Loss: 2.0878\n",
      "Epoch [7/10], Step [349/390], Loss: 2.3143\n",
      "Epoch [7/10], Step [350/390], Loss: 2.2368\n",
      "Epoch [7/10], Step [351/390], Loss: 2.3270\n",
      "Epoch [7/10], Step [352/390], Loss: 1.8385\n",
      "Epoch [7/10], Step [353/390], Loss: 1.9705\n",
      "Epoch [7/10], Step [354/390], Loss: 1.7534\n",
      "Epoch [7/10], Step [355/390], Loss: 2.2052\n",
      "Epoch [7/10], Step [356/390], Loss: 2.1391\n",
      "Epoch [7/10], Step [357/390], Loss: 2.1812\n",
      "Epoch [7/10], Step [358/390], Loss: 2.1095\n",
      "Epoch [7/10], Step [359/390], Loss: 2.4565\n",
      "Epoch [7/10], Step [360/390], Loss: 2.3909\n",
      "Epoch [7/10], Step [361/390], Loss: 2.2392\n",
      "Epoch [7/10], Step [362/390], Loss: 2.1067\n",
      "Epoch [7/10], Step [363/390], Loss: 2.0675\n",
      "Epoch [7/10], Step [364/390], Loss: 2.1378\n",
      "Epoch [7/10], Step [365/390], Loss: 2.2606\n",
      "Epoch [7/10], Step [366/390], Loss: 2.0903\n",
      "Epoch [7/10], Step [367/390], Loss: 2.3813\n",
      "Epoch [7/10], Step [368/390], Loss: 2.0922\n",
      "Epoch [7/10], Step [369/390], Loss: 1.8729\n",
      "Epoch [7/10], Step [370/390], Loss: 1.9947\n",
      "Epoch [7/10], Step [371/390], Loss: 2.3429\n",
      "Epoch [7/10], Step [372/390], Loss: 2.2782\n",
      "Epoch [7/10], Step [373/390], Loss: 2.1664\n",
      "Epoch [7/10], Step [374/390], Loss: 2.2189\n",
      "Epoch [7/10], Step [375/390], Loss: 2.0062\n",
      "Epoch [7/10], Step [376/390], Loss: 2.0822\n",
      "Epoch [7/10], Step [377/390], Loss: 2.1565\n",
      "Epoch [7/10], Step [378/390], Loss: 2.2078\n",
      "Epoch [7/10], Step [379/390], Loss: 2.2879\n",
      "Epoch [7/10], Step [380/390], Loss: 1.8937\n",
      "Epoch [7/10], Step [381/390], Loss: 2.1689\n",
      "Epoch [7/10], Step [382/390], Loss: 2.0730\n",
      "Epoch [7/10], Step [383/390], Loss: 2.1734\n",
      "Epoch [7/10], Step [384/390], Loss: 2.1978\n",
      "Epoch [7/10], Step [385/390], Loss: 1.7355\n",
      "Epoch [7/10], Step [386/390], Loss: 1.9099\n",
      "Epoch [7/10], Step [387/390], Loss: 2.3128\n",
      "Epoch [7/10], Step [388/390], Loss: 1.9419\n",
      "Epoch [7/10], Step [389/390], Loss: 2.2591\n",
      "Epoch [7/10], Step [390/390], Loss: 1.9208\n",
      "Epoch [7/10], Step [391/390], Loss: 2.0951\n",
      "Epoch [8/10], Step [1/390], Loss: 1.8601\n",
      "Epoch [8/10], Step [2/390], Loss: 1.8020\n",
      "Epoch [8/10], Step [3/390], Loss: 2.2598\n",
      "Epoch [8/10], Step [4/390], Loss: 1.6973\n",
      "Epoch [8/10], Step [5/390], Loss: 1.8639\n",
      "Epoch [8/10], Step [6/390], Loss: 1.8199\n",
      "Epoch [8/10], Step [7/390], Loss: 1.9564\n",
      "Epoch [8/10], Step [8/390], Loss: 1.8170\n",
      "Epoch [8/10], Step [9/390], Loss: 2.0871\n",
      "Epoch [8/10], Step [10/390], Loss: 1.9748\n",
      "Epoch [8/10], Step [11/390], Loss: 1.9553\n",
      "Epoch [8/10], Step [12/390], Loss: 1.7429\n",
      "Epoch [8/10], Step [13/390], Loss: 2.2171\n",
      "Epoch [8/10], Step [14/390], Loss: 2.2375\n",
      "Epoch [8/10], Step [15/390], Loss: 1.9115\n",
      "Epoch [8/10], Step [16/390], Loss: 2.0008\n",
      "Epoch [8/10], Step [17/390], Loss: 1.7904\n",
      "Epoch [8/10], Step [18/390], Loss: 1.9762\n",
      "Epoch [8/10], Step [19/390], Loss: 1.6450\n",
      "Epoch [8/10], Step [20/390], Loss: 1.9789\n",
      "Epoch [8/10], Step [21/390], Loss: 2.1949\n",
      "Epoch [8/10], Step [22/390], Loss: 2.3230\n",
      "Epoch [8/10], Step [23/390], Loss: 2.0199\n",
      "Epoch [8/10], Step [24/390], Loss: 1.7495\n",
      "Epoch [8/10], Step [25/390], Loss: 1.9016\n",
      "Epoch [8/10], Step [26/390], Loss: 1.9836\n",
      "Epoch [8/10], Step [27/390], Loss: 2.0670\n",
      "Epoch [8/10], Step [28/390], Loss: 2.0739\n",
      "Epoch [8/10], Step [29/390], Loss: 2.1358\n",
      "Epoch [8/10], Step [30/390], Loss: 1.9276\n",
      "Epoch [8/10], Step [31/390], Loss: 1.9030\n",
      "Epoch [8/10], Step [32/390], Loss: 1.9665\n",
      "Epoch [8/10], Step [33/390], Loss: 2.1699\n",
      "Epoch [8/10], Step [34/390], Loss: 1.8539\n",
      "Epoch [8/10], Step [35/390], Loss: 1.8613\n",
      "Epoch [8/10], Step [36/390], Loss: 1.8763\n",
      "Epoch [8/10], Step [37/390], Loss: 1.8399\n",
      "Epoch [8/10], Step [38/390], Loss: 1.8180\n",
      "Epoch [8/10], Step [39/390], Loss: 1.5951\n",
      "Epoch [8/10], Step [40/390], Loss: 2.1088\n",
      "Epoch [8/10], Step [41/390], Loss: 1.9832\n",
      "Epoch [8/10], Step [42/390], Loss: 1.9580\n",
      "Epoch [8/10], Step [43/390], Loss: 2.2541\n",
      "Epoch [8/10], Step [44/390], Loss: 2.0900\n",
      "Epoch [8/10], Step [45/390], Loss: 2.3182\n",
      "Epoch [8/10], Step [46/390], Loss: 1.9796\n",
      "Epoch [8/10], Step [47/390], Loss: 1.7872\n",
      "Epoch [8/10], Step [48/390], Loss: 2.1599\n",
      "Epoch [8/10], Step [49/390], Loss: 1.9442\n",
      "Epoch [8/10], Step [50/390], Loss: 2.1026\n",
      "Epoch [8/10], Step [51/390], Loss: 1.8576\n",
      "Epoch [8/10], Step [52/390], Loss: 1.7116\n",
      "Epoch [8/10], Step [53/390], Loss: 1.9014\n",
      "Epoch [8/10], Step [54/390], Loss: 1.9580\n",
      "Epoch [8/10], Step [55/390], Loss: 2.0811\n",
      "Epoch [8/10], Step [56/390], Loss: 1.8094\n",
      "Epoch [8/10], Step [57/390], Loss: 1.8793\n",
      "Epoch [8/10], Step [58/390], Loss: 2.0204\n",
      "Epoch [8/10], Step [59/390], Loss: 1.9117\n",
      "Epoch [8/10], Step [60/390], Loss: 2.0443\n",
      "Epoch [8/10], Step [61/390], Loss: 2.1278\n",
      "Epoch [8/10], Step [62/390], Loss: 1.9433\n",
      "Epoch [8/10], Step [63/390], Loss: 2.0973\n",
      "Epoch [8/10], Step [64/390], Loss: 1.7523\n",
      "Epoch [8/10], Step [65/390], Loss: 2.0750\n",
      "Epoch [8/10], Step [66/390], Loss: 2.1296\n",
      "Epoch [8/10], Step [67/390], Loss: 2.0937\n",
      "Epoch [8/10], Step [68/390], Loss: 2.1663\n",
      "Epoch [8/10], Step [69/390], Loss: 1.8983\n",
      "Epoch [8/10], Step [70/390], Loss: 2.0967\n",
      "Epoch [8/10], Step [71/390], Loss: 1.9557\n",
      "Epoch [8/10], Step [72/390], Loss: 1.9311\n",
      "Epoch [8/10], Step [73/390], Loss: 2.1050\n",
      "Epoch [8/10], Step [74/390], Loss: 2.1526\n",
      "Epoch [8/10], Step [75/390], Loss: 2.3905\n",
      "Epoch [8/10], Step [76/390], Loss: 1.9625\n",
      "Epoch [8/10], Step [77/390], Loss: 1.9658\n",
      "Epoch [8/10], Step [78/390], Loss: 1.8706\n",
      "Epoch [8/10], Step [79/390], Loss: 1.9542\n",
      "Epoch [8/10], Step [80/390], Loss: 2.1928\n",
      "Epoch [8/10], Step [81/390], Loss: 1.9026\n",
      "Epoch [8/10], Step [82/390], Loss: 1.8653\n",
      "Epoch [8/10], Step [83/390], Loss: 1.8693\n",
      "Epoch [8/10], Step [84/390], Loss: 2.0534\n",
      "Epoch [8/10], Step [85/390], Loss: 2.1168\n",
      "Epoch [8/10], Step [86/390], Loss: 1.7823\n",
      "Epoch [8/10], Step [87/390], Loss: 2.0736\n",
      "Epoch [8/10], Step [88/390], Loss: 2.2437\n",
      "Epoch [8/10], Step [89/390], Loss: 2.1737\n",
      "Epoch [8/10], Step [90/390], Loss: 1.9276\n",
      "Epoch [8/10], Step [91/390], Loss: 2.0224\n",
      "Epoch [8/10], Step [92/390], Loss: 1.8547\n",
      "Epoch [8/10], Step [93/390], Loss: 2.0871\n",
      "Epoch [8/10], Step [94/390], Loss: 1.8316\n",
      "Epoch [8/10], Step [95/390], Loss: 1.9236\n",
      "Epoch [8/10], Step [96/390], Loss: 1.8598\n",
      "Epoch [8/10], Step [97/390], Loss: 1.8933\n",
      "Epoch [8/10], Step [98/390], Loss: 1.9546\n",
      "Epoch [8/10], Step [99/390], Loss: 1.7561\n",
      "Epoch [8/10], Step [100/390], Loss: 1.8781\n",
      "Epoch [8/10], Step [101/390], Loss: 1.7677\n",
      "Epoch [8/10], Step [102/390], Loss: 2.1630\n",
      "Epoch [8/10], Step [103/390], Loss: 2.1977\n",
      "Epoch [8/10], Step [104/390], Loss: 1.9189\n",
      "Epoch [8/10], Step [105/390], Loss: 2.0773\n",
      "Epoch [8/10], Step [106/390], Loss: 2.0597\n",
      "Epoch [8/10], Step [107/390], Loss: 1.8662\n",
      "Epoch [8/10], Step [108/390], Loss: 2.0808\n",
      "Epoch [8/10], Step [109/390], Loss: 2.0114\n",
      "Epoch [8/10], Step [110/390], Loss: 2.4365\n",
      "Epoch [8/10], Step [111/390], Loss: 1.9126\n",
      "Epoch [8/10], Step [112/390], Loss: 2.0450\n",
      "Epoch [8/10], Step [113/390], Loss: 1.8452\n",
      "Epoch [8/10], Step [114/390], Loss: 1.8085\n",
      "Epoch [8/10], Step [115/390], Loss: 1.9347\n",
      "Epoch [8/10], Step [116/390], Loss: 2.1872\n",
      "Epoch [8/10], Step [117/390], Loss: 2.1358\n",
      "Epoch [8/10], Step [118/390], Loss: 2.0212\n",
      "Epoch [8/10], Step [119/390], Loss: 2.0078\n",
      "Epoch [8/10], Step [120/390], Loss: 2.0150\n",
      "Epoch [8/10], Step [121/390], Loss: 2.0436\n",
      "Epoch [8/10], Step [122/390], Loss: 1.8455\n",
      "Epoch [8/10], Step [123/390], Loss: 2.1329\n",
      "Epoch [8/10], Step [124/390], Loss: 2.1760\n",
      "Epoch [8/10], Step [125/390], Loss: 2.4117\n",
      "Epoch [8/10], Step [126/390], Loss: 1.9855\n",
      "Epoch [8/10], Step [127/390], Loss: 1.8184\n",
      "Epoch [8/10], Step [128/390], Loss: 1.9760\n",
      "Epoch [8/10], Step [129/390], Loss: 2.3512\n",
      "Epoch [8/10], Step [130/390], Loss: 1.8921\n",
      "Epoch [8/10], Step [131/390], Loss: 1.9513\n",
      "Epoch [8/10], Step [132/390], Loss: 1.8611\n",
      "Epoch [8/10], Step [133/390], Loss: 2.0734\n",
      "Epoch [8/10], Step [134/390], Loss: 1.8656\n",
      "Epoch [8/10], Step [135/390], Loss: 1.9691\n",
      "Epoch [8/10], Step [136/390], Loss: 1.7205\n",
      "Epoch [8/10], Step [137/390], Loss: 2.4282\n",
      "Epoch [8/10], Step [138/390], Loss: 2.0693\n",
      "Epoch [8/10], Step [139/390], Loss: 1.8195\n",
      "Epoch [8/10], Step [140/390], Loss: 2.3072\n",
      "Epoch [8/10], Step [141/390], Loss: 1.8248\n",
      "Epoch [8/10], Step [142/390], Loss: 2.4605\n",
      "Epoch [8/10], Step [143/390], Loss: 1.9848\n",
      "Epoch [8/10], Step [144/390], Loss: 1.8993\n",
      "Epoch [8/10], Step [145/390], Loss: 1.9685\n",
      "Epoch [8/10], Step [146/390], Loss: 2.0188\n",
      "Epoch [8/10], Step [147/390], Loss: 1.8552\n",
      "Epoch [8/10], Step [148/390], Loss: 2.0295\n",
      "Epoch [8/10], Step [149/390], Loss: 1.8905\n",
      "Epoch [8/10], Step [150/390], Loss: 1.9871\n",
      "Epoch [8/10], Step [151/390], Loss: 2.3185\n",
      "Epoch [8/10], Step [152/390], Loss: 1.9107\n",
      "Epoch [8/10], Step [153/390], Loss: 2.0022\n",
      "Epoch [8/10], Step [154/390], Loss: 2.0073\n",
      "Epoch [8/10], Step [155/390], Loss: 1.9920\n",
      "Epoch [8/10], Step [156/390], Loss: 2.0013\n",
      "Epoch [8/10], Step [157/390], Loss: 2.1073\n",
      "Epoch [8/10], Step [158/390], Loss: 1.8544\n",
      "Epoch [8/10], Step [159/390], Loss: 2.0163\n",
      "Epoch [8/10], Step [160/390], Loss: 2.0398\n",
      "Epoch [8/10], Step [161/390], Loss: 2.2877\n",
      "Epoch [8/10], Step [162/390], Loss: 1.9137\n",
      "Epoch [8/10], Step [163/390], Loss: 2.3000\n",
      "Epoch [8/10], Step [164/390], Loss: 1.6239\n",
      "Epoch [8/10], Step [165/390], Loss: 2.0737\n",
      "Epoch [8/10], Step [166/390], Loss: 2.0468\n",
      "Epoch [8/10], Step [167/390], Loss: 2.0965\n",
      "Epoch [8/10], Step [168/390], Loss: 1.8945\n",
      "Epoch [8/10], Step [169/390], Loss: 2.1208\n",
      "Epoch [8/10], Step [170/390], Loss: 2.0183\n",
      "Epoch [8/10], Step [171/390], Loss: 2.0392\n",
      "Epoch [8/10], Step [172/390], Loss: 2.1523\n",
      "Epoch [8/10], Step [173/390], Loss: 1.9138\n",
      "Epoch [8/10], Step [174/390], Loss: 1.9281\n",
      "Epoch [8/10], Step [175/390], Loss: 1.5998\n",
      "Epoch [8/10], Step [176/390], Loss: 2.1130\n",
      "Epoch [8/10], Step [177/390], Loss: 2.0453\n",
      "Epoch [8/10], Step [178/390], Loss: 1.8125\n",
      "Epoch [8/10], Step [179/390], Loss: 1.9911\n",
      "Epoch [8/10], Step [180/390], Loss: 1.7394\n",
      "Epoch [8/10], Step [181/390], Loss: 1.8877\n",
      "Epoch [8/10], Step [182/390], Loss: 1.9030\n",
      "Epoch [8/10], Step [183/390], Loss: 1.9707\n",
      "Epoch [8/10], Step [184/390], Loss: 2.0409\n",
      "Epoch [8/10], Step [185/390], Loss: 1.9074\n",
      "Epoch [8/10], Step [186/390], Loss: 2.0415\n",
      "Epoch [8/10], Step [187/390], Loss: 1.7676\n",
      "Epoch [8/10], Step [188/390], Loss: 2.0572\n",
      "Epoch [8/10], Step [189/390], Loss: 2.0777\n",
      "Epoch [8/10], Step [190/390], Loss: 2.3637\n",
      "Epoch [8/10], Step [191/390], Loss: 1.8307\n",
      "Epoch [8/10], Step [192/390], Loss: 2.1385\n",
      "Epoch [8/10], Step [193/390], Loss: 1.9043\n",
      "Epoch [8/10], Step [194/390], Loss: 1.8894\n",
      "Epoch [8/10], Step [195/390], Loss: 1.9358\n",
      "Epoch [8/10], Step [196/390], Loss: 1.9572\n",
      "Epoch [8/10], Step [197/390], Loss: 1.8958\n",
      "Epoch [8/10], Step [198/390], Loss: 2.0849\n",
      "Epoch [8/10], Step [199/390], Loss: 2.1142\n",
      "Epoch [8/10], Step [200/390], Loss: 1.9886\n",
      "Epoch [8/10], Step [201/390], Loss: 1.9391\n",
      "Epoch [8/10], Step [202/390], Loss: 2.0069\n",
      "Epoch [8/10], Step [203/390], Loss: 1.9421\n",
      "Epoch [8/10], Step [204/390], Loss: 2.1430\n",
      "Epoch [8/10], Step [205/390], Loss: 2.2460\n",
      "Epoch [8/10], Step [206/390], Loss: 1.9011\n",
      "Epoch [8/10], Step [207/390], Loss: 1.9333\n",
      "Epoch [8/10], Step [208/390], Loss: 1.9397\n",
      "Epoch [8/10], Step [209/390], Loss: 2.0823\n",
      "Epoch [8/10], Step [210/390], Loss: 2.0676\n",
      "Epoch [8/10], Step [211/390], Loss: 1.6208\n",
      "Epoch [8/10], Step [212/390], Loss: 2.1916\n",
      "Epoch [8/10], Step [213/390], Loss: 1.7948\n",
      "Epoch [8/10], Step [214/390], Loss: 1.8971\n",
      "Epoch [8/10], Step [215/390], Loss: 2.2175\n",
      "Epoch [8/10], Step [216/390], Loss: 2.1416\n",
      "Epoch [8/10], Step [217/390], Loss: 2.1762\n",
      "Epoch [8/10], Step [218/390], Loss: 2.0697\n",
      "Epoch [8/10], Step [219/390], Loss: 2.0018\n",
      "Epoch [8/10], Step [220/390], Loss: 1.9268\n",
      "Epoch [8/10], Step [221/390], Loss: 2.3119\n",
      "Epoch [8/10], Step [222/390], Loss: 2.1413\n",
      "Epoch [8/10], Step [223/390], Loss: 2.0632\n",
      "Epoch [8/10], Step [224/390], Loss: 1.9563\n",
      "Epoch [8/10], Step [225/390], Loss: 2.0105\n",
      "Epoch [8/10], Step [226/390], Loss: 1.7690\n",
      "Epoch [8/10], Step [227/390], Loss: 2.0058\n",
      "Epoch [8/10], Step [228/390], Loss: 2.0017\n",
      "Epoch [8/10], Step [229/390], Loss: 1.8171\n",
      "Epoch [8/10], Step [230/390], Loss: 2.0733\n",
      "Epoch [8/10], Step [231/390], Loss: 2.0181\n",
      "Epoch [8/10], Step [232/390], Loss: 1.9349\n",
      "Epoch [8/10], Step [233/390], Loss: 2.1573\n",
      "Epoch [8/10], Step [234/390], Loss: 2.2223\n",
      "Epoch [8/10], Step [235/390], Loss: 1.8632\n",
      "Epoch [8/10], Step [236/390], Loss: 2.0511\n",
      "Epoch [8/10], Step [237/390], Loss: 2.0662\n",
      "Epoch [8/10], Step [238/390], Loss: 2.0730\n",
      "Epoch [8/10], Step [239/390], Loss: 2.2794\n",
      "Epoch [8/10], Step [240/390], Loss: 2.0862\n",
      "Epoch [8/10], Step [241/390], Loss: 2.0529\n",
      "Epoch [8/10], Step [242/390], Loss: 1.8499\n",
      "Epoch [8/10], Step [243/390], Loss: 1.8311\n",
      "Epoch [8/10], Step [244/390], Loss: 1.9234\n",
      "Epoch [8/10], Step [245/390], Loss: 1.9566\n",
      "Epoch [8/10], Step [246/390], Loss: 2.0117\n",
      "Epoch [8/10], Step [247/390], Loss: 2.2004\n",
      "Epoch [8/10], Step [248/390], Loss: 1.9932\n",
      "Epoch [8/10], Step [249/390], Loss: 2.1370\n",
      "Epoch [8/10], Step [250/390], Loss: 1.9621\n",
      "Epoch [8/10], Step [251/390], Loss: 1.8345\n",
      "Epoch [8/10], Step [252/390], Loss: 2.0050\n",
      "Epoch [8/10], Step [253/390], Loss: 1.9029\n",
      "Epoch [8/10], Step [254/390], Loss: 1.9419\n",
      "Epoch [8/10], Step [255/390], Loss: 2.0726\n",
      "Epoch [8/10], Step [256/390], Loss: 2.0468\n",
      "Epoch [8/10], Step [257/390], Loss: 2.0730\n",
      "Epoch [8/10], Step [258/390], Loss: 1.9699\n",
      "Epoch [8/10], Step [259/390], Loss: 1.9869\n",
      "Epoch [8/10], Step [260/390], Loss: 1.8959\n",
      "Epoch [8/10], Step [261/390], Loss: 2.0950\n",
      "Epoch [8/10], Step [262/390], Loss: 2.0687\n",
      "Epoch [8/10], Step [263/390], Loss: 2.1174\n",
      "Epoch [8/10], Step [264/390], Loss: 2.2627\n",
      "Epoch [8/10], Step [265/390], Loss: 1.9054\n",
      "Epoch [8/10], Step [266/390], Loss: 2.4236\n",
      "Epoch [8/10], Step [267/390], Loss: 2.0650\n",
      "Epoch [8/10], Step [268/390], Loss: 2.0924\n",
      "Epoch [8/10], Step [269/390], Loss: 2.1510\n",
      "Epoch [8/10], Step [270/390], Loss: 2.2343\n",
      "Epoch [8/10], Step [271/390], Loss: 1.9387\n",
      "Epoch [8/10], Step [272/390], Loss: 2.2813\n",
      "Epoch [8/10], Step [273/390], Loss: 2.1823\n",
      "Epoch [8/10], Step [274/390], Loss: 2.0052\n",
      "Epoch [8/10], Step [275/390], Loss: 2.0099\n",
      "Epoch [8/10], Step [276/390], Loss: 2.1143\n",
      "Epoch [8/10], Step [277/390], Loss: 2.2869\n",
      "Epoch [8/10], Step [278/390], Loss: 1.9365\n",
      "Epoch [8/10], Step [279/390], Loss: 2.0831\n",
      "Epoch [8/10], Step [280/390], Loss: 2.1237\n",
      "Epoch [8/10], Step [281/390], Loss: 1.9842\n",
      "Epoch [8/10], Step [282/390], Loss: 1.9492\n",
      "Epoch [8/10], Step [283/390], Loss: 1.9607\n",
      "Epoch [8/10], Step [284/390], Loss: 1.9904\n",
      "Epoch [8/10], Step [285/390], Loss: 1.9297\n",
      "Epoch [8/10], Step [286/390], Loss: 2.0113\n",
      "Epoch [8/10], Step [287/390], Loss: 2.0115\n",
      "Epoch [8/10], Step [288/390], Loss: 1.9001\n",
      "Epoch [8/10], Step [289/390], Loss: 1.8447\n",
      "Epoch [8/10], Step [290/390], Loss: 1.8789\n",
      "Epoch [8/10], Step [291/390], Loss: 2.1345\n",
      "Epoch [8/10], Step [292/390], Loss: 1.9727\n",
      "Epoch [8/10], Step [293/390], Loss: 2.1028\n",
      "Epoch [8/10], Step [294/390], Loss: 1.8316\n",
      "Epoch [8/10], Step [295/390], Loss: 1.9456\n",
      "Epoch [8/10], Step [296/390], Loss: 1.9236\n",
      "Epoch [8/10], Step [297/390], Loss: 2.0417\n",
      "Epoch [8/10], Step [298/390], Loss: 2.0173\n",
      "Epoch [8/10], Step [299/390], Loss: 1.8352\n",
      "Epoch [8/10], Step [300/390], Loss: 2.0706\n",
      "Epoch [8/10], Step [301/390], Loss: 2.1140\n",
      "Epoch [8/10], Step [302/390], Loss: 1.8631\n",
      "Epoch [8/10], Step [303/390], Loss: 1.7486\n",
      "Epoch [8/10], Step [304/390], Loss: 1.9565\n",
      "Epoch [8/10], Step [305/390], Loss: 2.0501\n",
      "Epoch [8/10], Step [306/390], Loss: 1.9808\n",
      "Epoch [8/10], Step [307/390], Loss: 1.9071\n",
      "Epoch [8/10], Step [308/390], Loss: 2.0015\n",
      "Epoch [8/10], Step [309/390], Loss: 2.0647\n",
      "Epoch [8/10], Step [310/390], Loss: 1.9834\n",
      "Epoch [8/10], Step [311/390], Loss: 1.7001\n",
      "Epoch [8/10], Step [312/390], Loss: 1.9432\n",
      "Epoch [8/10], Step [313/390], Loss: 1.7105\n",
      "Epoch [8/10], Step [314/390], Loss: 2.0546\n",
      "Epoch [8/10], Step [315/390], Loss: 1.7368\n",
      "Epoch [8/10], Step [316/390], Loss: 1.9032\n",
      "Epoch [8/10], Step [317/390], Loss: 1.9798\n",
      "Epoch [8/10], Step [318/390], Loss: 2.0318\n",
      "Epoch [8/10], Step [319/390], Loss: 2.0730\n",
      "Epoch [8/10], Step [320/390], Loss: 1.8408\n",
      "Epoch [8/10], Step [321/390], Loss: 2.2804\n",
      "Epoch [8/10], Step [322/390], Loss: 2.2232\n",
      "Epoch [8/10], Step [323/390], Loss: 2.0414\n",
      "Epoch [8/10], Step [324/390], Loss: 1.8665\n",
      "Epoch [8/10], Step [325/390], Loss: 1.8741\n",
      "Epoch [8/10], Step [326/390], Loss: 2.0569\n",
      "Epoch [8/10], Step [327/390], Loss: 1.6354\n",
      "Epoch [8/10], Step [328/390], Loss: 2.0603\n",
      "Epoch [8/10], Step [329/390], Loss: 2.1394\n",
      "Epoch [8/10], Step [330/390], Loss: 2.0691\n",
      "Epoch [8/10], Step [331/390], Loss: 1.6855\n",
      "Epoch [8/10], Step [332/390], Loss: 1.8354\n",
      "Epoch [8/10], Step [333/390], Loss: 1.7553\n",
      "Epoch [8/10], Step [334/390], Loss: 1.8924\n",
      "Epoch [8/10], Step [335/390], Loss: 1.9672\n",
      "Epoch [8/10], Step [336/390], Loss: 2.0022\n",
      "Epoch [8/10], Step [337/390], Loss: 1.9521\n",
      "Epoch [8/10], Step [338/390], Loss: 1.8492\n",
      "Epoch [8/10], Step [339/390], Loss: 2.1069\n",
      "Epoch [8/10], Step [340/390], Loss: 2.1814\n",
      "Epoch [8/10], Step [341/390], Loss: 1.9084\n",
      "Epoch [8/10], Step [342/390], Loss: 2.0336\n",
      "Epoch [8/10], Step [343/390], Loss: 2.1818\n",
      "Epoch [8/10], Step [344/390], Loss: 2.0134\n",
      "Epoch [8/10], Step [345/390], Loss: 1.9753\n",
      "Epoch [8/10], Step [346/390], Loss: 1.6774\n",
      "Epoch [8/10], Step [347/390], Loss: 2.0158\n",
      "Epoch [8/10], Step [348/390], Loss: 2.0050\n",
      "Epoch [8/10], Step [349/390], Loss: 2.2263\n",
      "Epoch [8/10], Step [350/390], Loss: 1.8329\n",
      "Epoch [8/10], Step [351/390], Loss: 1.7729\n",
      "Epoch [8/10], Step [352/390], Loss: 2.0537\n",
      "Epoch [8/10], Step [353/390], Loss: 1.9578\n",
      "Epoch [8/10], Step [354/390], Loss: 2.1375\n",
      "Epoch [8/10], Step [355/390], Loss: 2.1863\n",
      "Epoch [8/10], Step [356/390], Loss: 1.9476\n",
      "Epoch [8/10], Step [357/390], Loss: 1.9924\n",
      "Epoch [8/10], Step [358/390], Loss: 2.0515\n",
      "Epoch [8/10], Step [359/390], Loss: 1.8704\n",
      "Epoch [8/10], Step [360/390], Loss: 1.9725\n",
      "Epoch [8/10], Step [361/390], Loss: 2.1280\n",
      "Epoch [8/10], Step [362/390], Loss: 2.0255\n",
      "Epoch [8/10], Step [363/390], Loss: 1.7453\n",
      "Epoch [8/10], Step [364/390], Loss: 2.0067\n",
      "Epoch [8/10], Step [365/390], Loss: 2.0562\n",
      "Epoch [8/10], Step [366/390], Loss: 2.0843\n",
      "Epoch [8/10], Step [367/390], Loss: 2.1086\n",
      "Epoch [8/10], Step [368/390], Loss: 2.3870\n",
      "Epoch [8/10], Step [369/390], Loss: 1.7984\n",
      "Epoch [8/10], Step [370/390], Loss: 1.9764\n",
      "Epoch [8/10], Step [371/390], Loss: 2.1153\n",
      "Epoch [8/10], Step [372/390], Loss: 1.8419\n",
      "Epoch [8/10], Step [373/390], Loss: 1.8525\n",
      "Epoch [8/10], Step [374/390], Loss: 1.9629\n",
      "Epoch [8/10], Step [375/390], Loss: 2.1535\n",
      "Epoch [8/10], Step [376/390], Loss: 1.8937\n",
      "Epoch [8/10], Step [377/390], Loss: 1.9802\n",
      "Epoch [8/10], Step [378/390], Loss: 2.1175\n",
      "Epoch [8/10], Step [379/390], Loss: 1.8855\n",
      "Epoch [8/10], Step [380/390], Loss: 2.0720\n",
      "Epoch [8/10], Step [381/390], Loss: 2.1939\n",
      "Epoch [8/10], Step [382/390], Loss: 2.1065\n",
      "Epoch [8/10], Step [383/390], Loss: 1.8406\n",
      "Epoch [8/10], Step [384/390], Loss: 1.8695\n",
      "Epoch [8/10], Step [385/390], Loss: 2.0906\n",
      "Epoch [8/10], Step [386/390], Loss: 2.3098\n",
      "Epoch [8/10], Step [387/390], Loss: 1.9594\n",
      "Epoch [8/10], Step [388/390], Loss: 2.3382\n",
      "Epoch [8/10], Step [389/390], Loss: 2.0089\n",
      "Epoch [8/10], Step [390/390], Loss: 2.2274\n",
      "Epoch [8/10], Step [391/390], Loss: 2.2238\n",
      "Epoch [9/10], Step [1/390], Loss: 2.0614\n",
      "Epoch [9/10], Step [2/390], Loss: 1.9513\n",
      "Epoch [9/10], Step [3/390], Loss: 1.8427\n",
      "Epoch [9/10], Step [4/390], Loss: 1.7529\n",
      "Epoch [9/10], Step [5/390], Loss: 1.8472\n",
      "Epoch [9/10], Step [6/390], Loss: 1.7751\n",
      "Epoch [9/10], Step [7/390], Loss: 1.7066\n",
      "Epoch [9/10], Step [8/390], Loss: 1.8099\n",
      "Epoch [9/10], Step [9/390], Loss: 1.9120\n",
      "Epoch [9/10], Step [10/390], Loss: 2.0585\n",
      "Epoch [9/10], Step [11/390], Loss: 1.5407\n",
      "Epoch [9/10], Step [12/390], Loss: 1.5980\n",
      "Epoch [9/10], Step [13/390], Loss: 2.0104\n",
      "Epoch [9/10], Step [14/390], Loss: 1.8791\n",
      "Epoch [9/10], Step [15/390], Loss: 1.7850\n",
      "Epoch [9/10], Step [16/390], Loss: 1.8446\n",
      "Epoch [9/10], Step [17/390], Loss: 1.7198\n",
      "Epoch [9/10], Step [18/390], Loss: 1.6709\n",
      "Epoch [9/10], Step [19/390], Loss: 1.6433\n",
      "Epoch [9/10], Step [20/390], Loss: 1.7912\n",
      "Epoch [9/10], Step [21/390], Loss: 1.5568\n",
      "Epoch [9/10], Step [22/390], Loss: 1.8269\n",
      "Epoch [9/10], Step [23/390], Loss: 1.7978\n",
      "Epoch [9/10], Step [24/390], Loss: 1.7741\n",
      "Epoch [9/10], Step [25/390], Loss: 2.0116\n",
      "Epoch [9/10], Step [26/390], Loss: 1.8882\n",
      "Epoch [9/10], Step [27/390], Loss: 1.8045\n",
      "Epoch [9/10], Step [28/390], Loss: 1.8633\n",
      "Epoch [9/10], Step [29/390], Loss: 1.9226\n",
      "Epoch [9/10], Step [30/390], Loss: 1.9342\n",
      "Epoch [9/10], Step [31/390], Loss: 1.9076\n",
      "Epoch [9/10], Step [32/390], Loss: 1.6988\n",
      "Epoch [9/10], Step [33/390], Loss: 1.7264\n",
      "Epoch [9/10], Step [34/390], Loss: 1.7614\n",
      "Epoch [9/10], Step [35/390], Loss: 1.5244\n",
      "Epoch [9/10], Step [36/390], Loss: 1.6711\n",
      "Epoch [9/10], Step [37/390], Loss: 1.9399\n",
      "Epoch [9/10], Step [38/390], Loss: 1.7302\n",
      "Epoch [9/10], Step [39/390], Loss: 1.7757\n",
      "Epoch [9/10], Step [40/390], Loss: 1.7312\n",
      "Epoch [9/10], Step [41/390], Loss: 1.5896\n",
      "Epoch [9/10], Step [42/390], Loss: 1.8068\n",
      "Epoch [9/10], Step [43/390], Loss: 1.7588\n",
      "Epoch [9/10], Step [44/390], Loss: 1.8126\n",
      "Epoch [9/10], Step [45/390], Loss: 2.2807\n",
      "Epoch [9/10], Step [46/390], Loss: 1.9532\n",
      "Epoch [9/10], Step [47/390], Loss: 1.6067\n",
      "Epoch [9/10], Step [48/390], Loss: 1.7019\n",
      "Epoch [9/10], Step [49/390], Loss: 1.9991\n",
      "Epoch [9/10], Step [50/390], Loss: 1.7398\n",
      "Epoch [9/10], Step [51/390], Loss: 1.6741\n",
      "Epoch [9/10], Step [52/390], Loss: 1.5295\n",
      "Epoch [9/10], Step [53/390], Loss: 1.8398\n",
      "Epoch [9/10], Step [54/390], Loss: 1.8640\n",
      "Epoch [9/10], Step [55/390], Loss: 1.8777\n",
      "Epoch [9/10], Step [56/390], Loss: 1.9215\n",
      "Epoch [9/10], Step [57/390], Loss: 1.8148\n",
      "Epoch [9/10], Step [58/390], Loss: 1.5459\n",
      "Epoch [9/10], Step [59/390], Loss: 1.6544\n",
      "Epoch [9/10], Step [60/390], Loss: 1.8828\n",
      "Epoch [9/10], Step [61/390], Loss: 2.0506\n",
      "Epoch [9/10], Step [62/390], Loss: 1.6444\n",
      "Epoch [9/10], Step [63/390], Loss: 1.9289\n",
      "Epoch [9/10], Step [64/390], Loss: 1.8605\n",
      "Epoch [9/10], Step [65/390], Loss: 1.6700\n",
      "Epoch [9/10], Step [66/390], Loss: 1.6616\n",
      "Epoch [9/10], Step [67/390], Loss: 1.7967\n",
      "Epoch [9/10], Step [68/390], Loss: 1.8783\n",
      "Epoch [9/10], Step [69/390], Loss: 1.9658\n",
      "Epoch [9/10], Step [70/390], Loss: 2.0089\n",
      "Epoch [9/10], Step [71/390], Loss: 1.7890\n",
      "Epoch [9/10], Step [72/390], Loss: 1.6541\n",
      "Epoch [9/10], Step [73/390], Loss: 1.8801\n",
      "Epoch [9/10], Step [74/390], Loss: 1.9215\n",
      "Epoch [9/10], Step [75/390], Loss: 1.7535\n",
      "Epoch [9/10], Step [76/390], Loss: 1.5996\n",
      "Epoch [9/10], Step [77/390], Loss: 1.7661\n",
      "Epoch [9/10], Step [78/390], Loss: 1.7075\n",
      "Epoch [9/10], Step [79/390], Loss: 1.6931\n",
      "Epoch [9/10], Step [80/390], Loss: 1.9177\n",
      "Epoch [9/10], Step [81/390], Loss: 1.7657\n",
      "Epoch [9/10], Step [82/390], Loss: 1.6939\n",
      "Epoch [9/10], Step [83/390], Loss: 2.0372\n",
      "Epoch [9/10], Step [84/390], Loss: 1.9037\n",
      "Epoch [9/10], Step [85/390], Loss: 1.8616\n",
      "Epoch [9/10], Step [86/390], Loss: 1.9120\n",
      "Epoch [9/10], Step [87/390], Loss: 2.1241\n",
      "Epoch [9/10], Step [88/390], Loss: 1.9658\n",
      "Epoch [9/10], Step [89/390], Loss: 1.6097\n",
      "Epoch [9/10], Step [90/390], Loss: 1.8588\n",
      "Epoch [9/10], Step [91/390], Loss: 1.8803\n",
      "Epoch [9/10], Step [92/390], Loss: 1.7798\n",
      "Epoch [9/10], Step [93/390], Loss: 1.7691\n",
      "Epoch [9/10], Step [94/390], Loss: 1.5964\n",
      "Epoch [9/10], Step [95/390], Loss: 1.8987\n",
      "Epoch [9/10], Step [96/390], Loss: 1.9845\n",
      "Epoch [9/10], Step [97/390], Loss: 1.6358\n",
      "Epoch [9/10], Step [98/390], Loss: 1.7652\n",
      "Epoch [9/10], Step [99/390], Loss: 1.8308\n",
      "Epoch [9/10], Step [100/390], Loss: 1.9043\n",
      "Epoch [9/10], Step [101/390], Loss: 1.8495\n",
      "Epoch [9/10], Step [102/390], Loss: 1.8199\n",
      "Epoch [9/10], Step [103/390], Loss: 1.6956\n",
      "Epoch [9/10], Step [104/390], Loss: 2.0128\n",
      "Epoch [9/10], Step [105/390], Loss: 1.8966\n",
      "Epoch [9/10], Step [106/390], Loss: 2.0668\n",
      "Epoch [9/10], Step [107/390], Loss: 1.7453\n",
      "Epoch [9/10], Step [108/390], Loss: 1.9174\n",
      "Epoch [9/10], Step [109/390], Loss: 2.0568\n",
      "Epoch [9/10], Step [110/390], Loss: 1.7866\n",
      "Epoch [9/10], Step [111/390], Loss: 1.8207\n",
      "Epoch [9/10], Step [112/390], Loss: 1.6856\n",
      "Epoch [9/10], Step [113/390], Loss: 2.0577\n",
      "Epoch [9/10], Step [114/390], Loss: 1.9508\n",
      "Epoch [9/10], Step [115/390], Loss: 1.8142\n",
      "Epoch [9/10], Step [116/390], Loss: 1.8018\n",
      "Epoch [9/10], Step [117/390], Loss: 1.9089\n",
      "Epoch [9/10], Step [118/390], Loss: 1.8851\n",
      "Epoch [9/10], Step [119/390], Loss: 1.7233\n",
      "Epoch [9/10], Step [120/390], Loss: 1.9582\n",
      "Epoch [9/10], Step [121/390], Loss: 1.8239\n",
      "Epoch [9/10], Step [122/390], Loss: 1.6912\n",
      "Epoch [9/10], Step [123/390], Loss: 1.7381\n",
      "Epoch [9/10], Step [124/390], Loss: 1.9164\n",
      "Epoch [9/10], Step [125/390], Loss: 1.7512\n",
      "Epoch [9/10], Step [126/390], Loss: 1.9645\n",
      "Epoch [9/10], Step [127/390], Loss: 1.6051\n",
      "Epoch [9/10], Step [128/390], Loss: 1.9526\n",
      "Epoch [9/10], Step [129/390], Loss: 1.8113\n",
      "Epoch [9/10], Step [130/390], Loss: 1.9525\n",
      "Epoch [9/10], Step [131/390], Loss: 1.5188\n",
      "Epoch [9/10], Step [132/390], Loss: 1.7792\n",
      "Epoch [9/10], Step [133/390], Loss: 1.9878\n",
      "Epoch [9/10], Step [134/390], Loss: 1.6934\n",
      "Epoch [9/10], Step [135/390], Loss: 1.7733\n",
      "Epoch [9/10], Step [136/390], Loss: 1.9726\n",
      "Epoch [9/10], Step [137/390], Loss: 1.7318\n",
      "Epoch [9/10], Step [138/390], Loss: 1.9310\n",
      "Epoch [9/10], Step [139/390], Loss: 1.9322\n",
      "Epoch [9/10], Step [140/390], Loss: 2.0448\n",
      "Epoch [9/10], Step [141/390], Loss: 1.3850\n",
      "Epoch [9/10], Step [142/390], Loss: 1.6499\n",
      "Epoch [9/10], Step [143/390], Loss: 1.8296\n",
      "Epoch [9/10], Step [144/390], Loss: 1.5709\n",
      "Epoch [9/10], Step [145/390], Loss: 1.8271\n",
      "Epoch [9/10], Step [146/390], Loss: 1.8083\n",
      "Epoch [9/10], Step [147/390], Loss: 1.9937\n",
      "Epoch [9/10], Step [148/390], Loss: 1.9468\n",
      "Epoch [9/10], Step [149/390], Loss: 1.9227\n",
      "Epoch [9/10], Step [150/390], Loss: 2.1326\n",
      "Epoch [9/10], Step [151/390], Loss: 1.6966\n",
      "Epoch [9/10], Step [152/390], Loss: 1.8401\n",
      "Epoch [9/10], Step [153/390], Loss: 1.9136\n",
      "Epoch [9/10], Step [154/390], Loss: 1.9733\n",
      "Epoch [9/10], Step [155/390], Loss: 1.8693\n",
      "Epoch [9/10], Step [156/390], Loss: 1.6912\n",
      "Epoch [9/10], Step [157/390], Loss: 1.8040\n",
      "Epoch [9/10], Step [158/390], Loss: 1.9626\n",
      "Epoch [9/10], Step [159/390], Loss: 2.0336\n",
      "Epoch [9/10], Step [160/390], Loss: 1.7082\n",
      "Epoch [9/10], Step [161/390], Loss: 1.9914\n",
      "Epoch [9/10], Step [162/390], Loss: 2.1159\n",
      "Epoch [9/10], Step [163/390], Loss: 1.6970\n",
      "Epoch [9/10], Step [164/390], Loss: 1.9842\n",
      "Epoch [9/10], Step [165/390], Loss: 1.7717\n",
      "Epoch [9/10], Step [166/390], Loss: 1.9356\n",
      "Epoch [9/10], Step [167/390], Loss: 1.7349\n",
      "Epoch [9/10], Step [168/390], Loss: 1.8946\n",
      "Epoch [9/10], Step [169/390], Loss: 2.0046\n",
      "Epoch [9/10], Step [170/390], Loss: 1.7468\n",
      "Epoch [9/10], Step [171/390], Loss: 1.8331\n",
      "Epoch [9/10], Step [172/390], Loss: 1.5695\n",
      "Epoch [9/10], Step [173/390], Loss: 1.6488\n",
      "Epoch [9/10], Step [174/390], Loss: 1.9137\n",
      "Epoch [9/10], Step [175/390], Loss: 1.8258\n",
      "Epoch [9/10], Step [176/390], Loss: 1.7276\n",
      "Epoch [9/10], Step [177/390], Loss: 1.7019\n",
      "Epoch [9/10], Step [178/390], Loss: 1.6893\n",
      "Epoch [9/10], Step [179/390], Loss: 1.7519\n",
      "Epoch [9/10], Step [180/390], Loss: 1.9455\n",
      "Epoch [9/10], Step [181/390], Loss: 1.7263\n",
      "Epoch [9/10], Step [182/390], Loss: 1.7619\n",
      "Epoch [9/10], Step [183/390], Loss: 2.0510\n",
      "Epoch [9/10], Step [184/390], Loss: 1.7678\n",
      "Epoch [9/10], Step [185/390], Loss: 1.8606\n",
      "Epoch [9/10], Step [186/390], Loss: 2.0573\n",
      "Epoch [9/10], Step [187/390], Loss: 1.6726\n",
      "Epoch [9/10], Step [188/390], Loss: 2.0184\n",
      "Epoch [9/10], Step [189/390], Loss: 1.8583\n",
      "Epoch [9/10], Step [190/390], Loss: 2.1711\n",
      "Epoch [9/10], Step [191/390], Loss: 1.8665\n",
      "Epoch [9/10], Step [192/390], Loss: 1.7258\n",
      "Epoch [9/10], Step [193/390], Loss: 1.7292\n",
      "Epoch [9/10], Step [194/390], Loss: 1.8700\n",
      "Epoch [9/10], Step [195/390], Loss: 1.8389\n",
      "Epoch [9/10], Step [196/390], Loss: 2.0746\n",
      "Epoch [9/10], Step [197/390], Loss: 1.6217\n",
      "Epoch [9/10], Step [198/390], Loss: 1.6925\n",
      "Epoch [9/10], Step [199/390], Loss: 1.8694\n",
      "Epoch [9/10], Step [200/390], Loss: 1.8063\n",
      "Epoch [9/10], Step [201/390], Loss: 1.7760\n",
      "Epoch [9/10], Step [202/390], Loss: 1.8645\n",
      "Epoch [9/10], Step [203/390], Loss: 1.9523\n",
      "Epoch [9/10], Step [204/390], Loss: 1.8557\n",
      "Epoch [9/10], Step [205/390], Loss: 1.7765\n",
      "Epoch [9/10], Step [206/390], Loss: 1.8255\n",
      "Epoch [9/10], Step [207/390], Loss: 2.0657\n",
      "Epoch [9/10], Step [208/390], Loss: 1.7455\n",
      "Epoch [9/10], Step [209/390], Loss: 2.1401\n",
      "Epoch [9/10], Step [210/390], Loss: 1.8266\n",
      "Epoch [9/10], Step [211/390], Loss: 2.0520\n",
      "Epoch [9/10], Step [212/390], Loss: 2.0010\n",
      "Epoch [9/10], Step [213/390], Loss: 2.0972\n",
      "Epoch [9/10], Step [214/390], Loss: 1.8768\n",
      "Epoch [9/10], Step [215/390], Loss: 1.7043\n",
      "Epoch [9/10], Step [216/390], Loss: 1.6226\n",
      "Epoch [9/10], Step [217/390], Loss: 1.7664\n",
      "Epoch [9/10], Step [218/390], Loss: 1.9365\n",
      "Epoch [9/10], Step [219/390], Loss: 1.9683\n",
      "Epoch [9/10], Step [220/390], Loss: 1.9496\n",
      "Epoch [9/10], Step [221/390], Loss: 1.8686\n",
      "Epoch [9/10], Step [222/390], Loss: 1.7062\n",
      "Epoch [9/10], Step [223/390], Loss: 1.8462\n",
      "Epoch [9/10], Step [224/390], Loss: 1.9355\n",
      "Epoch [9/10], Step [225/390], Loss: 1.8810\n",
      "Epoch [9/10], Step [226/390], Loss: 1.7151\n",
      "Epoch [9/10], Step [227/390], Loss: 1.9506\n",
      "Epoch [9/10], Step [228/390], Loss: 1.7938\n",
      "Epoch [9/10], Step [229/390], Loss: 1.8389\n",
      "Epoch [9/10], Step [230/390], Loss: 1.7219\n",
      "Epoch [9/10], Step [231/390], Loss: 1.8731\n",
      "Epoch [9/10], Step [232/390], Loss: 1.8898\n",
      "Epoch [9/10], Step [233/390], Loss: 1.9138\n",
      "Epoch [9/10], Step [234/390], Loss: 1.6770\n",
      "Epoch [9/10], Step [235/390], Loss: 1.9230\n",
      "Epoch [9/10], Step [236/390], Loss: 1.8210\n",
      "Epoch [9/10], Step [237/390], Loss: 1.7917\n",
      "Epoch [9/10], Step [238/390], Loss: 1.9380\n",
      "Epoch [9/10], Step [239/390], Loss: 1.9399\n",
      "Epoch [9/10], Step [240/390], Loss: 1.8583\n",
      "Epoch [9/10], Step [241/390], Loss: 1.9037\n",
      "Epoch [9/10], Step [242/390], Loss: 1.9531\n",
      "Epoch [9/10], Step [243/390], Loss: 1.8418\n",
      "Epoch [9/10], Step [244/390], Loss: 1.6164\n",
      "Epoch [9/10], Step [245/390], Loss: 1.8160\n",
      "Epoch [9/10], Step [246/390], Loss: 1.9463\n",
      "Epoch [9/10], Step [247/390], Loss: 1.6728\n",
      "Epoch [9/10], Step [248/390], Loss: 1.9672\n",
      "Epoch [9/10], Step [249/390], Loss: 2.0071\n",
      "Epoch [9/10], Step [250/390], Loss: 1.8750\n",
      "Epoch [9/10], Step [251/390], Loss: 1.4728\n",
      "Epoch [9/10], Step [252/390], Loss: 1.5937\n",
      "Epoch [9/10], Step [253/390], Loss: 2.1402\n",
      "Epoch [9/10], Step [254/390], Loss: 1.8329\n",
      "Epoch [9/10], Step [255/390], Loss: 1.9203\n",
      "Epoch [9/10], Step [256/390], Loss: 2.0646\n",
      "Epoch [9/10], Step [257/390], Loss: 1.8209\n",
      "Epoch [9/10], Step [258/390], Loss: 2.0187\n",
      "Epoch [9/10], Step [259/390], Loss: 1.8280\n",
      "Epoch [9/10], Step [260/390], Loss: 2.1051\n",
      "Epoch [9/10], Step [261/390], Loss: 1.9430\n",
      "Epoch [9/10], Step [262/390], Loss: 2.0080\n",
      "Epoch [9/10], Step [263/390], Loss: 1.7559\n",
      "Epoch [9/10], Step [264/390], Loss: 1.8116\n",
      "Epoch [9/10], Step [265/390], Loss: 1.8105\n",
      "Epoch [9/10], Step [266/390], Loss: 1.7316\n",
      "Epoch [9/10], Step [267/390], Loss: 1.7952\n",
      "Epoch [9/10], Step [268/390], Loss: 2.0148\n",
      "Epoch [9/10], Step [269/390], Loss: 1.9846\n",
      "Epoch [9/10], Step [270/390], Loss: 1.9946\n",
      "Epoch [9/10], Step [271/390], Loss: 1.8137\n",
      "Epoch [9/10], Step [272/390], Loss: 1.5638\n",
      "Epoch [9/10], Step [273/390], Loss: 1.8994\n",
      "Epoch [9/10], Step [274/390], Loss: 1.9182\n",
      "Epoch [9/10], Step [275/390], Loss: 1.8340\n",
      "Epoch [9/10], Step [276/390], Loss: 2.0155\n",
      "Epoch [9/10], Step [277/390], Loss: 1.8239\n",
      "Epoch [9/10], Step [278/390], Loss: 1.7624\n",
      "Epoch [9/10], Step [279/390], Loss: 2.0132\n",
      "Epoch [9/10], Step [280/390], Loss: 2.0798\n",
      "Epoch [9/10], Step [281/390], Loss: 2.1748\n",
      "Epoch [9/10], Step [282/390], Loss: 1.6956\n",
      "Epoch [9/10], Step [283/390], Loss: 1.7278\n",
      "Epoch [9/10], Step [284/390], Loss: 1.9353\n",
      "Epoch [9/10], Step [285/390], Loss: 1.9718\n",
      "Epoch [9/10], Step [286/390], Loss: 1.8899\n",
      "Epoch [9/10], Step [287/390], Loss: 1.8504\n",
      "Epoch [9/10], Step [288/390], Loss: 2.1040\n",
      "Epoch [9/10], Step [289/390], Loss: 2.0489\n",
      "Epoch [9/10], Step [290/390], Loss: 1.7520\n",
      "Epoch [9/10], Step [291/390], Loss: 1.9766\n",
      "Epoch [9/10], Step [292/390], Loss: 1.8038\n",
      "Epoch [9/10], Step [293/390], Loss: 1.8319\n",
      "Epoch [9/10], Step [294/390], Loss: 1.8018\n",
      "Epoch [9/10], Step [295/390], Loss: 1.9451\n",
      "Epoch [9/10], Step [296/390], Loss: 1.9310\n",
      "Epoch [9/10], Step [297/390], Loss: 1.9030\n",
      "Epoch [9/10], Step [298/390], Loss: 2.0695\n",
      "Epoch [9/10], Step [299/390], Loss: 2.2527\n",
      "Epoch [9/10], Step [300/390], Loss: 1.9737\n",
      "Epoch [9/10], Step [301/390], Loss: 1.9600\n",
      "Epoch [9/10], Step [302/390], Loss: 1.6948\n",
      "Epoch [9/10], Step [303/390], Loss: 1.7104\n",
      "Epoch [9/10], Step [304/390], Loss: 1.9984\n",
      "Epoch [9/10], Step [305/390], Loss: 1.7360\n",
      "Epoch [9/10], Step [306/390], Loss: 1.6205\n",
      "Epoch [9/10], Step [307/390], Loss: 1.7735\n",
      "Epoch [9/10], Step [308/390], Loss: 2.1924\n",
      "Epoch [9/10], Step [309/390], Loss: 1.6929\n",
      "Epoch [9/10], Step [310/390], Loss: 1.5477\n",
      "Epoch [9/10], Step [311/390], Loss: 1.8124\n",
      "Epoch [9/10], Step [312/390], Loss: 1.5915\n",
      "Epoch [9/10], Step [313/390], Loss: 1.8635\n",
      "Epoch [9/10], Step [314/390], Loss: 1.8157\n",
      "Epoch [9/10], Step [315/390], Loss: 2.1300\n",
      "Epoch [9/10], Step [316/390], Loss: 1.9282\n",
      "Epoch [9/10], Step [317/390], Loss: 1.8803\n",
      "Epoch [9/10], Step [318/390], Loss: 1.9133\n",
      "Epoch [9/10], Step [319/390], Loss: 1.7649\n",
      "Epoch [9/10], Step [320/390], Loss: 1.6223\n",
      "Epoch [9/10], Step [321/390], Loss: 1.9491\n",
      "Epoch [9/10], Step [322/390], Loss: 1.9891\n",
      "Epoch [9/10], Step [323/390], Loss: 1.6909\n",
      "Epoch [9/10], Step [324/390], Loss: 2.1182\n",
      "Epoch [9/10], Step [325/390], Loss: 1.5628\n",
      "Epoch [9/10], Step [326/390], Loss: 1.9169\n",
      "Epoch [9/10], Step [327/390], Loss: 1.8432\n",
      "Epoch [9/10], Step [328/390], Loss: 1.9940\n",
      "Epoch [9/10], Step [329/390], Loss: 1.8636\n",
      "Epoch [9/10], Step [330/390], Loss: 1.8185\n",
      "Epoch [9/10], Step [331/390], Loss: 1.8144\n",
      "Epoch [9/10], Step [332/390], Loss: 1.9817\n",
      "Epoch [9/10], Step [333/390], Loss: 2.0833\n",
      "Epoch [9/10], Step [334/390], Loss: 1.9868\n",
      "Epoch [9/10], Step [335/390], Loss: 1.8515\n",
      "Epoch [9/10], Step [336/390], Loss: 2.0431\n",
      "Epoch [9/10], Step [337/390], Loss: 1.9301\n",
      "Epoch [9/10], Step [338/390], Loss: 1.8036\n",
      "Epoch [9/10], Step [339/390], Loss: 1.7294\n",
      "Epoch [9/10], Step [340/390], Loss: 2.2078\n",
      "Epoch [9/10], Step [341/390], Loss: 1.8684\n",
      "Epoch [9/10], Step [342/390], Loss: 1.8447\n",
      "Epoch [9/10], Step [343/390], Loss: 2.0170\n",
      "Epoch [9/10], Step [344/390], Loss: 2.0941\n",
      "Epoch [9/10], Step [345/390], Loss: 1.7034\n",
      "Epoch [9/10], Step [346/390], Loss: 2.1379\n",
      "Epoch [9/10], Step [347/390], Loss: 1.8860\n",
      "Epoch [9/10], Step [348/390], Loss: 1.7211\n",
      "Epoch [9/10], Step [349/390], Loss: 2.1132\n",
      "Epoch [9/10], Step [350/390], Loss: 1.9507\n",
      "Epoch [9/10], Step [351/390], Loss: 1.9367\n",
      "Epoch [9/10], Step [352/390], Loss: 1.7674\n",
      "Epoch [9/10], Step [353/390], Loss: 1.6931\n",
      "Epoch [9/10], Step [354/390], Loss: 1.7737\n",
      "Epoch [9/10], Step [355/390], Loss: 1.7101\n",
      "Epoch [9/10], Step [356/390], Loss: 2.0441\n",
      "Epoch [9/10], Step [357/390], Loss: 1.8287\n",
      "Epoch [9/10], Step [358/390], Loss: 1.9895\n",
      "Epoch [9/10], Step [359/390], Loss: 1.7689\n",
      "Epoch [9/10], Step [360/390], Loss: 1.7747\n",
      "Epoch [9/10], Step [361/390], Loss: 2.1069\n",
      "Epoch [9/10], Step [362/390], Loss: 1.9574\n",
      "Epoch [9/10], Step [363/390], Loss: 1.9168\n",
      "Epoch [9/10], Step [364/390], Loss: 1.9907\n",
      "Epoch [9/10], Step [365/390], Loss: 1.7075\n",
      "Epoch [9/10], Step [366/390], Loss: 1.6169\n",
      "Epoch [9/10], Step [367/390], Loss: 1.8897\n",
      "Epoch [9/10], Step [368/390], Loss: 1.9457\n",
      "Epoch [9/10], Step [369/390], Loss: 1.7982\n",
      "Epoch [9/10], Step [370/390], Loss: 1.8342\n",
      "Epoch [9/10], Step [371/390], Loss: 1.6416\n",
      "Epoch [9/10], Step [372/390], Loss: 2.0662\n",
      "Epoch [9/10], Step [373/390], Loss: 1.8769\n",
      "Epoch [9/10], Step [374/390], Loss: 2.0680\n",
      "Epoch [9/10], Step [375/390], Loss: 1.9051\n",
      "Epoch [9/10], Step [376/390], Loss: 1.9004\n",
      "Epoch [9/10], Step [377/390], Loss: 1.9953\n",
      "Epoch [9/10], Step [378/390], Loss: 1.9289\n",
      "Epoch [9/10], Step [379/390], Loss: 1.8697\n",
      "Epoch [9/10], Step [380/390], Loss: 1.9244\n",
      "Epoch [9/10], Step [381/390], Loss: 1.8762\n",
      "Epoch [9/10], Step [382/390], Loss: 1.6975\n",
      "Epoch [9/10], Step [383/390], Loss: 1.8201\n",
      "Epoch [9/10], Step [384/390], Loss: 1.8757\n",
      "Epoch [9/10], Step [385/390], Loss: 1.7940\n",
      "Epoch [9/10], Step [386/390], Loss: 1.9445\n",
      "Epoch [9/10], Step [387/390], Loss: 1.8361\n",
      "Epoch [9/10], Step [388/390], Loss: 1.7135\n",
      "Epoch [9/10], Step [389/390], Loss: 1.8149\n",
      "Epoch [9/10], Step [390/390], Loss: 1.6698\n",
      "Epoch [9/10], Step [391/390], Loss: 1.8906\n",
      "Epoch [10/10], Step [1/390], Loss: 1.5334\n",
      "Epoch [10/10], Step [2/390], Loss: 1.5450\n",
      "Epoch [10/10], Step [3/390], Loss: 1.7640\n",
      "Epoch [10/10], Step [4/390], Loss: 1.8137\n",
      "Epoch [10/10], Step [5/390], Loss: 1.4452\n",
      "Epoch [10/10], Step [6/390], Loss: 1.8490\n",
      "Epoch [10/10], Step [7/390], Loss: 1.8023\n",
      "Epoch [10/10], Step [8/390], Loss: 1.8791\n",
      "Epoch [10/10], Step [9/390], Loss: 1.9317\n",
      "Epoch [10/10], Step [10/390], Loss: 1.5052\n",
      "Epoch [10/10], Step [11/390], Loss: 1.6175\n",
      "Epoch [10/10], Step [12/390], Loss: 1.6746\n",
      "Epoch [10/10], Step [13/390], Loss: 1.8629\n",
      "Epoch [10/10], Step [14/390], Loss: 1.5820\n",
      "Epoch [10/10], Step [15/390], Loss: 1.4607\n",
      "Epoch [10/10], Step [16/390], Loss: 1.4191\n",
      "Epoch [10/10], Step [17/390], Loss: 1.5239\n",
      "Epoch [10/10], Step [18/390], Loss: 1.5974\n",
      "Epoch [10/10], Step [19/390], Loss: 1.7618\n",
      "Epoch [10/10], Step [20/390], Loss: 1.6559\n",
      "Epoch [10/10], Step [21/390], Loss: 1.4865\n",
      "Epoch [10/10], Step [22/390], Loss: 1.6168\n",
      "Epoch [10/10], Step [23/390], Loss: 1.6426\n",
      "Epoch [10/10], Step [24/390], Loss: 1.8359\n",
      "Epoch [10/10], Step [25/390], Loss: 1.5128\n",
      "Epoch [10/10], Step [26/390], Loss: 1.8049\n",
      "Epoch [10/10], Step [27/390], Loss: 1.6149\n",
      "Epoch [10/10], Step [28/390], Loss: 1.7096\n",
      "Epoch [10/10], Step [29/390], Loss: 1.7510\n",
      "Epoch [10/10], Step [30/390], Loss: 1.6679\n",
      "Epoch [10/10], Step [31/390], Loss: 1.7227\n",
      "Epoch [10/10], Step [32/390], Loss: 1.7654\n",
      "Epoch [10/10], Step [33/390], Loss: 2.1262\n",
      "Epoch [10/10], Step [34/390], Loss: 1.6472\n",
      "Epoch [10/10], Step [35/390], Loss: 1.4337\n",
      "Epoch [10/10], Step [36/390], Loss: 1.6681\n",
      "Epoch [10/10], Step [37/390], Loss: 1.5474\n",
      "Epoch [10/10], Step [38/390], Loss: 1.8297\n",
      "Epoch [10/10], Step [39/390], Loss: 1.8454\n",
      "Epoch [10/10], Step [40/390], Loss: 1.6788\n",
      "Epoch [10/10], Step [41/390], Loss: 1.9076\n",
      "Epoch [10/10], Step [42/390], Loss: 1.7433\n",
      "Epoch [10/10], Step [43/390], Loss: 1.7314\n",
      "Epoch [10/10], Step [44/390], Loss: 1.5238\n",
      "Epoch [10/10], Step [45/390], Loss: 1.8827\n",
      "Epoch [10/10], Step [46/390], Loss: 1.6733\n",
      "Epoch [10/10], Step [47/390], Loss: 1.5920\n",
      "Epoch [10/10], Step [48/390], Loss: 1.7892\n",
      "Epoch [10/10], Step [49/390], Loss: 1.6554\n",
      "Epoch [10/10], Step [50/390], Loss: 1.9365\n",
      "Epoch [10/10], Step [51/390], Loss: 1.7018\n",
      "Epoch [10/10], Step [52/390], Loss: 1.7323\n",
      "Epoch [10/10], Step [53/390], Loss: 1.7226\n",
      "Epoch [10/10], Step [54/390], Loss: 1.7778\n",
      "Epoch [10/10], Step [55/390], Loss: 1.5882\n",
      "Epoch [10/10], Step [56/390], Loss: 1.9880\n",
      "Epoch [10/10], Step [57/390], Loss: 2.0902\n",
      "Epoch [10/10], Step [58/390], Loss: 1.5269\n",
      "Epoch [10/10], Step [59/390], Loss: 1.8303\n",
      "Epoch [10/10], Step [60/390], Loss: 1.5102\n",
      "Epoch [10/10], Step [61/390], Loss: 1.3659\n",
      "Epoch [10/10], Step [62/390], Loss: 1.8258\n",
      "Epoch [10/10], Step [63/390], Loss: 1.4400\n",
      "Epoch [10/10], Step [64/390], Loss: 1.6152\n",
      "Epoch [10/10], Step [65/390], Loss: 1.6500\n",
      "Epoch [10/10], Step [66/390], Loss: 1.6129\n",
      "Epoch [10/10], Step [67/390], Loss: 1.7137\n",
      "Epoch [10/10], Step [68/390], Loss: 1.8505\n",
      "Epoch [10/10], Step [69/390], Loss: 1.8037\n",
      "Epoch [10/10], Step [70/390], Loss: 1.5641\n",
      "Epoch [10/10], Step [71/390], Loss: 1.5810\n",
      "Epoch [10/10], Step [72/390], Loss: 1.6913\n",
      "Epoch [10/10], Step [73/390], Loss: 1.8350\n",
      "Epoch [10/10], Step [74/390], Loss: 1.4267\n",
      "Epoch [10/10], Step [75/390], Loss: 1.7570\n",
      "Epoch [10/10], Step [76/390], Loss: 2.0699\n",
      "Epoch [10/10], Step [77/390], Loss: 1.6518\n",
      "Epoch [10/10], Step [78/390], Loss: 2.0899\n",
      "Epoch [10/10], Step [79/390], Loss: 1.5960\n",
      "Epoch [10/10], Step [80/390], Loss: 1.9840\n",
      "Epoch [10/10], Step [81/390], Loss: 1.9304\n",
      "Epoch [10/10], Step [82/390], Loss: 1.7610\n",
      "Epoch [10/10], Step [83/390], Loss: 1.5671\n",
      "Epoch [10/10], Step [84/390], Loss: 1.7730\n",
      "Epoch [10/10], Step [85/390], Loss: 1.6870\n",
      "Epoch [10/10], Step [86/390], Loss: 1.8475\n",
      "Epoch [10/10], Step [87/390], Loss: 1.7407\n",
      "Epoch [10/10], Step [88/390], Loss: 1.4706\n",
      "Epoch [10/10], Step [89/390], Loss: 1.7331\n",
      "Epoch [10/10], Step [90/390], Loss: 1.8207\n",
      "Epoch [10/10], Step [91/390], Loss: 1.6810\n",
      "Epoch [10/10], Step [92/390], Loss: 1.6222\n",
      "Epoch [10/10], Step [93/390], Loss: 1.9889\n",
      "Epoch [10/10], Step [94/390], Loss: 1.6558\n",
      "Epoch [10/10], Step [95/390], Loss: 1.6957\n",
      "Epoch [10/10], Step [96/390], Loss: 1.7088\n",
      "Epoch [10/10], Step [97/390], Loss: 1.6486\n",
      "Epoch [10/10], Step [98/390], Loss: 1.8893\n",
      "Epoch [10/10], Step [99/390], Loss: 1.9218\n",
      "Epoch [10/10], Step [100/390], Loss: 1.7650\n",
      "Epoch [10/10], Step [101/390], Loss: 1.8178\n",
      "Epoch [10/10], Step [102/390], Loss: 1.6806\n",
      "Epoch [10/10], Step [103/390], Loss: 1.9036\n",
      "Epoch [10/10], Step [104/390], Loss: 1.7955\n",
      "Epoch [10/10], Step [105/390], Loss: 1.8094\n",
      "Epoch [10/10], Step [106/390], Loss: 1.6313\n",
      "Epoch [10/10], Step [107/390], Loss: 1.4331\n",
      "Epoch [10/10], Step [108/390], Loss: 1.6856\n",
      "Epoch [10/10], Step [109/390], Loss: 1.7206\n",
      "Epoch [10/10], Step [110/390], Loss: 1.6738\n",
      "Epoch [10/10], Step [111/390], Loss: 1.4246\n",
      "Epoch [10/10], Step [112/390], Loss: 1.8723\n",
      "Epoch [10/10], Step [113/390], Loss: 1.6680\n",
      "Epoch [10/10], Step [114/390], Loss: 1.6215\n",
      "Epoch [10/10], Step [115/390], Loss: 1.6111\n",
      "Epoch [10/10], Step [116/390], Loss: 1.7483\n",
      "Epoch [10/10], Step [117/390], Loss: 1.7840\n",
      "Epoch [10/10], Step [118/390], Loss: 1.6861\n",
      "Epoch [10/10], Step [119/390], Loss: 1.6041\n",
      "Epoch [10/10], Step [120/390], Loss: 1.7457\n",
      "Epoch [10/10], Step [121/390], Loss: 1.7315\n",
      "Epoch [10/10], Step [122/390], Loss: 1.5780\n",
      "Epoch [10/10], Step [123/390], Loss: 1.4867\n",
      "Epoch [10/10], Step [124/390], Loss: 1.9583\n",
      "Epoch [10/10], Step [125/390], Loss: 1.5629\n",
      "Epoch [10/10], Step [126/390], Loss: 1.7966\n",
      "Epoch [10/10], Step [127/390], Loss: 1.8869\n",
      "Epoch [10/10], Step [128/390], Loss: 1.5137\n",
      "Epoch [10/10], Step [129/390], Loss: 1.6524\n",
      "Epoch [10/10], Step [130/390], Loss: 1.8536\n",
      "Epoch [10/10], Step [131/390], Loss: 1.8752\n",
      "Epoch [10/10], Step [132/390], Loss: 1.4706\n",
      "Epoch [10/10], Step [133/390], Loss: 1.3370\n",
      "Epoch [10/10], Step [134/390], Loss: 1.7473\n",
      "Epoch [10/10], Step [135/390], Loss: 1.5239\n",
      "Epoch [10/10], Step [136/390], Loss: 1.7302\n",
      "Epoch [10/10], Step [137/390], Loss: 1.6685\n",
      "Epoch [10/10], Step [138/390], Loss: 1.7801\n",
      "Epoch [10/10], Step [139/390], Loss: 1.8506\n",
      "Epoch [10/10], Step [140/390], Loss: 1.8152\n",
      "Epoch [10/10], Step [141/390], Loss: 1.4488\n",
      "Epoch [10/10], Step [142/390], Loss: 1.5214\n",
      "Epoch [10/10], Step [143/390], Loss: 1.7971\n",
      "Epoch [10/10], Step [144/390], Loss: 1.7151\n",
      "Epoch [10/10], Step [145/390], Loss: 1.5657\n",
      "Epoch [10/10], Step [146/390], Loss: 1.6702\n",
      "Epoch [10/10], Step [147/390], Loss: 1.4052\n",
      "Epoch [10/10], Step [148/390], Loss: 1.7123\n",
      "Epoch [10/10], Step [149/390], Loss: 2.0406\n",
      "Epoch [10/10], Step [150/390], Loss: 1.7497\n",
      "Epoch [10/10], Step [151/390], Loss: 1.7766\n",
      "Epoch [10/10], Step [152/390], Loss: 1.6895\n",
      "Epoch [10/10], Step [153/390], Loss: 1.5627\n",
      "Epoch [10/10], Step [154/390], Loss: 1.5778\n",
      "Epoch [10/10], Step [155/390], Loss: 1.8306\n",
      "Epoch [10/10], Step [156/390], Loss: 1.5733\n",
      "Epoch [10/10], Step [157/390], Loss: 1.9104\n",
      "Epoch [10/10], Step [158/390], Loss: 1.7372\n",
      "Epoch [10/10], Step [159/390], Loss: 1.6278\n",
      "Epoch [10/10], Step [160/390], Loss: 1.5306\n",
      "Epoch [10/10], Step [161/390], Loss: 1.7879\n",
      "Epoch [10/10], Step [162/390], Loss: 1.8613\n",
      "Epoch [10/10], Step [163/390], Loss: 1.5236\n",
      "Epoch [10/10], Step [164/390], Loss: 1.5700\n",
      "Epoch [10/10], Step [165/390], Loss: 1.6162\n",
      "Epoch [10/10], Step [166/390], Loss: 1.8904\n",
      "Epoch [10/10], Step [167/390], Loss: 1.7172\n",
      "Epoch [10/10], Step [168/390], Loss: 1.5773\n",
      "Epoch [10/10], Step [169/390], Loss: 1.4954\n",
      "Epoch [10/10], Step [170/390], Loss: 1.7308\n",
      "Epoch [10/10], Step [171/390], Loss: 1.5735\n",
      "Epoch [10/10], Step [172/390], Loss: 1.7399\n",
      "Epoch [10/10], Step [173/390], Loss: 1.7670\n",
      "Epoch [10/10], Step [174/390], Loss: 1.6000\n",
      "Epoch [10/10], Step [175/390], Loss: 1.7710\n",
      "Epoch [10/10], Step [176/390], Loss: 1.7573\n",
      "Epoch [10/10], Step [177/390], Loss: 1.6930\n",
      "Epoch [10/10], Step [178/390], Loss: 1.9484\n",
      "Epoch [10/10], Step [179/390], Loss: 1.7384\n",
      "Epoch [10/10], Step [180/390], Loss: 1.8727\n",
      "Epoch [10/10], Step [181/390], Loss: 2.0197\n",
      "Epoch [10/10], Step [182/390], Loss: 1.5270\n",
      "Epoch [10/10], Step [183/390], Loss: 1.5864\n",
      "Epoch [10/10], Step [184/390], Loss: 1.5476\n",
      "Epoch [10/10], Step [185/390], Loss: 1.9334\n",
      "Epoch [10/10], Step [186/390], Loss: 1.7352\n",
      "Epoch [10/10], Step [187/390], Loss: 1.7664\n",
      "Epoch [10/10], Step [188/390], Loss: 1.7224\n",
      "Epoch [10/10], Step [189/390], Loss: 1.9229\n",
      "Epoch [10/10], Step [190/390], Loss: 1.5958\n",
      "Epoch [10/10], Step [191/390], Loss: 1.5596\n",
      "Epoch [10/10], Step [192/390], Loss: 1.8498\n",
      "Epoch [10/10], Step [193/390], Loss: 1.8079\n",
      "Epoch [10/10], Step [194/390], Loss: 1.8421\n",
      "Epoch [10/10], Step [195/390], Loss: 1.7000\n",
      "Epoch [10/10], Step [196/390], Loss: 1.7405\n",
      "Epoch [10/10], Step [197/390], Loss: 1.8300\n",
      "Epoch [10/10], Step [198/390], Loss: 1.7378\n",
      "Epoch [10/10], Step [199/390], Loss: 1.9210\n",
      "Epoch [10/10], Step [200/390], Loss: 1.8472\n",
      "Epoch [10/10], Step [201/390], Loss: 1.6335\n",
      "Epoch [10/10], Step [202/390], Loss: 1.5950\n",
      "Epoch [10/10], Step [203/390], Loss: 1.6397\n",
      "Epoch [10/10], Step [204/390], Loss: 1.7666\n",
      "Epoch [10/10], Step [205/390], Loss: 1.7436\n",
      "Epoch [10/10], Step [206/390], Loss: 1.7169\n",
      "Epoch [10/10], Step [207/390], Loss: 1.8411\n",
      "Epoch [10/10], Step [208/390], Loss: 1.6925\n",
      "Epoch [10/10], Step [209/390], Loss: 1.4239\n",
      "Epoch [10/10], Step [210/390], Loss: 1.8243\n",
      "Epoch [10/10], Step [211/390], Loss: 2.0716\n",
      "Epoch [10/10], Step [212/390], Loss: 1.6180\n",
      "Epoch [10/10], Step [213/390], Loss: 1.5370\n",
      "Epoch [10/10], Step [214/390], Loss: 1.8878\n",
      "Epoch [10/10], Step [215/390], Loss: 1.5517\n",
      "Epoch [10/10], Step [216/390], Loss: 2.1502\n",
      "Epoch [10/10], Step [217/390], Loss: 1.8063\n",
      "Epoch [10/10], Step [218/390], Loss: 1.9373\n",
      "Epoch [10/10], Step [219/390], Loss: 1.6488\n",
      "Epoch [10/10], Step [220/390], Loss: 1.5531\n",
      "Epoch [10/10], Step [221/390], Loss: 1.5998\n",
      "Epoch [10/10], Step [222/390], Loss: 1.5989\n",
      "Epoch [10/10], Step [223/390], Loss: 1.6719\n",
      "Epoch [10/10], Step [224/390], Loss: 1.6649\n",
      "Epoch [10/10], Step [225/390], Loss: 1.5996\n",
      "Epoch [10/10], Step [226/390], Loss: 2.2414\n",
      "Epoch [10/10], Step [227/390], Loss: 1.7235\n",
      "Epoch [10/10], Step [228/390], Loss: 1.7551\n",
      "Epoch [10/10], Step [229/390], Loss: 1.7032\n",
      "Epoch [10/10], Step [230/390], Loss: 1.8070\n",
      "Epoch [10/10], Step [231/390], Loss: 1.7456\n",
      "Epoch [10/10], Step [232/390], Loss: 1.8181\n",
      "Epoch [10/10], Step [233/390], Loss: 1.6786\n",
      "Epoch [10/10], Step [234/390], Loss: 1.5730\n",
      "Epoch [10/10], Step [235/390], Loss: 1.7989\n",
      "Epoch [10/10], Step [236/390], Loss: 1.6064\n",
      "Epoch [10/10], Step [237/390], Loss: 1.6389\n",
      "Epoch [10/10], Step [238/390], Loss: 1.5214\n",
      "Epoch [10/10], Step [239/390], Loss: 1.7216\n",
      "Epoch [10/10], Step [240/390], Loss: 1.7952\n",
      "Epoch [10/10], Step [241/390], Loss: 1.6260\n",
      "Epoch [10/10], Step [242/390], Loss: 1.6539\n",
      "Epoch [10/10], Step [243/390], Loss: 1.6780\n",
      "Epoch [10/10], Step [244/390], Loss: 1.9669\n",
      "Epoch [10/10], Step [245/390], Loss: 1.8516\n",
      "Epoch [10/10], Step [246/390], Loss: 1.7625\n",
      "Epoch [10/10], Step [247/390], Loss: 1.9037\n",
      "Epoch [10/10], Step [248/390], Loss: 1.8986\n",
      "Epoch [10/10], Step [249/390], Loss: 1.7329\n",
      "Epoch [10/10], Step [250/390], Loss: 1.8154\n",
      "Epoch [10/10], Step [251/390], Loss: 1.6678\n",
      "Epoch [10/10], Step [252/390], Loss: 1.8261\n",
      "Epoch [10/10], Step [253/390], Loss: 1.7021\n",
      "Epoch [10/10], Step [254/390], Loss: 1.6651\n",
      "Epoch [10/10], Step [255/390], Loss: 1.7190\n",
      "Epoch [10/10], Step [256/390], Loss: 1.6227\n",
      "Epoch [10/10], Step [257/390], Loss: 1.6123\n",
      "Epoch [10/10], Step [258/390], Loss: 1.6797\n",
      "Epoch [10/10], Step [259/390], Loss: 1.9036\n",
      "Epoch [10/10], Step [260/390], Loss: 1.6038\n",
      "Epoch [10/10], Step [261/390], Loss: 1.6038\n",
      "Epoch [10/10], Step [262/390], Loss: 1.6371\n",
      "Epoch [10/10], Step [263/390], Loss: 1.7513\n",
      "Epoch [10/10], Step [264/390], Loss: 1.6898\n",
      "Epoch [10/10], Step [265/390], Loss: 1.8039\n",
      "Epoch [10/10], Step [266/390], Loss: 1.7421\n",
      "Epoch [10/10], Step [267/390], Loss: 1.8345\n",
      "Epoch [10/10], Step [268/390], Loss: 1.7364\n",
      "Epoch [10/10], Step [269/390], Loss: 1.8808\n",
      "Epoch [10/10], Step [270/390], Loss: 1.5783\n",
      "Epoch [10/10], Step [271/390], Loss: 1.6245\n",
      "Epoch [10/10], Step [272/390], Loss: 1.5240\n",
      "Epoch [10/10], Step [273/390], Loss: 1.3993\n",
      "Epoch [10/10], Step [274/390], Loss: 1.9795\n",
      "Epoch [10/10], Step [275/390], Loss: 1.7877\n",
      "Epoch [10/10], Step [276/390], Loss: 2.0104\n",
      "Epoch [10/10], Step [277/390], Loss: 1.9560\n",
      "Epoch [10/10], Step [278/390], Loss: 1.7068\n",
      "Epoch [10/10], Step [279/390], Loss: 1.7662\n",
      "Epoch [10/10], Step [280/390], Loss: 1.8315\n",
      "Epoch [10/10], Step [281/390], Loss: 2.0307\n",
      "Epoch [10/10], Step [282/390], Loss: 1.5982\n",
      "Epoch [10/10], Step [283/390], Loss: 1.7211\n",
      "Epoch [10/10], Step [284/390], Loss: 1.8906\n",
      "Epoch [10/10], Step [285/390], Loss: 1.5813\n",
      "Epoch [10/10], Step [286/390], Loss: 1.5730\n",
      "Epoch [10/10], Step [287/390], Loss: 1.7361\n",
      "Epoch [10/10], Step [288/390], Loss: 1.8568\n",
      "Epoch [10/10], Step [289/390], Loss: 1.7789\n",
      "Epoch [10/10], Step [290/390], Loss: 1.5543\n",
      "Epoch [10/10], Step [291/390], Loss: 2.0972\n",
      "Epoch [10/10], Step [292/390], Loss: 1.9958\n",
      "Epoch [10/10], Step [293/390], Loss: 2.1437\n",
      "Epoch [10/10], Step [294/390], Loss: 1.8493\n",
      "Epoch [10/10], Step [295/390], Loss: 1.6537\n",
      "Epoch [10/10], Step [296/390], Loss: 1.5724\n",
      "Epoch [10/10], Step [297/390], Loss: 1.7866\n",
      "Epoch [10/10], Step [298/390], Loss: 1.8920\n",
      "Epoch [10/10], Step [299/390], Loss: 1.8210\n",
      "Epoch [10/10], Step [300/390], Loss: 1.6818\n",
      "Epoch [10/10], Step [301/390], Loss: 1.7762\n",
      "Epoch [10/10], Step [302/390], Loss: 1.8280\n",
      "Epoch [10/10], Step [303/390], Loss: 1.7369\n",
      "Epoch [10/10], Step [304/390], Loss: 1.7650\n",
      "Epoch [10/10], Step [305/390], Loss: 1.9779\n",
      "Epoch [10/10], Step [306/390], Loss: 1.6486\n",
      "Epoch [10/10], Step [307/390], Loss: 2.1217\n",
      "Epoch [10/10], Step [308/390], Loss: 1.5309\n",
      "Epoch [10/10], Step [309/390], Loss: 1.7998\n",
      "Epoch [10/10], Step [310/390], Loss: 1.5799\n",
      "Epoch [10/10], Step [311/390], Loss: 1.6228\n",
      "Epoch [10/10], Step [312/390], Loss: 2.0419\n",
      "Epoch [10/10], Step [313/390], Loss: 1.6873\n",
      "Epoch [10/10], Step [314/390], Loss: 1.6386\n",
      "Epoch [10/10], Step [315/390], Loss: 1.7901\n",
      "Epoch [10/10], Step [316/390], Loss: 1.7665\n",
      "Epoch [10/10], Step [317/390], Loss: 1.6714\n",
      "Epoch [10/10], Step [318/390], Loss: 1.8086\n",
      "Epoch [10/10], Step [319/390], Loss: 1.9028\n",
      "Epoch [10/10], Step [320/390], Loss: 1.7455\n",
      "Epoch [10/10], Step [321/390], Loss: 1.8003\n",
      "Epoch [10/10], Step [322/390], Loss: 1.6668\n",
      "Epoch [10/10], Step [323/390], Loss: 1.8931\n",
      "Epoch [10/10], Step [324/390], Loss: 1.5384\n",
      "Epoch [10/10], Step [325/390], Loss: 1.8421\n",
      "Epoch [10/10], Step [326/390], Loss: 2.0280\n",
      "Epoch [10/10], Step [327/390], Loss: 1.7708\n",
      "Epoch [10/10], Step [328/390], Loss: 1.6861\n",
      "Epoch [10/10], Step [329/390], Loss: 1.7656\n",
      "Epoch [10/10], Step [330/390], Loss: 1.4609\n",
      "Epoch [10/10], Step [331/390], Loss: 1.7748\n",
      "Epoch [10/10], Step [332/390], Loss: 1.7855\n",
      "Epoch [10/10], Step [333/390], Loss: 1.8547\n",
      "Epoch [10/10], Step [334/390], Loss: 1.8373\n",
      "Epoch [10/10], Step [335/390], Loss: 1.7927\n",
      "Epoch [10/10], Step [336/390], Loss: 1.6863\n",
      "Epoch [10/10], Step [337/390], Loss: 1.7005\n",
      "Epoch [10/10], Step [338/390], Loss: 1.5451\n",
      "Epoch [10/10], Step [339/390], Loss: 1.9799\n",
      "Epoch [10/10], Step [340/390], Loss: 1.8838\n",
      "Epoch [10/10], Step [341/390], Loss: 1.6211\n",
      "Epoch [10/10], Step [342/390], Loss: 1.7161\n",
      "Epoch [10/10], Step [343/390], Loss: 1.7988\n",
      "Epoch [10/10], Step [344/390], Loss: 1.5388\n",
      "Epoch [10/10], Step [345/390], Loss: 1.5835\n",
      "Epoch [10/10], Step [346/390], Loss: 1.7310\n",
      "Epoch [10/10], Step [347/390], Loss: 2.0602\n",
      "Epoch [10/10], Step [348/390], Loss: 1.6796\n",
      "Epoch [10/10], Step [349/390], Loss: 1.7888\n",
      "Epoch [10/10], Step [350/390], Loss: 1.8932\n",
      "Epoch [10/10], Step [351/390], Loss: 1.8632\n",
      "Epoch [10/10], Step [352/390], Loss: 2.1290\n",
      "Epoch [10/10], Step [353/390], Loss: 1.9347\n",
      "Epoch [10/10], Step [354/390], Loss: 1.7520\n",
      "Epoch [10/10], Step [355/390], Loss: 1.9450\n",
      "Epoch [10/10], Step [356/390], Loss: 1.8997\n",
      "Epoch [10/10], Step [357/390], Loss: 1.8401\n",
      "Epoch [10/10], Step [358/390], Loss: 1.7846\n",
      "Epoch [10/10], Step [359/390], Loss: 1.8415\n",
      "Epoch [10/10], Step [360/390], Loss: 1.7096\n",
      "Epoch [10/10], Step [361/390], Loss: 1.8743\n",
      "Epoch [10/10], Step [362/390], Loss: 1.7824\n",
      "Epoch [10/10], Step [363/390], Loss: 1.9499\n",
      "Epoch [10/10], Step [364/390], Loss: 1.7336\n",
      "Epoch [10/10], Step [365/390], Loss: 1.6361\n",
      "Epoch [10/10], Step [366/390], Loss: 1.6430\n",
      "Epoch [10/10], Step [367/390], Loss: 1.8652\n",
      "Epoch [10/10], Step [368/390], Loss: 1.9414\n",
      "Epoch [10/10], Step [369/390], Loss: 1.8915\n",
      "Epoch [10/10], Step [370/390], Loss: 1.4339\n",
      "Epoch [10/10], Step [371/390], Loss: 1.8427\n",
      "Epoch [10/10], Step [372/390], Loss: 1.7674\n",
      "Epoch [10/10], Step [373/390], Loss: 1.8109\n",
      "Epoch [10/10], Step [374/390], Loss: 1.8520\n",
      "Epoch [10/10], Step [375/390], Loss: 1.9091\n",
      "Epoch [10/10], Step [376/390], Loss: 1.6336\n",
      "Epoch [10/10], Step [377/390], Loss: 1.7148\n",
      "Epoch [10/10], Step [378/390], Loss: 1.7805\n",
      "Epoch [10/10], Step [379/390], Loss: 1.8230\n",
      "Epoch [10/10], Step [380/390], Loss: 1.8110\n",
      "Epoch [10/10], Step [381/390], Loss: 1.8911\n",
      "Epoch [10/10], Step [382/390], Loss: 1.8230\n",
      "Epoch [10/10], Step [383/390], Loss: 1.6642\n",
      "Epoch [10/10], Step [384/390], Loss: 1.6524\n",
      "Epoch [10/10], Step [385/390], Loss: 1.8555\n",
      "Epoch [10/10], Step [386/390], Loss: 1.9224\n",
      "Epoch [10/10], Step [387/390], Loss: 1.8673\n",
      "Epoch [10/10], Step [388/390], Loss: 1.7935\n",
      "Epoch [10/10], Step [389/390], Loss: 1.8746\n",
      "Epoch [10/10], Step [390/390], Loss: 2.1527\n",
      "Epoch [10/10], Step [391/390], Loss: 1.6612\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## While it is training here we have some lectures about the parameters of a CNN.\n",
    "\n",
    "\n",
    "\n",
    "1.   Stride: https://deepai.org/machine-learning-glossary-and-terms/stride\n",
    "2.   Padding: https://deepai.org/machine-learning-glossary-and-terms/padding\n",
    "\n",
    "Some lectures about CNN:\n",
    "https://deepai.org/machine-learning-glossary-and-terms/convolutional-neural-network\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "uBBhZ9g17YIp",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation of the model. \n",
    "The model has decreased the loss during the training, but Loss is not a very intuitive metric. We will first evaluate the metric on the training set."
   ],
   "metadata": {
    "id": "N91COub8Ad8W",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "  num_correct = 0\n",
    "  num_samples = 0\n",
    "  for batch_idx, (data,targets) in enumerate(test_gen):\n",
    "    data = data.cuda()\n",
    "    targets = targets.cuda()\n",
    "    ## Forward Pass\n",
    "    scores = net(data)\n",
    "    _, predictions = scores.max(1)\n",
    "    num_correct += (predictions == targets).sum()\n",
    "    num_samples += predictions.size(0)\n",
    "print('Training Accuracy of the model: %.3f %%' %((100*num_correct)/(num_samples+1)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ti7QcVu6BOF6",
    "outputId": "8e17a38b-6b57-4da4-cac0-08fde3fd5742",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Accuracy of the model: 42.986 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now is your turn. Some things you can try:\n",
    "\n",
    "*   Modify the batch size. The batch size is a number of samples processed before the model is updated. \n",
    "*   Modify the number of epochs. The number of epochs is the number of complete passes through the training dataset.\n",
    "*   Modify the learning rate. The learning rate controls how quickly the model is adapted to the problem.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "zpWY3gCECNFn",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}