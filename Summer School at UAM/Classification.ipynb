{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "Classification.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [
    "L44fBr_IuRvi",
    "KpmvG2-AuRvl",
    "VVbii4JbuRvr",
    "hoHcKCRvuRvv",
    "KCVtPa5MuRv4",
    "TTaAujwPuRwL",
    "sWnt8v4XuRwS",
    "IaGwFrBAuRwc",
    "ef9vsjS1uRwd",
    "IjYk0onGuRwn",
    "1Ic95ehTuRwr",
    "TEW0OalquRw3",
    "uSPcihXDuRw6",
    "WT_z_iDTuRw8",
    "qTxvEDjuuRxC",
    "N4W877t0uRxH"
   ],
   "include_colab_link": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ivangtorre/classification_UAM/blob/master/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L44fBr_IuRvi",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset Information\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmnU0zGyuRvk",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n",
    "\n",
    "1 - Spruce/Fir\n",
    "2 - Lodgepole Pine\n",
    "3 - Ponderosa Pine\n",
    "4 - Cottonwood/Willow\n",
    "5 - Aspen\n",
    "6 - Douglas-fir\n",
    "7 - Krummholz\n",
    "\n",
    "The training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. You must predict the Cover_Type for every row in the test set (565892 observations).\n",
    "\n",
    "Data Fields\n",
    "Elevation - Elevation in meters\n",
    "Aspect - Aspect in degrees azimuth\n",
    "Slope - Slope in degrees\n",
    "Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\n",
    "Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\n",
    "Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway\n",
    "Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\n",
    "Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\n",
    "Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\n",
    "Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points\n",
    "Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\n",
    "Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n",
    "Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation\n",
    "\n",
    "The wilderness areas are:\n",
    "\n",
    "1 - Rawah Wilderness Area\n",
    "2 - Neota Wilderness Area\n",
    "3 - Comanche Peak Wilderness Area\n",
    "4 - Cache la Poudre Wilderness Area\n",
    "\n",
    "The soil types are:\n",
    "\n",
    "1 Cathedral family - Rock outcrop complex, extremely stony.\n",
    "2 Vanet - Ratake families complex, very stony.\n",
    "3 Haploborolis - Rock outcrop complex, rubbly.\n",
    "4 Ratake family - Rock outcrop complex, rubbly.\n",
    "5 Vanet family - Rock outcrop complex complex, rubbly.\n",
    "6 Vanet - Wetmore families - Rock outcrop complex, stony.\n",
    "7 Gothic family.\n",
    "8 Supervisor - Limber families complex.\n",
    "9 Troutville family, very stony.\n",
    "10 Bullwark - Catamount families - Rock outcrop complex, rubbly.\n",
    "11 Bullwark - Catamount families - Rock land complex, rubbly.\n",
    "12 Legault family - Rock land complex, stony.\n",
    "13 Catamount family - Rock land - Bullwark family complex, rubbly.\n",
    "14 Pachic Argiborolis - Aquolis complex.\n",
    "15 unspecified in the USFS Soil and ELU Survey.\n",
    "16 Cryaquolis - Cryoborolis complex.\n",
    "17 Gateview family - Cryaquolis complex.\n",
    "18 Rogert family, very stony.\n",
    "19 Typic Cryaquolis - Borohemists complex.\n",
    "20 Typic Cryaquepts - Typic Cryaquolls complex.\n",
    "21 Typic Cryaquolls - Leighcan family, till substratum complex.\n",
    "22 Leighcan family, till substratum, extremely bouldery.\n",
    "23 Leighcan family, till substratum - Typic Cryaquolls complex.\n",
    "24 Leighcan family, extremely stony.\n",
    "25 Leighcan family, warm, extremely stony.\n",
    "26 Granile - Catamount families complex, very stony.\n",
    "27 Leighcan family, warm - Rock outcrop complex, extremely stony.\n",
    "28 Leighcan family - Rock outcrop complex, extremely stony.\n",
    "29 Como - Legault families complex, extremely stony.\n",
    "30 Como family - Rock land - Legault family complex, extremely stony.\n",
    "31 Leighcan - Catamount families complex, extremely stony.\n",
    "32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n",
    "33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n",
    "34 Cryorthents - Rock land complex, extremely stony.\n",
    "35 Cryumbrepts - Rock outcrop - Cryaquepts complex.\n",
    "36 Bross family - Rock land - Cryumbrepts complex, extremely stony.\n",
    "37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n",
    "38 Leighcan - Moran families - Cryaquolls complex, extremely stony.\n",
    "39 Moran family - Cryorthents - Leighcan family complex, extremely stony.\n",
    "40 Moran family - Cryorthents - Rock land complex, extremely stony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpmvG2-AuRvl",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QUszTHdpuRvn",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVbii4JbuRvr",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "Kau7AoGTuRvs",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "url =\"https://raw.githubusercontent.com/ivangtorre/classification_UAM/master/train.csv\"\n",
    "train = pd.read_csv(url)\n",
    "print(\"Rows and Columns(Train): \",train.shape)\n",
    "      \n",
    "      \n",
    "      "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoHcKCRvuRvv",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Check data and clean "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eJ5iy_MXuRvw",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# train.info()\n",
    "train.head()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hhbP59dXuRvz",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# # Datos perdidos\n",
    "# train.isnull().any().any()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cniO6zaTuRv1",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# # Duplicados\n",
    "# sum(train.duplicated())"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCVtPa5MuRv4",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exploracion de los datos\n",
    "Numero de datos por tipo, como son los features, sudistribucion, etc \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "x4gsup8-uRv5",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Exporacion de los tipo de vegetacion\n",
    "train['Cover_Type'].value_counts()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "51NqyZObuRv7",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Tambi√©n lo podemos representar\n",
    "sns.countplot('Cover_Type', data=train)\n",
    "\n",
    "# plt.bar(train['Soil_Type39'].value_counts())"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1zep4QnVuRv-",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Vamos a ver las variables continuas # Slope\n",
    "plt.hist(train.Elevation, bins=30)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "emJFpUSduRwB",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Las variables discretas\n",
    "train.columns"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JchT6RDkuRwF",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Representamos los tipos de suelos que tenemos en la zona\n",
    "# for name in ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n",
    "#        'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n",
    "#        'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n",
    "#        'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n",
    "#        'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n",
    "#        'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n",
    "#        'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n",
    "#        'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n",
    "#        'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n",
    "#        'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']:\n",
    "#     print( train[name].sum())\n",
    "   "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "tWixp6oGuRwH",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "train.columns"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xig1YAVpuRwJ",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "sns.boxplot(x=\"Cover_Type\", y=\"Elevation\", data=train);\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTaAujwPuRwL",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Correlacion entre variables\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FQMdricKuRwM",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# List of pairs along with correlation above threshold\n",
    "size = 40\n",
    "data=train.iloc[:,:size] \n",
    "data_corr = data.corr()\n",
    "threshold = 0.2\n",
    "corr_list = []\n",
    "\n",
    "#Search for the highly correlated pairs\n",
    "for i in range(0,size): #for 'size' features\n",
    "    for j in range(i+1,size): #avoid repetition\n",
    "        if (data_corr.iloc[i,j] >= threshold and data_corr.iloc[i,j] < 1) or (data_corr.iloc[i,j] < 0 and data_corr.iloc[i,j] <= -threshold):\n",
    "            corr_list.append([data_corr.iloc[i,j],i,j]) #store correlation and columns index\n",
    "\n",
    "#Sort to show higher ones first            \n",
    "s_corr_list = sorted(corr_list,key=lambda x: -abs(x[0]))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "qswdDiTJuRwO",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "sns.pairplot(train, vars=['Elevation', 'Aspect', 'Slope', \"Cover_Type\", \"Hillshade_Noon\"])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wkcN76q8uRwQ",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "correlations = train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\n",
    "correlations = correlations[correlations['level_0'] != correlations['level_1']]\n",
    "correlations = correlations[~correlations[0].isna()]\n",
    "#correlations[correlations['level_0'] == \"Cover_Type\"]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWnt8v4XuRwS",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8QAlByRruRwT",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "y=train['Cover_Type']\n",
    "train=train.drop(['Id','Cover_Type'],1)\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SK6dC2a3uRwV",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, y, test_size=0.3, random_state=1)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "qdzHFZy_uRwY",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "x_train.describe()\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaGwFrBAuRwc",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Construimos un modelo de Random Forest\n",
    "\n",
    "\n",
    "Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model‚Äôs prediction\n",
    "\n",
    "\n",
    "A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.\n",
    "The low correlation between models is the key. Just like how investments with low correlations (like stocks and bonds) come together to form a portfolio that is greater than the sum of its parts, uncorrelated models can produce ensemble predictions that are more accurate than any of the individual predictions. The reason for this wonderful effect is that the trees protect each other from their individual errors (as long as they don‚Äôt constantly all err in the same direction). While some trees may be wrong, many other trees will be right, so as a group the trees are able to move in the correct direction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef9vsjS1uRwd",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Detalles Random Forest\n",
    "\n",
    "RandomForestClassifier(n_estimators=‚Äôwarn‚Äô, criterion=‚Äôgini‚Äô, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=‚Äôauto‚Äô, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "\n",
    "Parameters:\t\n",
    "n_estimators : integer, optional (default=10)\n",
    "The number of trees in the forest.\n",
    "\n",
    "Changed in version 0.20: The default value of n_estimators will change from 10 in version 0.20 to 100 in version 0.22.\n",
    "\n",
    "criterion : string, optional (default=‚Äùgini‚Äù)\n",
    "The function to measure the quality of a split. Supported criteria are ‚Äúgini‚Äù for the Gini impurity and ‚Äúentropy‚Äù for the information gain. Note: this parameter is tree-specific.\n",
    "\n",
    "max_depth : integer or None, optional (default=None)\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "min_samples_split : int, float, optional (default=2)\n",
    "The minimum number of samples required to split an internal node:\n",
    "\n",
    "If int, then consider min_samples_split as the minimum number.\n",
    "If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "Changed in version 0.18: Added float values for fractions.\n",
    "\n",
    "min_samples_leaf : int, float, optional (default=1)\n",
    "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "If int, then consider min_samples_leaf as the minimum number.\n",
    "If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "Changed in version 0.18: Added float values for fractions.\n",
    "\n",
    "min_weight_fraction_leaf : float, optional (default=0.)\n",
    "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "huFbUSctuRwe",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Construimos un modelo\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=2, max_depth=5, random_state=1)\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8NySh0NNuRwg",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Fit\n",
    "rf.fit(x_train, y_train)\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g2m4K3znuRwi",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Prediction\n",
    "pred=rf.predict(x_test)\n",
    "pred"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XK7Nv0QBuRwl",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Score\n",
    "acc=rf.score(x_test,y_test)\n",
    "print(acc)\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjYk0onGuRwn",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Crossvalidation\n",
    "Cross-validation, it‚Äôs a model validation techniques for assessing how the results of a statistical analysis (model) will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n",
    "The goal of cross-validation is to define a data set to test the model in the training phase (i.e. validation data set) in order to limit problems like overfitting,underfitting and get an insight on how the model will generalize to an independent data set. It is important the validation and the training set to be drawn from the same distribution otherwise it would make things worse."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "hm32ym-yuRwn",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Construimos 5 sets\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import numpy as np\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "ySjDHwWwuRwp",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Se construye el mismo modelo pero diferente set de datos\n",
    "rf = RandomForestClassifier(n_estimators=100,n_jobs=2, max_depth=5, random_state=1)\n",
    "total_acc = []\n",
    "from sklearn.metrics import accuracy_score\n",
    "for fold_n, (train_index, test_index) in enumerate(folds.split(train, y)):\n",
    "    #print('Fold', fold_n, 'started at', time.ctime(),end = \"  \")\n",
    "    x_train, x_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    rf.fit(x_train, y_train)\n",
    "    pred=rf.predict(x_test)\n",
    "    total_acc.append(rf.score(x_test,y_test))\n",
    "    print(\"Validation Score: \",rf.score(x_test,y_test))\n",
    "\n",
    "print(\"Mean Testing Score: \",np.mean(total_acc))       \n",
    " "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ic95ehTuRwr",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Busqueda de hyperparametros + CrossValidation\n",
    "Grid-searching is the process of scanning the data to configure optimal parameters for a given model. Depending on the type of model utilized, certain parameters are necessary. Grid-searching does NOT only apply to one model type. Grid-searching can be applied across machine learning to calculate the best parameters to use for any given model. It is important to note that Grid-searching can be extremely computationally expensive and may take your machine quite a long time to run. Grid-Search will build a model on each parameter combination possible. It iterates through every parameter combination and stores a model for each combination"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n-apKKyHuRws",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Dummy model\n",
    "rf = RandomForestClassifier(n_estimators=400,n_jobs=2, max_depth=5, random_state=1)\n",
    "total_acc = []\n",
    "from sklearn.metrics import accuracy_score\n",
    "for fold_n, (train_index, test_index) in enumerate(folds.split(train, y)):\n",
    "    #print('Fold', fold_n, 'started at', time.ctime(),end = \"  \")\n",
    "    x_train, x_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    rf.fit(x_train, y_train)\n",
    "    pred=rf.predict(x_test)\n",
    "    total_acc.append(rf.score(x_test,y_test))\n",
    "    print(\"Validation Score: \",rf.score(x_test,y_test))\n",
    "\n",
    "print(\"Mean Testing Score: \",np.mean(total_acc))   "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xpi3z-JUuRwu",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "9_iPcd5huRwx",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Malla de hyperpar√°metros. OJO que escala muy r√°pido el coste computacional\n",
    "tuned_parameters = {'n_estimators':[300], \n",
    "              'max_depth':[5, None], \n",
    "              'max_leaf_nodes':[5,None],\n",
    "              'min_samples_leaf':[1,2]\n",
    "              }\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(random_state=1), tuned_parameters, cv=folds, scoring='accuracy', verbose = 2, n_jobs=-1)\n",
    "grid_fit = clf.fit(train, y)\n",
    "\n",
    "print(\"Mejores parametros en train:\")\n",
    "print()\n",
    "print(grid_fit.best_params_)\n",
    "print()\n",
    "print(\"Grid scores en validation:\")\n",
    "print()\n",
    "means = grid_fit.cv_results_['mean_test_score']\n",
    "stds = grid_fit.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_fit.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "\n",
    "# grid_obj = GridSearchCV(clf, parameters, scoring='accuracy', verbose=1, cv=2)\n",
    "\n",
    "# grid_fit = grid_obj.fit(X_train, y_train)\n",
    "# best_clf = grid_fit.best_estimator_\n",
    "# print(best_clf)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r2_PqASfuRwz",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "best_clf = grid_fit.best_estimator_\n",
    "print(best_clf)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r3DbJB53uRw2",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "means = grid_fit.cv_results_['mean_test_score']\n",
    "stds = grid_fit.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_fit.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEW0OalquRw3",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Selecccion del mejor modelo e importancia de los features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "6e9OYEO4uRw4",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Dummy model\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n",
    "            oob_score=False, random_state=1, verbose=0, warm_start=False)\n",
    "\n",
    "\n",
    "total_acc = []\n",
    "from sklearn.metrics import accuracy_score\n",
    "for fold_n, (train_index, test_index) in enumerate(folds.split(train, y)):\n",
    "    #print('Fold', fold_n, 'started at', time.ctime(),end = \"  \")\n",
    "    x_train, x_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    rf.fit(x_train, y_train)\n",
    "    pred=rf.predict(x_test)\n",
    "    total_acc.append(rf.score(x_test,y_test))\n",
    "    print(\"Validation Score: \",rf.score(x_test,y_test))\n",
    "\n",
    "print(\"Mean Testing Score: \",np.mean(total_acc))   "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSPcihXDuRw6",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Representacion de la importancia de las features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oDgsvpU8uRw7",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "features = train.columns.values\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT_z_iDTuRw8",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Support Vector Machine\n",
    "Support vector machine is a representation of the training data as points in space separated into categories by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n",
    "\n",
    "Advantages: Effective in high dimensional spaces and uses a subset of training points in the decision function so it is also memory efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXRSf7LRuRw9",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "C-Support Vector Classification.\n",
    "\n",
    "The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using sklearn.linear_model.LinearSVC or sklearn.linear_model.SGDClassifier instead, possibly after a sklearn.kernel_approximation.Nystroem transformer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Dk0Sj772uRw-",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "9J3lOAFnuRxA",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Malla de hyperpar√°metros. OJO que escala muy r√°pido el coste computacional\n",
    "tuned_parameters = {'gamma': [1e-4],\n",
    "                     'C': [1]}\n",
    "\n",
    "clf = GridSearchCV(SVC(random_state=1), tuned_parameters, cv=folds, scoring='accuracy', verbose = 2, n_jobs=-1)\n",
    "grid_fit = clf.fit(train, y)\n",
    "\n",
    "print(\"Mejores parametros en train:\")\n",
    "print()\n",
    "print(grid_fit.best_params_)\n",
    "print()\n",
    "print(\"Grid scores en validation:\")\n",
    "print()\n",
    "means = grid_fit.cv_results_['mean_test_score']\n",
    "stds = grid_fit.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_fit.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "\n",
    "# grid_obj = GridSearchCV(clf, parameters, scoring='accuracy', verbose=1, cv=2)\n",
    "\n",
    "# grid_fit = grid_obj.fit(X_train, y_train)\n",
    "# best_clf = grid_fit.best_estimator_\n",
    "# print(best_clf)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTxvEDjuuRxC",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nyU4Qw41uRxD",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MOQjBb-9uRxE",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Malla de hyperpar√°metros. OJO que escala muy r√°pido el coste computacional\n",
    "tuned_parameters = {'n_estimators': [50, 200]}\n",
    "\n",
    "clf = GridSearchCV(AdaBoostClassifier(random_state=1), tuned_parameters, cv=folds, scoring='accuracy', verbose = 2, n_jobs=-1)\n",
    "grid_fit = clf.fit(train, y)\n",
    "\n",
    "print(\"Mejores parametros en train:\")\n",
    "print()\n",
    "print(grid_fit.best_params_)\n",
    "print()\n",
    "print(\"Grid scores en validation:\")\n",
    "print()\n",
    "means = grid_fit.cv_results_['mean_test_score']\n",
    "stds = grid_fit.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_fit.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "\n",
    "# grid_obj = GridSearchCV(clf, parameters, scoring='accuracy', verbose=1, cv=2)\n",
    "\n",
    "# grid_fit = grid_obj.fit(X_train, y_train)\n",
    "# best_clf = grid_fit.best_estimator_\n",
    "# print(best_clf)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4W877t0uRxH",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g0tHKsdnuRxH",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H8Uy9iTmuRxJ",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "execution_count": 0,
   "outputs": []
  }
 ]
}